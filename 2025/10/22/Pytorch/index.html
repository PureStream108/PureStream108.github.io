<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Pytorch | PureStream &amp; Marblue</title><meta name="author" content="Pure Stream"><meta name="copyright" content="Pure Stream"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="AI安全学习(1)">
<meta property="og:type" content="article">
<meta property="og:title" content="Pytorch">
<meta property="og:url" content="https://purestream108.github.io/project/2025/10/22/Pytorch/index.html">
<meta property="og:site_name" content="PureStream &amp; Marblue">
<meta property="og:description" content="AI安全学习(1)">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://purestream108.github.io/project/img/purestream.jpg">
<meta property="article:published_time" content="2025-10-22T10:49:46.000Z">
<meta property="article:modified_time" content="2025-10-30T11:23:31.330Z">
<meta property="article:author" content="Pure Stream">
<meta property="article:tag" content="AI">
<meta property="article:tag" content="Pytorch">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://purestream108.github.io/project/img/purestream.jpg"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Pytorch",
  "url": "https://purestream108.github.io/project/2025/10/22/Pytorch/",
  "image": "https://purestream108.github.io/project/img/purestream.jpg",
  "datePublished": "2025-10-22T10:49:46.000Z",
  "dateModified": "2025-10-30T11:23:31.330Z",
  "author": [
    {
      "@type": "Person",
      "name": "Pure Stream",
      "url": "https://PureStream108.github.io/project"
    }
  ]
}</script><link rel="shortcut icon" href="/img/marblue.jpg"><link rel="canonical" href="https://purestream108.github.io/project/2025/10/22/Pytorch/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"limitCount":50,"languages":{"author":"作者: Pure Stream","link":"链接: ","source":"来源: PureStream & Marblue","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: true,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Pytorch',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><meta name="generator" content="Hexo 7.3.0"><link rel="stylesheet" href="/css/prism.css" type="text/css">
<link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/lazy.gif" data-original="/img/purestream.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">23</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">29</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">20</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/bangumis/"><i class="fa-fw fas fa-heart"></i><span> Animation</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(/img/w1.jpg);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">PureStream &amp; Marblue</span></a><a class="nav-page-title" href="/"><span class="site-name">Pytorch</span><span class="site-name"><i class="fa-solid fa-circle-arrow-left"></i><span>  返回首页</span></span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/bangumis/"><i class="fa-fw fas fa-heart"></i><span> Animation</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">Pytorch</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-10-22T10:49:46.000Z" title="发表于 2025-10-22 18:49:46">2025-10-22</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-10-30T11:23:31.330Z" title="更新于 2025-10-30 19:23:31">2025-10-30</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/AI%E5%AE%89%E5%85%A8/">AI安全</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h1 id="Pytorch（持续更新ing）"><a href="#Pytorch（持续更新ing）" class="headerlink" title="Pytorch（持续更新ing）"></a>Pytorch（持续更新ing）</h1><p>（本文档只是给我自己看的，大部分来源菜鸟教程，小部分我不理解的我会自己补充）</p>
<h2 id="开始"><a href="#开始" class="headerlink" title="开始"></a>开始</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip install torch</span><br><span class="line">pip install torchvision</span><br></pre></td></tr></table></figure>
<p>如果不想要GPU加速，可以换成</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu</span><br></pre></td></tr></table></figure>
<p>然后直接运行第一段代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">a = torch.randn(<span class="number">2</span>, <span class="number">3</span>) <span class="comment">#创建一个服从正态分布的随机张量，均值为 0，标准差为 1</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(a)</span><br></pre></td></tr></table></figure>
<h2 id="张量（tensor）"><a href="#张量（tensor）" class="headerlink" title="张量（tensor）"></a>张量（tensor）</h2><p>张量（Tensor）是 PyTorch 中的核心数据结构，用于存储和操作多维数组</p>
<p>张量可以视为一个多维数组，支持加速计算的操作</p>
<p>在 PyTorch 中，张量的概念类似于 NumPy 中的数组，但是 PyTorch 的张量可以运行在不同的设备上，比如 CPU 和 GPU，这使得它们非常适合于进行大规模并行计算，特别是在深度学习领域。</p>
<ul>
<li><strong>维度（Dimensionality）</strong>：张量的维度指的是数据的多维数组结构。例如，一个标量（0维张量）是一个单独的数字，一个向量（1维张量）是一个一维数组，一个矩阵（2维张量）是一个二维数组，以此类推</li>
<li><strong>形状（Shape）</strong>：张量的形状是指每个维度上的大小。例如，一个形状为<code>(3, 4)</code>的张量意味着它有3行4列</li>
<li><strong>数据类型（Dtype）</strong>：张量中的数据类型定义了存储每个元素所需的内存大小和解释方式。PyTorch支持多种数据类型，包括整数型（如<code>torch.int8</code>、<code>torch.int32</code>）、浮点型（如<code>torch.float32</code>、<code>torch.float64</code>）和布尔型（<code>torch.bool</code>）</li>
</ul>
<p>张量可以存储在 CPU 或 GPU 中，GPU 张量可显著加速计算，先放张菜鸟教程的图：</p>
<p><img src="/img/lazy.gif" data-original="1.jpg" alt=""></p>
<h3 id="张量基本方式"><a href="#张量基本方式" class="headerlink" title="张量基本方式"></a>张量基本方式</h3><p>但感觉直接说概念有点太过于抽象，不如先看看几个基本方式：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left"><strong>方法</strong></th>
<th style="text-align:left"><strong>说明</strong></th>
<th style="text-align:left"><strong>示例代码</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>torch.tensor(data)</code></td>
<td style="text-align:left">从 Python 列表或 NumPy 数组创建张量</td>
<td style="text-align:left"><code>x = torch.tensor([[1, 2], [3, 4]])</code></td>
</tr>
<tr>
<td style="text-align:left"><code>torch.zeros(size)</code></td>
<td style="text-align:left">创建一个全为零的张量</td>
<td style="text-align:left"><code>x = torch.zeros((2, 3))</code></td>
</tr>
<tr>
<td style="text-align:left"><code>torch.ones(size)</code></td>
<td style="text-align:left">创建一个全为 1 的张量</td>
<td style="text-align:left"><code>x = torch.ones((2, 3))</code></td>
</tr>
<tr>
<td style="text-align:left"><code>torch.empty(size)</code></td>
<td style="text-align:left">创建一个未初始化的张量</td>
<td style="text-align:left"><code>x = torch.empty((2, 3))</code></td>
</tr>
<tr>
<td style="text-align:left"><code>torch.rand(size)</code></td>
<td style="text-align:left">创建一个服从均匀分布的随机张量，值在 <code>[0, 1)</code></td>
<td style="text-align:left"><code>x = torch.rand((2, 3))</code></td>
</tr>
<tr>
<td style="text-align:left"><code>torch.randn(size)</code></td>
<td style="text-align:left">创建一个服从正态分布的随机张量，均值为 0，标准差为 1</td>
<td style="text-align:left"><code>x = torch.randn((2, 3))</code></td>
</tr>
<tr>
<td style="text-align:left"><code>torch.arange(start, end, step)</code></td>
<td style="text-align:left">创建一个一维序列张量，类似于 Python 的 <code>range</code></td>
<td style="text-align:left"><code>x = torch.arange(0, 10, 2)</code></td>
</tr>
<tr>
<td style="text-align:left"><code>torch.linspace(start, end, steps)</code></td>
<td style="text-align:left">创建一个在指定范围内等间隔的序列张量</td>
<td style="text-align:left"><code>x = torch.linspace(0, 1, 5)</code></td>
</tr>
<tr>
<td style="text-align:left"><code>torch.eye(size)</code></td>
<td style="text-align:left">创建一个单位矩阵（对角线为 1，其他为 0）</td>
<td style="text-align:left"><code>x = torch.eye(3)</code></td>
</tr>
<tr>
<td style="text-align:left"><code>torch.from_numpy(ndarray)</code></td>
<td style="text-align:left">将 NumPy 数组转换为张量</td>
<td style="text-align:left"><code>x = torch.from_numpy(np.array([1, 2, 3]))</code></td>
</tr>
</tbody>
</table>
</div>
<h3 id="张量基本属性"><a href="#张量基本属性" class="headerlink" title="张量基本属性"></a>张量基本属性</h3><p>然后张量的属性：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left"><strong>属性</strong></th>
<th style="text-align:left"><strong>说明</strong></th>
<th style="text-align:left"><strong>示例</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>.shape</code></td>
<td style="text-align:left">获取张量的形状</td>
<td style="text-align:left"><code>tensor.shape</code></td>
</tr>
<tr>
<td style="text-align:left"><code>.size()</code></td>
<td style="text-align:left">获取张量的形状</td>
<td style="text-align:left"><code>tensor.size()</code></td>
</tr>
<tr>
<td style="text-align:left"><code>.dtype</code></td>
<td style="text-align:left">获取张量的数据类型</td>
<td style="text-align:left"><code>tensor.dtype</code></td>
</tr>
<tr>
<td style="text-align:left"><code>.device</code></td>
<td style="text-align:left">查看张量所在的设备 (CPU/GPU)</td>
<td style="text-align:left"><code>tensor.device</code></td>
</tr>
<tr>
<td style="text-align:left"><code>.dim()</code></td>
<td style="text-align:left">获取张量的维度数</td>
<td style="text-align:left"><code>tensor.dim()</code></td>
</tr>
<tr>
<td style="text-align:left"><code>.requires_grad</code></td>
<td style="text-align:left">是否启用梯度计算</td>
<td style="text-align:left"><code>tensor.requires_grad</code></td>
</tr>
<tr>
<td style="text-align:left"><code>.numel()</code></td>
<td style="text-align:left">获取张量中的元素总数</td>
<td style="text-align:left"><code>tensor.numel()</code></td>
</tr>
<tr>
<td style="text-align:left"><code>.is_cuda</code></td>
<td style="text-align:left">检查张量是否在 GPU 上</td>
<td style="text-align:left"><code>tensor.is_cuda</code></td>
</tr>
<tr>
<td style="text-align:left"><code>.T</code></td>
<td style="text-align:left">获取张量的转置（适用于 2D 张量）</td>
<td style="text-align:left"><code>tensor.T</code></td>
</tr>
<tr>
<td style="text-align:left"><code>.item()</code></td>
<td style="text-align:left">获取单元素张量的值</td>
<td style="text-align:left"><code>tensor.item()</code></td>
</tr>
<tr>
<td style="text-align:left"><code>.is_contiguous()</code></td>
<td style="text-align:left">检查张量是否连续存储</td>
<td style="text-align:left"><code>tensor.is_contiguous()</code></td>
</tr>
</tbody>
</table>
</div>
<p>看了上面之后，结合下面示例可以更加理解：（依旧来自菜鸟）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个 2D 张量</span></span><br><span class="line">tensor = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]], dtype=torch.float32)  </span><br><span class="line"><span class="comment">#指定为浮点数，也可以自己尝试 torch. 然后可以看到其他的变量类型</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 张量的属性</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Tensor:\n&quot;</span>, tensor)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Shape:&quot;</span>, tensor.shape)  <span class="comment"># 获取形状</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Size:&quot;</span>, tensor.size())  <span class="comment"># 获取形状（另一种方法）</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Data Type:&quot;</span>, tensor.dtype)  <span class="comment"># 数据类型</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Device:&quot;</span>, tensor.device)  <span class="comment"># 设备</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Dimensions:&quot;</span>, tensor.dim())  <span class="comment"># 维度数</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Total Elements:&quot;</span>, tensor.numel())  <span class="comment"># 元素总数</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Requires Grad:&quot;</span>, tensor.requires_grad)  <span class="comment"># 是否启用梯度</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Is CUDA:&quot;</span>, tensor.is_cuda)  <span class="comment"># 是否在 GPU 上</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Is Contiguous:&quot;</span>, tensor.is_contiguous())  <span class="comment"># 是否连续存储</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取单元素值</span></span><br><span class="line">single_value = torch.tensor(<span class="number">42</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Single Element Value:&quot;</span>, single_value.item())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 转置张量</span></span><br><span class="line">tensor_T = tensor.T</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Transposed Tensor:\n&quot;</span>, tensor_T)</span><br></pre></td></tr></table></figure>
<p>输出结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">Tensor:</span><br><span class="line"> tensor([[<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>],</span><br><span class="line">         [<span class="number">4.</span>, <span class="number">5.</span>, <span class="number">6.</span>]])</span><br><span class="line">Shape: torch.Size([<span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">Size: torch.Size([<span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">Data <span class="type">Type</span>: torch.float32</span><br><span class="line">Device: cpu</span><br><span class="line">Dimensions: <span class="number">2</span></span><br><span class="line">Total Elements: <span class="number">6</span></span><br><span class="line">Requires Grad: <span class="literal">False</span></span><br><span class="line">Is CUDA: <span class="literal">False</span></span><br><span class="line">Is Contiguous: <span class="literal">True</span></span><br><span class="line">Single Element Value: <span class="number">42</span></span><br><span class="line">Transposed Tensor:</span><br><span class="line"> tensor([[<span class="number">1.</span>, <span class="number">4.</span>],</span><br><span class="line">         [<span class="number">2.</span>, <span class="number">5.</span>],</span><br><span class="line">         [<span class="number">3.</span>, <span class="number">6.</span>]])</span><br></pre></td></tr></table></figure>
<p>除此之外还有个是<code>torch.stack((x, x), dim=?)</code>，这里表示将张量x和x在第几维度（dim）进行堆叠，比如说dim=0,1,2，这边类比到坐标轴上，那么</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#dim=0 Z轴  层数+1（层）</span><br><span class="line">#dim=1 Y轴  行数+1（行）</span><br><span class="line">#dim=2 X轴  列数+1（列）</span><br></pre></td></tr></table></figure>
<p>可以具体看个例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">a = torch.randn(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"><span class="comment">#b = torch.randn(2, 3)</span></span><br><span class="line"><span class="comment">#c = torch.eye(3)</span></span><br><span class="line"><span class="comment">#d = torch.from_numpy(np.array([[1, 2], [3, 4]]))</span></span><br><span class="line">e = torch.stack((a, a), dim=<span class="number">0</span>)</span><br><span class="line">f = torch.stack((a, a), dim=<span class="number">1</span>)</span><br><span class="line">g = torch.stack((a, a), dim=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot; &quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;===== a =====&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot; &quot;</span>)</span><br><span class="line"><span class="comment">#print(b)</span></span><br><span class="line"><span class="comment">#print(c)</span></span><br><span class="line"><span class="comment">#print(c.shape)</span></span><br><span class="line"><span class="comment">#print(d)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;===== dim=0 =====&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(e)</span><br><span class="line"><span class="built_in">print</span>(e.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot; &quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;===== dim=1 =====&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(f)</span><br><span class="line"><span class="built_in">print</span>(f.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot; &quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;===== dim=2 =====&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(g)</span><br><span class="line"><span class="built_in">print</span>(g.shape)</span><br></pre></td></tr></table></figure>
<p>运行后结果是：（size里的[2, 3, 4]就分别对应0,1,2）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">===== a =====</span><br><span class="line">tensor([[<span class="number">0.3607</span>, <span class="number">0.9021</span>, <span class="number">0.0285</span>, <span class="number">0.0186</span>],</span><br><span class="line">        [<span class="number">0.1229</span>, <span class="number">0.5829</span>, <span class="number">0.8991</span>, <span class="number">0.4807</span>],</span><br><span class="line">        [<span class="number">0.0543</span>, <span class="number">0.2008</span>, <span class="number">0.0578</span>, <span class="number">0.4865</span>]])</span><br><span class="line"> </span><br><span class="line">===== dim=<span class="number">0</span> =====                                 // 直接层数 + <span class="number">1</span>（原本层数为<span class="number">1</span>，在dim=<span class="number">0</span>进行两个x的堆叠所以层数为<span class="number">2</span>）</span><br><span class="line">tensor([[[<span class="number">0.3607</span>, <span class="number">0.9021</span>, <span class="number">0.0285</span>, <span class="number">0.0186</span>],</span><br><span class="line">         [<span class="number">0.1229</span>, <span class="number">0.5829</span>, <span class="number">0.8991</span>, <span class="number">0.4807</span>],</span><br><span class="line">         [<span class="number">0.0543</span>, <span class="number">0.2008</span>, <span class="number">0.0578</span>, <span class="number">0.4865</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">0.3607</span>, <span class="number">0.9021</span>, <span class="number">0.0285</span>, <span class="number">0.0186</span>],</span><br><span class="line">         [<span class="number">0.1229</span>, <span class="number">0.5829</span>, <span class="number">0.8991</span>, <span class="number">0.4807</span>],</span><br><span class="line">         [<span class="number">0.0543</span>, <span class="number">0.2008</span>, <span class="number">0.0578</span>, <span class="number">0.4865</span>]]])</span><br><span class="line">torch.Size([<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line"> </span><br><span class="line">===== dim=<span class="number">1</span> =====                                // 直接行数 + <span class="number">1</span>（原本层数为<span class="number">1</span>，在dim=<span class="number">1</span>进行两个x的堆叠）</span><br><span class="line">tensor([[[<span class="number">0.3607</span>, <span class="number">0.9021</span>, <span class="number">0.0285</span>, <span class="number">0.0186</span>],       // 此时由于<span class="number">1</span>对应行，所以层变为<span class="number">3</span>，每层函数变为<span class="number">2</span></span><br><span class="line">         [<span class="number">0.3607</span>, <span class="number">0.9021</span>, <span class="number">0.0285</span>, <span class="number">0.0186</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">0.1229</span>, <span class="number">0.5829</span>, <span class="number">0.8991</span>, <span class="number">0.4807</span>],</span><br><span class="line">         [<span class="number">0.1229</span>, <span class="number">0.5829</span>, <span class="number">0.8991</span>, <span class="number">0.4807</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">0.0543</span>, <span class="number">0.2008</span>, <span class="number">0.0578</span>, <span class="number">0.4865</span>],</span><br><span class="line">         [<span class="number">0.0543</span>, <span class="number">0.2008</span>, <span class="number">0.0578</span>, <span class="number">0.4865</span>]]])</span><br><span class="line">torch.Size([<span class="number">3</span>, <span class="number">2</span>, <span class="number">4</span>])</span><br><span class="line"> </span><br><span class="line">===== dim=<span class="number">2</span> =====                                // 直接列数 + <span class="number">1</span></span><br><span class="line">tensor([[[<span class="number">0.3607</span>, <span class="number">0.3607</span>],                       // 将原本a的列转换为行然后copy一份（原理与上相同）</span><br><span class="line">         [<span class="number">0.9021</span>, <span class="number">0.9021</span>],</span><br><span class="line">         [<span class="number">0.0285</span>, <span class="number">0.0285</span>],</span><br><span class="line">         [<span class="number">0.0186</span>, <span class="number">0.0186</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">0.1229</span>, <span class="number">0.1229</span>],</span><br><span class="line">         [<span class="number">0.5829</span>, <span class="number">0.5829</span>],</span><br><span class="line">         [<span class="number">0.8991</span>, <span class="number">0.8991</span>],</span><br><span class="line">         [<span class="number">0.4807</span>, <span class="number">0.4807</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">0.0543</span>, <span class="number">0.0543</span>],</span><br><span class="line">         [<span class="number">0.2008</span>, <span class="number">0.2008</span>],</span><br><span class="line">         [<span class="number">0.0578</span>, <span class="number">0.0578</span>],</span><br><span class="line">         [<span class="number">0.4865</span>, <span class="number">0.4865</span>]]])</span><br><span class="line">torch.Size([<span class="number">3</span>, <span class="number">4</span>, <span class="number">2</span>])</span><br></pre></td></tr></table></figure>
<p>这样映射到XYZ轴后相比于直接写<strong>dim在新的维度上进行拼接</strong>，这个<strong>新的维度出现在第1维（从0开始计数）</strong>这样子好理解的多</p>
<h3 id="张量基本操作"><a href="#张量基本操作" class="headerlink" title="张量基本操作"></a>张量基本操作</h3><h4 id="基础操作："><a href="#基础操作：" class="headerlink" title="基础操作："></a>基础操作：</h4><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left"><strong>操作</strong></th>
<th style="text-align:left"><strong>说明</strong></th>
<th style="text-align:left"><strong>示例代码</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>+</code>, <code>-</code>, <code>*</code>, <code>/</code></td>
<td style="text-align:left">元素级加法、减法、乘法、除法</td>
<td style="text-align:left"><code>z = x + y</code></td>
</tr>
<tr>
<td style="text-align:left"><code>torch.matmul(x, y)</code></td>
<td style="text-align:left">矩阵乘法</td>
<td style="text-align:left"><code>z = torch.matmul(x, y)</code></td>
</tr>
<tr>
<td style="text-align:left"><code>torch.dot(x, y)</code></td>
<td style="text-align:left">向量点积（仅适用于 1D 张量）</td>
<td style="text-align:left"><code>z = torch.dot(x, y)</code></td>
</tr>
<tr>
<td style="text-align:left"><code>torch.sum(x)</code></td>
<td style="text-align:left">求和</td>
<td style="text-align:left"><code>z = torch.sum(x)</code></td>
</tr>
<tr>
<td style="text-align:left"><code>torch.mean(x)</code></td>
<td style="text-align:left">求均值</td>
<td style="text-align:left"><code>z = torch.mean(x)</code></td>
</tr>
<tr>
<td style="text-align:left"><code>torch.max(x)</code></td>
<td style="text-align:left">求最大值</td>
<td style="text-align:left"><code>z = torch.max(x)</code></td>
</tr>
<tr>
<td style="text-align:left"><code>torch.min(x)</code></td>
<td style="text-align:left">求最小值</td>
<td style="text-align:left"><code>z = torch.min(x)</code></td>
</tr>
<tr>
<td style="text-align:left"><code>torch.argmax(x, dim)</code></td>
<td style="text-align:left">返回最大值的索引（指定维度）</td>
<td style="text-align:left"><code>z = torch.argmax(x, dim=1)</code></td>
</tr>
<tr>
<td style="text-align:left"><code>torch.softmax(x, dim)</code></td>
<td style="text-align:left">计算 softmax（指定维度）</td>
<td style="text-align:left"><code>z = torch.softmax(x, dim=1)</code></td>
</tr>
</tbody>
</table>
</div>
<p> 这里大部分如果学过线性代数或者加减法的一般都很好理解，然后这里比较需要注意的就是torch.dot()、argmax()和softmax()，这里我们先看 <code>torch.dot()</code> ，这里的点积（适用于一维向量），其实就是我们数学中学的向量的点乘，具体公式为：</p>
<script type="math/tex; mode=display">
\mathbf{a} \cdot \mathbf{b} = a_1 b_1 + a_2 b_2 + \dots + a_n b_n = \sum_{i=1}^n a_i b_i</script><p>用torch来表示就是：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">a = torch.tensor([<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>])</span><br><span class="line">b = torch.tensor([<span class="number">4.0</span>, <span class="number">5.0</span>, <span class="number">6.0</span>])</span><br><span class="line"></span><br><span class="line">dot_product = torch.dot(a, b)</span><br><span class="line"><span class="built_in">print</span>(dot_product)  <span class="comment"># 输出: 1*4 + 2*5 + 3*6 = 32</span></span><br></pre></td></tr></table></figure>
<p>然后就是 <code>torch.argmax()</code>，返回最大值的索引，不过这个最大值的索引是指将数组展开来后的，比如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">a = torch.randn(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"><span class="built_in">print</span>(a.<span class="built_in">min</span>())</span><br><span class="line"><span class="built_in">print</span>(a.<span class="built_in">max</span>())</span><br><span class="line"><span class="built_in">print</span>(a.mean())</span><br><span class="line"><span class="built_in">print</span>(a.argmax())</span><br><span class="line"></span><br><span class="line"><span class="comment">#tensor([[-0.0678, -0.9911, -1.4649,  0.9039],</span></span><br><span class="line"><span class="comment">#        [ 1.1405,  0.7636, -0.0447, -0.4907],</span></span><br><span class="line"><span class="comment">#        [-0.8256, -1.1506,  0.7874, -0.5046]])</span></span><br><span class="line"><span class="comment">#tensor(-1.4649)</span></span><br><span class="line"><span class="comment">#tensor(1.1405)</span></span><br><span class="line"><span class="comment">#tensor(-0.1620)</span></span><br><span class="line"><span class="comment">#tensor(4)         &lt;--- 指的是1.1405，最开始从0开始数</span></span><br></pre></td></tr></table></figure>
<p>最后就是 <code>torch.softmax()</code> ，他是种常用的<strong>激活函数</strong>，主要用于<strong>多分类问题</strong>的输出层。它的作用是将一个<strong>任意实数向量</strong>转换成一个<strong>概率分布</strong>，总结来说，就是每个数值的概率密度，对于输入如下向量：</p>
<script type="math/tex; mode=display">
\mathbf{z} = [z_1, z_2, \dots, z_n]</script><p>公式如下：</p>
<script type="math/tex; mode=display">
\text{softmax}(z_i) = \frac{e^{z_i}}{\sum_{j=1}^{n} e^{z_j}}</script><p>为了避免整数溢出，实际通常减去最大值：</p>
<script type="math/tex; mode=display">
\text{softmax}(z_i) = \frac{e^{z_i - \max(\mathbf{z})}}{\sum_{j=1}^{n} e^{z_j - \max(\mathbf{z})}}</script><p>用torch表示就是：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line">a = torch.tensor([[<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>]])</span><br><span class="line"></span><br><span class="line">probs = F.softmax(a, dim=<span class="number">1</span>)      <span class="comment"># dim=0是行方向，dim=1是列方向</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(probs)</span><br><span class="line"></span><br><span class="line"><span class="comment">#tensor([[0.0900, 0.2447, 0.6652]])</span></span><br></pre></td></tr></table></figure>
<h4 id="形状操作："><a href="#形状操作：" class="headerlink" title="形状操作："></a>形状操作：</h4><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left"><strong>操作</strong></th>
<th style="text-align:left"><strong>说明</strong></th>
<th style="text-align:left"><strong>示例代码</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>x.view(shape)</code></td>
<td style="text-align:left">改变张量的形状（不改变数据）</td>
<td style="text-align:left"><code>z = x.view(3, 4)</code></td>
</tr>
<tr>
<td style="text-align:left"><code>x.reshape(shape)</code></td>
<td style="text-align:left">类似于 <code>view</code>，但更灵活</td>
<td style="text-align:left"><code>z = x.reshape(3, 4)</code></td>
</tr>
<tr>
<td style="text-align:left"><code>x.t()</code></td>
<td style="text-align:left">转置矩阵</td>
<td style="text-align:left"><code>z = x.t()</code></td>
</tr>
<tr>
<td style="text-align:left"><code>x.unsqueeze(dim)</code></td>
<td style="text-align:left">在指定维度添加一个维度</td>
<td style="text-align:left"><code>z = x.unsqueeze(0)</code></td>
</tr>
<tr>
<td style="text-align:left"><code>x.squeeze(dim)</code></td>
<td style="text-align:left">去掉指定维度为 1 的维度</td>
<td style="text-align:left"><code>z = x.squeeze(0)</code></td>
</tr>
<tr>
<td style="text-align:left"><code>torch.cat((x, y), dim)</code></td>
<td style="text-align:left">按指定维度连接多个张量</td>
<td style="text-align:left"><code>z = torch.cat((x, y), dim=1)</code></td>
</tr>
<tr>
<td style="text-align:left"><code>x.flatten()</code></td>
<td style="text-align:left">将张量展平成一维</td>
<td style="text-align:left"><code>z = x.flatten()</code></td>
</tr>
</tbody>
</table>
</div>
<p>同样地，这里只介绍<code>unsqueez</code>，<code>squeeze</code> 和 <code>cat</code></p>
<p>先看个示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">a = torch.tensor([[<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>]])</span><br><span class="line">b = torch.tensor([[<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>]])</span><br><span class="line"></span><br><span class="line">c = torch.cat((a, b), dim=<span class="number">0</span>)</span><br><span class="line">d = torch.cat((a, b), dim=<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(c)</span><br><span class="line"><span class="built_in">print</span>(d)</span><br><span class="line"><span class="comment">#tensor([[1.0000, 2.0000, 3.0000],</span></span><br><span class="line"><span class="comment">#       [0.5000, 0.5000, 0.5000]])</span></span><br><span class="line"><span class="comment">#tensor([[1.0000, 2.0000, 3.0000, 0.5000, 0.5000, 0.5000]])</span></span><br></pre></td></tr></table></figure>
<p><code>dim=0</code>依旧是行方向，然后<code>dim=1</code>是列方向，用cat连接之后可以发现，c变为2行，而d变为6列</p>
<p>然后就是<code>squeeze</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">c = torch.randn(<span class="number">1</span>, <span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">e = c.squeeze(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(c)</span><br><span class="line"><span class="built_in">print</span>(e)</span><br><span class="line"></span><br><span class="line"><span class="comment">#tensor([[[-0.9388,  0.3443,  0.7916, -1.5823],</span></span><br><span class="line"><span class="comment">#         [-1.8765, -0.2879,  0.8101,  0.2462],</span></span><br><span class="line"><span class="comment">#         [-1.5106,  0.7375, -0.7615,  0.1300]]])</span></span><br><span class="line"><span class="comment">#tensor([[-0.9388,  0.3443,  0.7916, -1.5823],</span></span><br><span class="line"><span class="comment">#        [-1.8765, -0.2879,  0.8101,  0.2462],</span></span><br><span class="line"><span class="comment">#        [-1.5106,  0.7375, -0.7615,  0.1300]])</span></span><br></pre></td></tr></table></figure>
<p>很明显的可以看到，e被删去了0维（此时dim=0的大小是1，也就是c只有1层）</p>
<p>那么的，<code>unsqueeze</code> 同理</p>
<h3 id="GPU加速"><a href="#GPU加速" class="headerlink" title="GPU加速"></a>GPU加速</h3><p>将张量转移到 GPU：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">device = torch.device(<span class="string">&#x27;cuda&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line">x = torch.tensor([<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>], device=device)</span><br></pre></td></tr></table></figure>
<p>检查 GPU 是否可用：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.cuda.is_available()  <span class="comment"># 返回 True 或 False</span></span><br></pre></td></tr></table></figure>
<h3 id="张量与Numpy互相操作"><a href="#张量与Numpy互相操作" class="headerlink" title="张量与Numpy互相操作"></a>张量与Numpy互相操作</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">a = np.array([[<span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">2.0</span>, <span class="number">3.0</span>]])</span><br><span class="line">b = torch.from_numpy(a)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"><span class="built_in">print</span>(b)</span><br><span class="line"></span><br><span class="line"><span class="comment">#[[1. 2.]</span></span><br><span class="line"><span class="comment"># [2. 3.]]</span></span><br><span class="line"><span class="comment">#tensor([[1., 2.],</span></span><br><span class="line"><span class="comment">#        [2., 3.]], dtype=torch.float64)</span></span><br></pre></td></tr></table></figure>
<h2 id="神经网络基础"><a href="#神经网络基础" class="headerlink" title="神经网络基础"></a>神经网络基础</h2><p>此节点只做基本介绍，后面会有详细的单独介绍（公式部分也仅做了解）</p>
<h3 id="神经元（Neuron）"><a href="#神经元（Neuron）" class="headerlink" title="神经元（Neuron）"></a>神经元（Neuron）</h3><p>神经元是神经网络的基本单元，它接收输入信号，通过加权求和后与偏置（bias）相加，然后通过激活函数处理以产生输出</p>
<p>神经元的权重和偏置是网络学习过程中需要调整的参数，其公式如下：</p>
<script type="math/tex; mode=display">
z = \sum_{i=1}^{n} w_ix_i + Bias</script><p>其中w表示权重，x表示输入，Bias表示偏置</p>
<p>神经元接收多个输入（例如x1, x2, …, xn），如果输入的加权和大于激活阈值（activation potential），则产生二进制输出：</p>
<p><img src="/img/lazy.gif" data-original="1.png" alt=""></p>
<h3 id="层（Layer）"><a href="#层（Layer）" class="headerlink" title="层（Layer）"></a>层（Layer）</h3><p>输入层和输出层之间的层被称为隐藏层，层与层之间的连接密度和类型构成了网络的配置</p>
<p>神经网络由多个层组成，包括：</p>
<ul>
<li><strong>输入层（Input Layer）</strong>：接收原始输入数据</li>
<li><strong>隐藏层（Hidden Layer）</strong>：对输入数据进行处理，可以有多个隐藏层</li>
<li><strong>输出层（Output Layer）</strong>：产生最终的输出结果</li>
</ul>
<p>经典神经网络如下图：</p>
<p><img src="/img/lazy.gif" data-original="2.png" alt=""></p>
<p>这也是前馈神经网络的基本结构</p>
<h3 id="前馈神经网络（FNN）"><a href="#前馈神经网络（FNN）" class="headerlink" title="前馈神经网络（FNN）"></a>前馈神经网络（FNN）</h3><p>前馈神经网络（Feedforward Neural Network，FNN）是神经网络家族中的基本单元</p>
<p>前馈神经网络特点是数据从输入层开始，经过一个或多个隐藏层，最后到达输出层，全过程没有循环或反馈</p>
<p><strong>前馈神经网络的基本结构：</strong></p>
<ul>
<li><strong>输入层：</strong> 数据进入网络的入口点。输入层的每个节点代表一个输入特征</li>
<li><strong>隐藏层：</strong>一个或多个层，用于捕获数据的非线性特征。每个隐藏层由多个神经元组成，每个神经元通过激活函数增加非线性能力</li>
<li><strong>输出层：</strong>输出网络的预测结果。节点数和问题类型相关，例如分类问题的输出节点数等于类别数</li>
<li><strong>连接权重与偏置：</strong>每个神经元的输入通过权重进行加权求和，并加上偏置值，然后通过激活函数传递</li>
</ul>
<h3 id="循环神经网络（RNN）"><a href="#循环神经网络（RNN）" class="headerlink" title="循环神经网络（RNN）"></a>循环神经网络（RNN）</h3><p>循环神经网络（Recurrent Neural Network, RNN）络是一类专门处理序列数据的神经网络，能够捕获输入数据中时间或顺序信息的依赖关系</p>
<p>RNN 的特别之处在于它具有”记忆能力”，可以在网络的隐藏状态中保存之前时间步的信息</p>
<p>循环神经网络用于处理随时间变化的数据模式</p>
<p>在 RNN 中，相同的层被用来接收输入参数，并在指定的神经网络中显示输出参数</p>
<p><img src="/img/lazy.gif" data-original="3.png" alt=""></p>
<h3 id="卷积神经网络（CNN）"><a href="#卷积神经网络（CNN）" class="headerlink" title="卷积神经网络（CNN）"></a>卷积神经网络（CNN）</h3><p><strong>卷积神经网络</strong>（Convolutional Neural Network, CNN）是一种专门用于处理<strong>具有网格结构数据</strong>（如图像、视频、语音频谱图）的深度学习模型，通过<strong>局部连接、权值共享和池化</strong>等机制，自动学习空间层次特征，在计算机视觉任务中取得了巨大成功</p>
<p><img src="/img/lazy.gif" data-original="4.png" alt=""></p>
<h3 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h3><p>在 PyTorch 中，构建神经网络通常需要继承 nn.Module 类</p>
<p>nn.Module 是所有神经网络模块的基类，你需要定义以下两个部分：</p>
<ul>
<li><strong><code>__init__()</code></strong>：定义网络层</li>
<li><strong><code>forward()</code></strong>：定义数据的前向传播过程</li>
</ul>
<p>简单的全连接神经网络（Fully Connected Network）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SimpleNN</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(SimpleNN, <span class="variable language_">self</span>).__init__()  </span><br><span class="line">        <span class="comment"># 定义一个输入层到隐藏层的全连接层</span></span><br><span class="line">        <span class="variable language_">self</span>.fc1 = nn.Linear(<span class="number">2</span>, <span class="number">2</span>)      <span class="comment"># 输入两个特征，输出两个特征</span></span><br><span class="line">        <span class="variable language_">self</span>.fc2 = nn.Linear(<span class="number">2</span>, <span class="number">1</span>)      <span class="comment"># 输入两个特征，输出一个特征</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = torch.relu(<span class="variable language_">self</span>.fc1(x))     <span class="comment"># 使用relu激活函数</span></span><br><span class="line">        x = <span class="variable language_">self</span>.fc2(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    </span><br><span class="line">model = SimpleNN()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(model)</span><br><span class="line"></span><br><span class="line"><span class="comment">#SimpleNN(</span></span><br><span class="line"><span class="comment">#  (fc1): Linear(in_features=2, out_features=2, bias=True)</span></span><br><span class="line"><span class="comment">#  (fc2): Linear(in_features=2, out_features=1, bias=True)</span></span><br><span class="line"><span class="comment">#)</span></span><br></pre></td></tr></table></figure>
<p>初次看到可能有些地方会不理解，下面介绍几个：</p>
<p><code>super(SimpleNN, self).__init__()</code>  <strong>调用父类的初始化方法</strong></p>
<p><code>relu</code> 是最常用的激活函数之一，其定义为：</p>
<script type="math/tex; mode=display">
Relu(x) = \max(0,x)</script><p>relu计算简单，只需判断正负，而且正区梯度恒为1，利于训练</p>
<p>PyTorch 提供了许多常见的神经网络层，以下是几个常见的：</p>
<ul>
<li><strong><code>nn.Linear(in_features, out_features)</code></strong>：全连接层，输入 <code>in_features</code> 个特征，输出 <code>out_features</code> 个特征</li>
<li><strong><code>nn.Conv2d(in_channels, out_channels, kernel_size)</code></strong>：2D 卷积层，用于图像处理</li>
<li><strong><code>nn.MaxPool2d(kernel_size)</code></strong>：2D 最大池化层，用于降维</li>
<li><strong><code>nn.ReLU()</code></strong>：ReLU 激活函数，常用于隐藏层</li>
<li><strong><code>nn.Softmax(dim)</code></strong>：Softmax 激活函数，通常用于输出层，适用于多类分类问</li>
</ul>
<h3 id="激活函数（Activation-Function）"><a href="#激活函数（Activation-Function）" class="headerlink" title="激活函数（Activation Function）"></a>激活函数（Activation Function）</h3><p>激活函数决定了神经元是否应该被激活。它们是非线性函数，使得神经网络能够学习和执行更复杂的任务。常见的激活函数包括：</p>
<p><strong>Sigmoid</strong>： 用于二分类问题，输出值在 0 和 1 之间</p>
<script type="math/tex; mode=display">
\sigma(x) = \frac{1}{1 + e^{-x}}</script><p><strong>Tanh：</strong> 输出值在 -1 和 1 之间，常用于输出层之前</p>
<script type="math/tex; mode=display">
\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}</script><p><strong>ReLU（Rectified Linear Unit）：</strong>目前最流行的激活函数之一，定义为 <code>f(x) = max(0, x)</code>，有助于解决梯度消失问题</p>
<p><strong>Softmax： </strong>常用于多分类问题的输出层，将输出转换为概率分布</p>
<p>各激活方式如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn.fuctional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">output1 = F.relu(<span class="built_in">input</span>)</span><br><span class="line"></span><br><span class="line">output2 = torch.sigmoid(<span class="built_in">input</span>)</span><br><span class="line"></span><br><span class="line">output3 = torch.tanh(<span class="built_in">input</span>)</span><br><span class="line"></span><br><span class="line">output4 = F.softmax(<span class="built_in">input</span>, dim=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<h3 id="损失函数（Loss-Function）"><a href="#损失函数（Loss-Function）" class="headerlink" title="损失函数（Loss Function）"></a>损失函数（Loss Function）</h3><p>logit（对数几率）是<strong>未经过归一化或激活函数</strong>（如 Softmax、Sigmoid）处理的线性输出，其表达式如下：</p>
<script type="math/tex; mode=display">
logit(p) = log(\frac{p}{1-p})</script><p>损失函数用于衡量模型的预测值与真实值之间的差异</p>
<p>常见的损失函数包括：</p>
<p><strong>均方误差（MSELoss）</strong>：回归问题常用，计算输出与目标值的平方差</p>
<script type="math/tex; mode=display">
\mathcal{L}_{\text{MSE}} = \frac{1}{N} \sum_{i=1}^{N} (y_i - \hat{y}_i)^2</script><p><strong>交叉熵损失（CrossEntropyLoss）</strong>：分类问题常用，计算输出和真实标签之间的交叉熵</p>
<script type="math/tex; mode=display">
\mathcal{L}_{\text{CE}} = \frac{1}{N} \sum_{i=1}^{N} \left( -z_{i, y_i} + \log \left( \sum_{j=1}^{C} e^{z_{i,j}} \right) \right)</script><script type="math/tex; mode=display">
C:类别总数</script><script type="math/tex; mode=display">
z_i,_y:第i个样本在正式类别y_i上的logit值</script><p><strong>BCEWithLogitsLoss</strong>：二分类问题，结合了 Sigmoid 激活和二元交叉熵损失</p>
<script type="math/tex; mode=display">
\mathcal{L} = \frac{1}{N \cdot H} \sum_{i=1}^{N} \sum_{k=1}^{H} \left[ -y_{i,k} \cdot z_{i,k} + \log(1 + e^{z_{i,k}}) \right]</script><script type="math/tex; mode=display">
H : 输出维度（二分类时 H=1 ，多标签时H>1 ）</script><script type="math/tex; mode=display">
y_i,_k∈[0,1] 真实标签（可为软标签）</script><script type="math/tex; mode=display">
z_i,_k : logits</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 均方误差损失</span></span><br><span class="line">criterion = nn.MSELoss()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 交叉熵损失</span></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 二分类交叉熵损失</span></span><br><span class="line">criterion = nn.BCEWithLogitsLoss()</span><br></pre></td></tr></table></figure>
<h3 id="优化器（Optimizer）"><a href="#优化器（Optimizer）" class="headerlink" title="优化器（Optimizer）"></a>优化器（Optimizer）</h3><p>优化器负责在训练过程中更新网络的权重和偏置</p>
<p>常见的优化器包括：</p>
<p><strong>SGD（随机梯度下降）</strong></p>
<script type="math/tex; mode=display">
\theta_{t+1} = \theta_t - \eta \nabla_\theta \mathcal{L}(\theta_t)</script><script type="math/tex; mode=display">
θ 
t
​
  ：第 t 步的模型参数</script><script type="math/tex; mode=display">
η ：学习率（learning rate）</script><script type="math/tex; mode=display">
∇_θL(θ_t) ：损失函数 L 对 θ 的梯度（基于当前 batch）</script><p><strong>Adam（自适应矩估计）</strong></p>
<script type="math/tex; mode=display">
\begin{aligned}
s_{t+1} &= \beta s_t + (1 - \beta) \left( \nabla_\theta \mathcal{L}(\theta_t) \right)^2 \\
\theta_{t+1} &= \theta_t - \frac{\eta}{\sqrt{s_{t+1} + \epsilon}} \nabla_\theta \mathcal{L}(\theta_t)
\end{aligned}</script><script type="math/tex; mode=display">
s_t
  ：梯度平方的指数移动平均（二阶矩估计）</script><script type="math/tex; mode=display">
β ：衰减率（通常取 0.9）</script><script type="math/tex; mode=display">
ϵ ：小常数（如 10^{−8}），防止除零</script><p><strong>RMSprop（均方根传播）</strong></p>
<script type="math/tex; mode=display">
\begin{aligned}
g_t &= \nabla_\theta \mathcal{L}(\theta_t) \\
m_t &= \beta_1 m_{t-1} + (1 - \beta_1) g_t \\
s_t &= \beta_2 s_{t-1} + (1 - \beta_2) g_t^2 \\
\hat{m}_t &= \frac{m_t}{1 - \beta_1^t} \\
\hat{s}_t &= \frac{s_t}{1 - \beta_2^t} \\
\theta_{t+1} &= \theta_t - \frac{\eta}{\sqrt{\hat{s}_t} + \epsilon} \hat{m}_t
\end{aligned}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用 SGD 优化器</span></span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用 Adam 优化器</span></span><br><span class="line">optimizer = optim.Adam(model.parameters(), lr=<span class="number">0.001</span>)</span><br></pre></td></tr></table></figure>
<h3 id="训练过程（Training-Process）"><a href="#训练过程（Training-Process）" class="headerlink" title="训练过程（Training Process）"></a>训练过程（Training Process）</h3><p>训练神经网络涉及以下步骤：</p>
<ol>
<li><strong>准备数据</strong>：通过 <code>DataLoader</code> 加载数据</li>
<li><strong>定义损失函数和优化器</strong></li>
<li><strong>前向传播</strong>：计算模型的输出</li>
<li><strong>计算损失</strong>：与目标进行比较，得到损失值</li>
<li><strong>反向传播</strong>：通过 <code>loss.backward()</code> 计算梯度</li>
<li><strong>更新参数</strong>：通过 <code>optimizer.step()</code> 更新模型的参数</li>
<li><strong>重复上述步骤</strong>，直到达到预定的训练轮数</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 假设已经定义好了模型、损失函数和优化器</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练数据示例</span></span><br><span class="line">X = torch.randn(<span class="number">10</span>, <span class="number">2</span>) <span class="comment"># 10 个样本，每个样本有 2 个特征</span></span><br><span class="line">Y = torch.randn(<span class="number">10</span>, <span class="number">1</span>) <span class="comment"># 10 个目标标签</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练过程</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):  <span class="comment"># 训练 100 轮</span></span><br><span class="line">  model.train() <span class="comment"># 设置模型为训练模式</span></span><br><span class="line">  optimizer.zero_grad() <span class="comment"># 清除梯度</span></span><br><span class="line">  output = model(X) <span class="comment"># 前向传播</span></span><br><span class="line">  loss = criterion(output, Y) <span class="comment"># 计算损失</span></span><br><span class="line">  loss.backward() <span class="comment"># 反向传播</span></span><br><span class="line">  optimizer.step() <span class="comment"># 更新权重</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (epoch + <span class="number">1</span>) % <span class="number">10</span> == <span class="number">0</span>:  <span class="comment"># 每 10 轮输出一次损失</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;Epoch [<span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>/100], Loss: <span class="subst">&#123;loss.item():<span class="number">.4</span>f&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h2 id="第一个神经网络"><a href="#第一个神经网络" class="headerlink" title="第一个神经网络"></a>第一个神经网络</h2><h3 id="实例1"><a href="#实例1" class="headerlink" title="实例1"></a>实例1</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义输入层大小、隐藏层大小、输出层大小和批量大小</span></span><br><span class="line">inp, hid, outp, size = <span class="number">10</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">10</span></span><br><span class="line"></span><br><span class="line">x = torch.randn(size, inp)</span><br><span class="line">y = torch.tensor([[<span class="number">1.0</span>], [<span class="number">0.0</span>], [<span class="number">0.0</span>],</span><br><span class="line">                 [<span class="number">1.0</span>], [<span class="number">1.0</span>], [<span class="number">1.0</span>], [<span class="number">0.0</span>], [<span class="number">0.0</span>], [<span class="number">1.0</span>], [<span class="number">1.0</span>]])  <span class="comment"># 目标数据</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建顺序模型，包含线性层、ReLU激活函数和Sigmoid激活函数</span></span><br><span class="line">model = nn.Sequential(</span><br><span class="line">    nn.Linear(inp, hid),        <span class="comment"># 输入层 -&gt; 隐藏层的线性变化</span></span><br><span class="line">    nn.ReLU(),                  <span class="comment"># 隐藏层的激活函数</span></span><br><span class="line">    nn.Linear(hid, outp),       <span class="comment"># 隐藏层 -&gt; 输出层的线性变化</span></span><br><span class="line">    nn.Sigmoid()                <span class="comment"># 输出层的激活函数</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义均方误差损失函数和随机梯度下降优化器</span></span><br><span class="line">criterion = nn.MSELoss()</span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">0.01</span>) <span class="comment">#学习率为0.01</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 执行梯度下降算法进行模型训练</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">50</span>):</span><br><span class="line">    y_pred = model(x)</span><br><span class="line">    loss = criterion(y_pred, y)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;epoch: &quot;</span>, epoch, <span class="string">&quot; loss: &quot;</span>, loss)</span><br><span class="line"></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br></pre></td></tr></table></figure>
<ul>
<li><code>inp</code>：输入层大小为 10，即每个数据点有 10 个特征</li>
<li><code>hid</code>：隐藏层大小为 5，即隐藏层包含 5 个神经元</li>
<li><code>outp</code>：输出层大小为 1，即输出一个标量，表示二分类结果（0 或 1）</li>
<li><code>size</code>：每个批次包含 10 个样本</li>
</ul>
<p><code>nn.Sequential</code> 用于按顺序定义网络层</p>
<ul>
<li><code>nn.Linear(inp, hid)</code>：定义输入层到隐藏层的线性变换，输入特征是 10 个，隐藏层有 5 个神经元</li>
<li><code>nn.ReLU()</code>：在隐藏层后添加 ReLU 激活函数，增加非线性</li>
<li><code>nn.Linear(hid, outp)</code>：定义隐藏层到输出层的线性变换，输出为 1 个神经元</li>
<li><code>nn.Sigmoid()</code>：输出层使用 Sigmoid 激活函数，将结果映射到 0 到 1 之间，用于二分类任务</li>
</ul>
<h3 id="梯度下降算法"><a href="#梯度下降算法" class="headerlink" title="梯度下降算法"></a><strong>梯度下降算法</strong></h3><p>假设我们有一个可微的损失函数 <script type="math/tex">L(θ)</script> ，其中 <em>θ</em> 是模型的参数（例如神经网络的权重），我们希望找到使 <script type="math/tex">L(θ)</script> 最小的参数值 <em>θ</em></p>
<p>梯度下降利用这样一个事实：<strong>函数在某点处的梯度方向是函数值增长最快的方向</strong>。因此，<strong>负梯度方向就是函数值下降最快的方向</strong></p>
<p>于是，我们迭代地更新参数：</p>
<script type="math/tex; mode=display">
θ ← θ - η∇_θL(θ)</script><p>其中：</p>
<script type="math/tex; mode=display">
∇_θL(θ) 是损失函数对参数的梯度（偏导数组成的向量）</script><script type="math/tex; mode=display">
η>0 是学习率，控制每次更新的步长</script><p>那么问题来了，现在我们找到了下降最快的方向时候，需要做什么？</p>
<p>答案就是，往下降最快的地方迈一步，这样的话迈完之后可以继续找下一个最小的地方，其中，迈多少就是步长，也就是学习率(lr)，如果步子太大（学习率过高）：可能一步跨过最低点，甚至越走越高（发散），如果步子太小（学习率过低）：收敛太慢，训练效率低，所以，我们更新公式如下：</p>
<script type="math/tex; mode=display">
θ_{new} = θ_{old} - η∇_θL(θ_{old})</script><p>比如上面代码所写的那样：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">optimizer.zero_grad()</span><br><span class="line">loss.backward()</span><br><span class="line">optimizer.step()</span><br></pre></td></tr></table></figure>
<p>先清空旧梯度，然后计算当前梯度（下降最快的方向），然后沿着负梯度方向走一步，那会不会一直走下去呢？答案是不会，当下面几种情况下停止：</p>
<ul>
<li>损失函数变化非常小（收敛）</li>
<li>达到最大训练轮数（epochs）</li>
<li>验证集性能不再提升（防止过拟合）</li>
</ul>
<p>下面举个例子：</p>
<script type="math/tex; mode=display">
假设损失函数是 \\L(w)=w^2 ，当前w=3</script><script type="math/tex; mode=display">
计算梯度：\frac{dw}{dL}=2w=6 → 上升最快方向是 +6</script><script type="math/tex; mode=display">
所以下降最快方向是 -6</script><script type="math/tex; mode=display">
设学习率 η=0.1 ，则更新：

w_{new}=3−0.1×6=2.4</script><script type="math/tex; mode=display">
下一轮再算梯度：2×2.4=4.8 ，继续更新……</script><script type="math/tex; mode=display">
最终 w→0 ，损失 →0 ，达到最小值</script><p>Q1：为什么 “最快下降方向” ≠ “直达最低点”</p>
<p>A1：因为 <strong>梯度只反映局部信息</strong>（一阶导数），它假设函数在附近是线性的。但实际损失函数可能是弯曲的（非线性）。所以你只能“走一小步”，然后重新评估方向——这正是迭代优化的本质</p>
<h3 id="实例2"><a href="#实例2" class="headerlink" title="实例2"></a>实例2</h3><p>你也可以自定义一个神经网络：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line">n_samples = <span class="number">100</span></span><br><span class="line">data = torch.randn(n_samples, <span class="number">2</span>)</span><br><span class="line">labels = (data[:, <span class="number">0</span>]**<span class="number">2</span> + data[:, <span class="number">1</span>]**<span class="number">2</span> &lt; <span class="number">1</span>).<span class="built_in">float</span>().unsqueeze(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#定义神经网络</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SimpleNN</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(SimpleNN, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.fc1 = nn.Linear(<span class="number">2</span>, <span class="number">4</span>)</span><br><span class="line">        <span class="variable language_">self</span>.fc2 = nn.Linear(<span class="number">4</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="variable language_">self</span>.sigmoid = nn.Sigmoid()  <span class="comment"># 二分类激活函数</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = torch.relu(<span class="variable language_">self</span>.fc1(x))    <span class="comment"># 输入层用relu激活函数</span></span><br><span class="line">        x = torch.sigmoid(<span class="variable language_">self</span>.fc2(x)) <span class="comment"># 输出层用sigmoid激活函数</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    </span><br><span class="line">model = SimpleNN()</span><br><span class="line"></span><br><span class="line"><span class="comment">#定义损失函数和优化器</span></span><br><span class="line">criterion = nn.BCELoss()  <span class="comment"># 二元交叉熵损失</span></span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">0.1</span>) <span class="comment"># 随机梯度下降熵优化器</span></span><br><span class="line"></span><br><span class="line">epochs = <span class="number">100</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    outputs = model(data)</span><br><span class="line">    loss = criterion(outputs, labels)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#反向传播</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (epoch + <span class="number">1</span>) % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;Epoch [<span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>/<span class="subst">&#123;epochs&#125;</span>], Loss: <span class="subst">&#123;loss.item():<span class="number">.4</span>f&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h2 id="数据处理与加载"><a href="#数据处理与加载" class="headerlink" title="数据处理与加载"></a>数据处理与加载</h2><p>PyTorch 数据处理与加载的介绍：</p>
<ul>
<li><strong>自定义 Dataset</strong>：通过继承 <code>torch.utils.data.Dataset</code> 来加载自己的数据集</li>
<li><strong>DataLoader</strong>：<code>DataLoader</code> 按批次加载数据，支持多线程加载并进行数据打乱</li>
<li><strong>数据预处理与增强</strong>：使用 <code>torchvision.transforms</code> 进行常见的图像预处理和增强操作，提高模型的泛化能力</li>
<li><strong>加载标准数据集</strong>：<code>torchvision.datasets</code> 提供了许多常见的数据集，简化了数据加载过程</li>
<li><strong>多个数据源</strong>：通过组合多个 <code>Dataset</code> 实例来处理来自不同来源的数据</li>
</ul>
<h3 id="自定义Dataset"><a href="#自定义Dataset" class="headerlink" title="自定义Dataset"></a>自定义Dataset</h3><p><strong>torch.utils.data.Dataset</strong> 是一个抽象类，允许你从自己的数据源中创建数据集。</p>
<p>我们需要继承该类并实现以下两个方法：</p>
<ul>
<li><code>__len__(self)</code>：返回数据集中的样本数量</li>
<li><code>__getitem__(self, idx)</code>：通过索引返回一个样本</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, X_Data, Y_Data</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        初始化数据 X_data 和 Y_data 是两个列表或数组</span></span><br><span class="line"><span class="string">        X_data: 输入特征</span></span><br><span class="line"><span class="string">        Y_data: 目标标签</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="variable language_">self</span>.X_Data = X_Data</span><br><span class="line">        <span class="variable language_">self</span>.Y_Data = Y_Data</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(<span class="variable language_">self</span>.X_Data)   <span class="comment"># 返回数据集大小</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, index</span>):</span><br><span class="line">        <span class="comment">#返回指定的索引数据</span></span><br><span class="line">        x = torch.tensor(<span class="variable language_">self</span>.X_Data[index], dtype=torch.float32)</span><br><span class="line">        y = torch.tensor(<span class="variable language_">self</span>.Y_Data[index], dtype=torch.float32)</span><br><span class="line">        <span class="keyword">return</span> x, y</span><br><span class="line"></span><br><span class="line">X_Data = [[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>], [<span class="number">5</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>]] <span class="comment"># 输入特征</span></span><br><span class="line">Y_Data = [<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]  <span class="comment"># 输出特征</span></span><br><span class="line"></span><br><span class="line">dataset = MyDataset(X_Data, Y_Data)</span><br><span class="line"></span><br><span class="line"><span class="comment">#打印数据</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(dataset)):</span><br><span class="line">    <span class="built_in">print</span>(dataset[i])</span><br><span class="line">    </span><br><span class="line"><span class="comment">#(tensor([1., 2.]), tensor(1.))</span></span><br><span class="line"><span class="comment">#(tensor([3., 4.]), tensor(0.))</span></span><br><span class="line"><span class="comment">#(tensor([5., 6.]), tensor(1.))</span></span><br><span class="line"><span class="comment">#(tensor([7., 8.]), tensor(0.))</span></span><br></pre></td></tr></table></figure>
<h3 id="DataLoader加载数据"><a href="#DataLoader加载数据" class="headerlink" title="DataLoader加载数据"></a>DataLoader加载数据</h3><p><code>DataLoader</code> 用于在pytorch中从 Dataset 中按批次（batch）加载数据</p>
<p>以上面的Dataset为例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset, DataLoader</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, X_Data, Y_Data</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        初始化数据 X_data 和 Y_data 是两个列表或数组</span></span><br><span class="line"><span class="string">        X_data: 输入特征</span></span><br><span class="line"><span class="string">        Y_data: 目标标签</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="variable language_">self</span>.X_Data = X_Data</span><br><span class="line">        <span class="variable language_">self</span>.Y_Data = Y_Data</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(<span class="variable language_">self</span>.X_Data)   <span class="comment"># 返回数据集大小</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, index</span>):</span><br><span class="line">        <span class="comment">#返回指定的索引数据</span></span><br><span class="line">        x = torch.tensor(<span class="variable language_">self</span>.X_Data[index], dtype=torch.float32)</span><br><span class="line">        y = torch.tensor(<span class="variable language_">self</span>.Y_Data[index], dtype=torch.float32)</span><br><span class="line">        <span class="keyword">return</span> x, y</span><br><span class="line"></span><br><span class="line">X_Data = [[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>], [<span class="number">5</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>]] <span class="comment"># 输入特征</span></span><br><span class="line">Y_Data = [<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]  <span class="comment"># 输出特征</span></span><br><span class="line"></span><br><span class="line">dataset = MyDataset(X_Data, Y_Data)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(dataset)):</span><br><span class="line">    <span class="built_in">print</span>(dataset[i])</span><br><span class="line"></span><br><span class="line">dataLoader = DataLoader(dataset, batch_size=<span class="number">2</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>):</span><br><span class="line">    <span class="keyword">for</span> index, (inputs, labels) <span class="keyword">in</span> <span class="built_in">enumerate</span>(dataLoader):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;Batch <span class="subst">&#123;index + <span class="number">1</span>&#125;</span>:&#x27;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;Inputs: <span class="subst">&#123;inputs&#125;</span>&#x27;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;Labels: <span class="subst">&#123;labels&#125;</span>&#x27;</span>)</span><br><span class="line">        </span><br><span class="line"><span class="comment">#(tensor([1., 2.]), tensor(1.))</span></span><br><span class="line"><span class="comment">#(tensor([3., 4.]), tensor(0.))</span></span><br><span class="line"><span class="comment">#(tensor([5., 6.]), tensor(1.))</span></span><br><span class="line"><span class="comment">#(tensor([7., 8.]), tensor(0.))</span></span><br><span class="line"><span class="comment">#Batch 1:</span></span><br><span class="line"><span class="comment">#Inputs: tensor([[1., 2.],</span></span><br><span class="line"><span class="comment">#        [5., 6.]])</span></span><br><span class="line"><span class="comment">#Labels: tensor([1., 1.])</span></span><br><span class="line"><span class="comment">#Batch 2:</span></span><br><span class="line"><span class="comment">#Inputs: tensor([[7., 8.],</span></span><br><span class="line"><span class="comment">#        [3., 4.]])</span></span><br><span class="line"><span class="comment">#Labels: tensor([0., 0.])</span></span><br></pre></td></tr></table></figure>
<ul>
<li><strong>batch_size</strong> 为每次加载的样本量</li>
<li><strong>shuffle</strong> 意为洗牌，也就是将数据打乱（通常训练时都需要打乱）</li>
<li><strong>drop_last</strong> 如果数据集中的样本数不能被 <code>batch_size</code> 整除，设置为 <code>True</code> 时，丢弃最后一个不完整的 batch </li>
</ul>
<p>每次循环都会返回每个批次与每个批次所包含的输入特征和目标标签</p>
<h3 id="预处理和数据增强"><a href="#预处理和数据增强" class="headerlink" title="预处理和数据增强"></a>预处理和数据增强</h3><p><code>torchvision.transforms</code> 模块来进行常见的图像预处理和增强操作，如旋转、裁剪、归一化等</p>
<p>常见预处理如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"></span><br><span class="line"><span class="comment">#定义数据预处理的流水线</span></span><br><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.Resize((<span class="number">128</span>, <span class="number">128</span>)),  <span class="comment"># 将图像调整为 128x128</span></span><br><span class="line">    transforms.ToTensor(),  <span class="comment"># 将图像转换为张量</span></span><br><span class="line">    transforms.Normalize(mean=[<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], std=[<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])  <span class="comment"># 标准化</span></span><br><span class="line">])</span><br><span class="line">image = Image.<span class="built_in">open</span>(<span class="string">&#x27;image.jpg&#x27;</span>)</span><br><span class="line"></span><br><span class="line">image_tensor = transform(image)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(image_tensor.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment">#torch.Size([3, 128, 128])</span></span><br></pre></td></tr></table></figure>
<ul>
<li><strong>transforms.Compose()</strong>：将多个变换操作组合在一起</li>
<li><strong>transforms.Resize()</strong>：调整图像大小</li>
<li><strong>transforms.ToTensor()</strong>：将图像转换为 PyTorch 张量，值会被归一化到 <code>[0, 1]</code> 范围</li>
<li><strong>transforms.Normalize()</strong>：标准化图像数据，通常使用预训练模型时需要进行标准化处理</li>
</ul>
<p>另外，transforms也提供了图片增强的功能</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.RandomHorizontalFlip(),  <span class="comment"># 随机水平翻转</span></span><br><span class="line">    transforms.RandomRotation(<span class="number">30</span>),  <span class="comment"># 随机旋转 30 度</span></span><br><span class="line">    transforms.RandomResizedCrop(<span class="number">128</span>),  <span class="comment"># 随机裁剪并调整为 128x128</span></span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize(mean=[<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], std=[<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])</span><br><span class="line">])</span><br></pre></td></tr></table></figure>
<p>其中这里的mean和std分别为RGB 三个通道的<strong>经验均值（mean）和标准差（std）</strong>，在Normalize()中分别在R，G，B三种通道中做以下操作：</p>
<script type="math/tex; mode=display">
output = \frac{input - mean}{std}</script><ul>
<li><code>mean = [0.485, 0.456, 0.406]</code> 对应 <strong>R、G、B 通道的平均像素值</strong>（归一化到 [0,1] 后的均值）</li>
<li><code>std = [0.229, 0.224, 0.225]</code> 对应 <strong>R、G、B 通道的标准差</strong></li>
</ul>
<h3 id="加载图像数据集"><a href="#加载图像数据集" class="headerlink" title="加载图像数据集"></a>加载图像数据集</h3><p>对于图像数据集，torchvision.datasets 提供了许多常见数据集（如 CIFAR-10、ImageNet、MNIST 等）以及用于加载图像数据的工具</p>
<p>加载 MNIST 数据集:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision.datasets <span class="keyword">as</span> datasets</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义预处理操作</span></span><br><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize((<span class="number">0.5</span>,), (<span class="number">0.5</span>,))  <span class="comment"># 对灰度图像进行标准化</span></span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 下载并加载 MNIST 数据集</span></span><br><span class="line">train_dataset = datasets.MNIST(root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>, transform=transform)</span><br><span class="line">test_dataset = datasets.MNIST(root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">False</span>, download=<span class="literal">True</span>, transform=transform)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建 DataLoader</span></span><br><span class="line">train_loader = DataLoader(train_dataset, batch_size=<span class="number">64</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line">test_loader = DataLoader(test_dataset, batch_size=<span class="number">64</span>, shuffle=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 迭代训练数据</span></span><br><span class="line"><span class="keyword">for</span> inputs, labels <span class="keyword">in</span> train_loader:</span><br><span class="line">    <span class="built_in">print</span>(inputs.shape)  <span class="comment"># 每个批次的输入数据形状</span></span><br><span class="line">    <span class="built_in">print</span>(labels.shape)  <span class="comment"># 每个批次的标签形状</span></span><br></pre></td></tr></table></figure>
<ul>
<li><code>datasets.MNIST()</code> 会自动下载 MNIST 数据集并加载</li>
<li><code>transform</code> 参数允许我们对数据进行预处理</li>
<li><code>train=True</code> 和 <code>train=False</code> 分别表示训练集和测试集</li>
</ul>
<h3 id="用多个数据源（Multi-source-Dataset）"><a href="#用多个数据源（Multi-source-Dataset）" class="headerlink" title="用多个数据源（Multi-source Dataset）"></a>用多个数据源（Multi-source Dataset）</h3><p>如果你的数据集由多个文件、多个来源（例如多个图像文件夹）组成，可以通过继承 Dataset 类自定义加载多个数据源</p>
<p>PyTorch 提供了 ConcatDataset 和 ChainDataset 等类来连接多个数据集</p>
<p>例如，假设我们有多个图像文件夹的数据，可以将它们合并为一个数据集</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> ConcatDataset</span><br><span class="line"></span><br><span class="line"><span class="comment"># 假设 dataset1 和 dataset2 是两个 Dataset 对象</span></span><br><span class="line">combined_dataset = ConcatDataset([dataset1, dataset2])</span><br><span class="line">combined_loader = DataLoader(combined_dataset, batch_size=<span class="number">64</span>, shuffle=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<h2 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h2><p>线性回归是最基本的机器学习算法之一，用于预测一个连续值。</p>
<p>线性回归是一种简单且常见的回归分析方法，目的是通过拟合一个线性函数来预测输出。</p>
<p>对于一个简单的线性回归问题，模型可以表示为：</p>
<script type="math/tex; mode=display">
y = w_1x_1+w_2x_2+...+w_nx_n+b</script><p>其中：</p>
<ul>
<li><script type="math/tex">y</script> 是预测值（目标值）</li>
<li><script type="math/tex">x_1,x_2..x_n</script> 是输入特征</li>
<li><script type="math/tex">w_1,w_2...w_n</script> 是每个输入特征所对应的权重（待学习的权重）</li>
<li><script type="math/tex">b</script> 是偏置</li>
</ul>
<p>现使用Pytorch来实现一个简单的线性回归</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机种子，确保每次结果一致</span></span><br><span class="line">torch.manual_seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line">X = torch.randn(<span class="number">100</span> ,<span class="number">2</span>)</span><br><span class="line">true_w = torch.tensor([<span class="number">2.0</span>, <span class="number">3.0</span>])</span><br><span class="line">true_b = <span class="number">4.0</span>     <span class="comment"># 偏置</span></span><br><span class="line">Y = X @ true_w + true_b + torch.randn(<span class="number">100</span>) * <span class="number">0.1</span> <span class="comment"># 加入一点噪声</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(X[:<span class="number">5</span>])</span><br><span class="line"><span class="built_in">print</span>(Y[:<span class="number">5</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment">#tensor([[ 1.9269,  1.4873],</span></span><br><span class="line"><span class="comment">#        [ 0.9007, -2.1055],</span></span><br><span class="line"><span class="comment">#        [ 0.6784, -1.2345],</span></span><br><span class="line"><span class="comment">#        [-0.0431, -1.6047],</span></span><br><span class="line"><span class="comment">#        [-0.7521,  1.6487]])</span></span><br><span class="line"><span class="comment">#tensor([12.4460, -0.4663,  1.7666, -0.9357,  7.4781])</span></span><br></pre></td></tr></table></figure>
<p>其中：</p>
<ul>
<li>@表示的是矩阵乘法运算符，<code>X @ true_w</code>就是对每一行做点积，即<code>X[i, :] @ true_w = X[i, 0] * 2.0 + X[i, 1] * 3.0</code></li>
<li>噪声：<code>torch.randn(100) * 0.1</code> 生成了 100 个服从 <strong>标准正态分布</strong>（均值为 0，标准差为 1）的随机数，再乘以 0.1，变成 <strong>均值为 0、标准差为 0.1 的小扰动</strong></li>
<li>为什么要加入噪声？因为现实中数据存在测量误差、环境干扰等，如果不加入噪声，模型将完全由 <code>X @ true_w + true_b</code> 来决定，会出现百分百拟合，不符合实际</li>
</ul>
<h3 id="定义线性回归模型"><a href="#定义线性回归模型" class="headerlink" title="定义线性回归模型"></a>定义线性回归模型</h3><p>可以通过继承 <code>nn.Module</code> 来定义一个简单的线性回归模型</p>
<p>在 PyTorch 中，线性回归的核心是 <code>nn.Linear()</code> 层，它会自动处理权重和偏置的初始化</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SimpleNN</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(SimpleNN, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.linear = nn.Linear(<span class="number">2</span>, <span class="number">1</span>)  <span class="comment"># 定义线性层，输入两个特征，输出一个特征</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.linear(x)</span><br><span class="line"></span><br><span class="line">model = SimpleNN()</span><br></pre></td></tr></table></figure>
<h3 id="定义损失函数与优化器"><a href="#定义损失函数与优化器" class="headerlink" title="定义损失函数与优化器"></a>定义损失函数与优化器</h3><p>线性回归的常见损失函数是 <strong>均方误差损失（MSELoss）</strong>，用于衡量预测值与真实值之间的差异</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">criterion = nn.MSELoss()</span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">0.01</span>)</span><br></pre></td></tr></table></figure>
<h3 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h3><p>依旧按照前面训练过程，即前向传播 -&gt; 计算损失 -&gt; 反向传播 -&gt; 计算梯度并清空 -&gt; 更新/优化参数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">epochs = <span class="number">1000</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    model.train()</span><br><span class="line">    data = model(X)</span><br><span class="line"></span><br><span class="line">    loss = criterion = (data.squeeze(), Y)  <span class="comment"># 预测值data需要压缩成1维</span></span><br><span class="line"></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (epoch + <span class="number">1</span>) % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;Epoch [<span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>/1000], Loss: <span class="subst">&#123;loss.item():<span class="number">.4</span>f&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>其中：</p>
<ul>
<li>“训练模式”：对于<strong>包含某些特定层</strong>的模型（如 <code>Dropout</code>、<code>BatchNorm</code> 等），PyTorch 需要知道当前是在 <strong>训练</strong> 还是 <strong>推理/评估</strong> 阶段</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th>层类型</th>
<th>训练模式 model.train()</th>
<th>评估模式 model.eval()</th>
</tr>
</thead>
<tbody>
<tr>
<td>Dropout</td>
<td>随机“丢弃”一部分神经元（防止过拟合）</td>
<td><strong>不丢弃</strong>，所有神经元都参与，但权重会缩放</td>
</tr>
<tr>
<td>BathNorm</td>
<td>使用当前 batch 的均值和方差做归一化</td>
<td>使用训练时累计的 <strong>全局均值和方差</strong></td>
</tr>
</tbody>
</table>
</div>
<ul>
<li>不过为什么之前的示例中没用到 <code>train()</code> ？因为对于只包含简单线性层的，其实在 <code>train()</code> 和 <code>eval()</code> 下的工作行为完全一样，所以理论上可以不用写（Dropout 默认处于 <code>eval</code> 模式）</li>
<li>为什么需要压缩成一维？因为目标值Y就是一维</li>
</ul>
<h3 id="评估"><a href="#评估" class="headerlink" title="评估"></a>评估</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Predicted Weight: <span class="subst">&#123;model.linear.weight.data.numpy()&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Predicted Bias: <span class="subst">&#123;model.linear.bias.data.numpy()&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    predictions = model(X)</span><br><span class="line"></span><br><span class="line">plt.scatter(X[:, <span class="number">0</span>], Y, color=<span class="string">&#x27;blue&#x27;</span>, label=<span class="string">&#x27;True values&#x27;</span>)</span><br><span class="line">plt.scatter(X[:, <span class="number">0</span>], predictions, color=<span class="string">&#x27;red&#x27;</span>, label=<span class="string">&#x27;Predictions&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>其中：</p>
<p><code>model.linear.weight.data.numpy()</code> 和 <code>model.linear.bias.data.numpy()</code> 是什么？</p>
<p>假设你的模型定义如下（这是最常见的情况）：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model = torch.nn.Linear(2, 1)</span><br></pre></td></tr></table></figure>
<p>那么 <code>model</code> 内部有两个可学习的参数：</p>
<ul>
<li><code>weight</code>：形状为 <code>(1, 2)</code> 的张量，对应两个输入特征的权重</li>
<li><code>bias</code>：形状为 <code>(1,)</code> 的张量，对应偏置项</li>
</ul>
<p><strong>各部分含义：</strong></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>.data</th>
<th>获取张量底层的<strong>数据（Tensor）</strong>，不包含梯度信息</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>.numpy()</strong></td>
<td><strong>将 PyTorch 张量（CPU 上）转换为 NumPy 数组，便于打印或绘图</strong></td>
</tr>
</tbody>
</table>
</div>
<p><code>with torch.no_grad():</code> 是什么？</p>
<p>到代码的结尾，模型已经训练完了，现在要进行绘图操作或者对比真实值与预测值，那么就不需要梯度，如果不关闭自动求导（autograd）的话，只会浪费内存（保存中间结果）和浪费时间</p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>以上的整体代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机种子，确保每次结果一致</span></span><br><span class="line">torch.manual_seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line">X = torch.randn(<span class="number">100</span> ,<span class="number">2</span>)</span><br><span class="line">true_w = torch.tensor([<span class="number">2.0</span>, <span class="number">3.0</span>])</span><br><span class="line">true_b = <span class="number">4.0</span>     <span class="comment"># 偏置</span></span><br><span class="line">Y = X @ true_w + true_b + torch.randn(<span class="number">100</span>) * <span class="number">0.1</span> <span class="comment"># 加入一点噪声</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SimpleNN</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(SimpleNN, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.linear = nn.Linear(<span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.linear(x)</span><br><span class="line"></span><br><span class="line">model = SimpleNN()</span><br><span class="line"></span><br><span class="line">criterion = nn.MSELoss()</span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">epochs = <span class="number">1000</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    model.train()</span><br><span class="line">    data = model(X)</span><br><span class="line"></span><br><span class="line">    loss = criterion(data.squeeze(), Y)  <span class="comment"># 预测值data需要压缩成1维</span></span><br><span class="line"></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (epoch + <span class="number">1</span>) % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;Epoch [<span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>/1000], Loss: <span class="subst">&#123;loss.item():<span class="number">.4</span>f&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Predicted Weight: <span class="subst">&#123;model.linear.weight.data.numpy()&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Predicted Bias: <span class="subst">&#123;model.linear.bias.data.numpy()&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    predictions = model(X)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘图</span></span><br><span class="line">plt.scatter(X[:, <span class="number">0</span>], Y, color=<span class="string">&#x27;blue&#x27;</span>, label=<span class="string">&#x27;True values&#x27;</span>)</span><br><span class="line">plt.scatter(X[:, <span class="number">0</span>], predictions, color=<span class="string">&#x27;red&#x27;</span>, label=<span class="string">&#x27;Predictions&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>结果如下：</p>
<p><img src="/img/lazy.gif" data-original="5.png" alt=""></p>
<h2 id="卷积神经网络"><a href="#卷积神经网络" class="headerlink" title="卷积神经网络"></a>卷积神经网络</h2><p><strong>卷积神经网络 (Convolutional Neural Networks, CNN)</strong> 是一类专门用于处理具有网格状拓扑结构数据（如图像）的深度学习模型</p>
<p><img src="/img/lazy.gif" data-original="6.png" alt=""></p>
<p>在图中，CNN 的输出层给出了三个类别的概率：Donald（0.2）、Goofy（0.1）和Tweety（0.7），这表明网络认为输入图像最有可能是 Tweety</p>
<h3 id="基本结构"><a href="#基本结构" class="headerlink" title="基本结构"></a>基本结构</h3><h4 id="输入层（Input-Layer）"><a href="#输入层（Input-Layer）" class="headerlink" title="输入层（Input Layer）"></a>输入层（Input Layer）</h4><p>接收原始图像数据，图像通常被表示为一个三维数组，其中两个维度代表图像的宽度和高度，第三个维度代表颜色通道（例如，RGB图像有三个通道）</p>
<h4 id="卷积层（Convolutional-Layer）"><a href="#卷积层（Convolutional-Layer）" class="headerlink" title="卷积层（Convolutional Layer）"></a>卷积层（Convolutional Layer）</h4><p>用卷积核提取局部特征，如边缘、纹理等，公式如下：</p>
<script type="math/tex; mode=display">
y[i,j]=\sum_{m}\sum_{n}x[i+m,j+n]·k[m,n]+b</script><ul>
<li><script type="math/tex">x</script>：输入图像</li>
<li><script type="math/tex">k</script>：卷积核（权重矩阵）</li>
<li><script type="math/tex">b</script>：偏置</li>
</ul>
<p>应用一组可学习的滤波器（或卷积核）在输入图像上进行卷积操作，以提取局部特征</p>
<p>每个滤波器在输入图像上滑动，生成一个特征图（Feature Map），表示滤波器在不同位置的激活</p>
<p>卷积层可以有多个滤波器，每个滤波器生成一个特征图，所有特征图组成一个特征图集合</p>
<h4 id="激活函数（Activation-Function）-1"><a href="#激活函数（Activation-Function）-1" class="headerlink" title="激活函数（Activation Function）"></a>激活函数（Activation Function）</h4><p>通常在卷积层之后应用非线性激活函数，如 ReLU（Rectified Linear Unit），以引入非线性特性，使网络能够学习更复杂的模式</p>
<h4 id="池化层（Pooling-Layer）"><a href="#池化层（Pooling-Layer）" class="headerlink" title="池化层（Pooling Layer）"></a>池化层（Pooling Layer）</h4><ul>
<li>用于降低特征图的空间维度，减少计算量和参数数量，同时保留最重要的特征信息</li>
<li>最常见的池化操作是 <strong>最大池化（Max Pooling）</strong> 和 <strong>平均池化（Average Pooling）</strong></li>
<li>最大池化选择区域内的最大值，而平均池化计算区域内的平均值</li>
</ul>
<h4 id="归一化层（Normalization-Layer，-可选）"><a href="#归一化层（Normalization-Layer，-可选）" class="headerlink" title="归一化层（Normalization Layer， 可选）"></a>归一化层（Normalization Layer， 可选）</h4><p>例如，<strong>局部响应归一化（Local Response Normalization, LRN）</strong> 或 <strong>批归一化（Batch Normalization）</strong></p>
<p>这些层有助于加速训练过程，提高模型的稳定性</p>
<p>归一化：</p>
<script type="math/tex; mode=display">
\hat{x}_i = \frac{x_i - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}}</script><p>其中 <script type="math/tex">\epsilon</script> 是一个极小常数，防止除以0</p>
<h4 id="全连接层（Fully-Connected-Layer）"><a href="#全连接层（Fully-Connected-Layer）" class="headerlink" title="全连接层（Fully Connected Layer）"></a>全连接层（Fully Connected Layer）</h4><p>在 CNN 的末端，将前面层提取的特征图展平（Flatten）成一维向量，然后输入到全连接层</p>
<p>全连接层的每个神经元都与前一层的所有神经元相连，用于综合特征并进行最终的分类或回归</p>
<h4 id="输出层（Output-Layer）"><a href="#输出层（Output-Layer）" class="headerlink" title="输出层（Output Layer）"></a>输出层（Output Layer）</h4><p>根据任务的不同，输出层可以有不同的形式</p>
<p>对于分类任务，通常使用 Softmax 函数将输出转换为概率分布，表示输入属于各个类别的概率</p>
<h4 id="损失函数（Loss-Function）-1"><a href="#损失函数（Loss-Function）-1" class="headerlink" title="损失函数（Loss Function）"></a>损失函数（Loss Function）</h4><p>用于衡量模型预测与真实标签之间的差异</p>
<p>常见的损失函数包括交叉熵损失（Cross-Entropy Loss）用于多分类任务，均方误差（Mean Squared Error, MSE）用于回归任务</p>
<h4 id="优化器（Optimizer）-1"><a href="#优化器（Optimizer）-1" class="headerlink" title="优化器（Optimizer）"></a>优化器（Optimizer）</h4><p>用于根据损失函数的梯度更新网络的权重。常见的优化器包括随机梯度下降（SGD）、Adam、RMSprop等</p>
<h4 id="正则化（Regularization）"><a href="#正则化（Regularization）" class="headerlink" title="正则化（Regularization）"></a>正则化（Regularization）</h4><p>包括 Dropout、L1/L2 正则化等技术，用于防止模型过拟合</p>
<p>这些层可以堆叠形成更深的网络结构，以提高模型的学习能力</p>
<p>CNN 的深度和复杂性可以根据任务的需求进行调整</p>
<h3 id="实例-1"><a href="#实例-1" class="headerlink" title="实例"></a>实例</h3><p>主要步骤：</p>
<ul>
<li><strong>数据加载与预处理</strong>：使用 <code>torchvision</code> 加载和预处理 MNIST 数据</li>
<li><strong>模型构建</strong>：定义卷积层、池化层和全连接层</li>
<li><strong>训练</strong>：通过损失函数和优化器进行模型训练</li>
<li><strong>评估</strong>：测试集上计算模型的准确率</li>
<li><strong>可视化</strong>：展示部分测试样本及其预测结果</li>
</ul>
<h4 id="数据加载"><a href="#数据加载" class="headerlink" title="数据加载"></a>数据加载</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.ToTensor(),                   <span class="comment"># 转为张量</span></span><br><span class="line">    transforms.Normalize((<span class="number">0.5</span>,), (<span class="number">0.5</span>,))     <span class="comment"># 归一化到 [-1, 1]</span></span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载 MNIST 数据集</span></span><br><span class="line">train_dataset = datasets.MNIST(root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">True</span>, transform=transform, download=<span class="literal">True</span>)</span><br><span class="line">test_dataset = datasets.MNIST(root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">False</span>, transform=transform, download=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=<span class="number">64</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line">test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=<span class="number">64</span>, shuffle=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<h4 id="定义CNN模型"><a href="#定义CNN模型" class="headerlink" title="定义CNN模型"></a>定义CNN模型</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定于一个CNN</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SimpleNN</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(SimpleNN, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="comment"># 定义卷积层：输入1通道，输出32通道，卷积核大小3x3</span></span><br><span class="line">        <span class="variable language_">self</span>.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">32</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 定义卷积层：输入32通道，输出64通道</span></span><br><span class="line">        <span class="variable language_">self</span>.conv2 = nn.Conv2d(<span class="number">32</span>, <span class="number">64</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 定义全连接层</span></span><br><span class="line">        <span class="variable language_">self</span>.fc1 = nn.Linear(<span class="number">64</span> * <span class="number">7</span> * <span class="number">7</span>, <span class="number">128</span>)</span><br><span class="line">        <span class="variable language_">self</span>.fc2 = nn.Linear(<span class="number">128</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = F.relu(<span class="variable language_">self</span>.conv1(x))   <span class="comment"># 第一层卷积 + ReLU</span></span><br><span class="line">        x = F.max_pool2d(x, <span class="number">2</span>)      <span class="comment"># 最大池化</span></span><br><span class="line">        x = F.relu(<span class="variable language_">self</span>.conv2(x))   <span class="comment"># 第二层卷积 + ReLU</span></span><br><span class="line">        x = F.max_pool2d(x, <span class="number">2</span>)      <span class="comment"># 最大池化</span></span><br><span class="line">        x = x.view(-<span class="number">1</span>, <span class="number">64</span> * <span class="number">7</span> * <span class="number">7</span>)  <span class="comment"># 展平操作 (改变张量形状)</span></span><br><span class="line">        x = F.relu(<span class="variable language_">self</span>.fc1(x))     <span class="comment"># 全连接层 + ReLU</span></span><br><span class="line">        x = <span class="variable language_">self</span>.fc2(x)             <span class="comment"># 全连接层输出</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建模型实例</span></span><br><span class="line">model = SimpleNN()</span><br></pre></td></tr></table></figure>
<p>其中：</p>
<ul>
<li><strong>卷积核大小</strong>（kernel size）对卷积神经网络（CNN）的性能、感受野、参数量和特征提取能力有 <strong>显著影响</strong> （现代CNN普遍使用3x3小卷积核）</li>
<li><strong>感受野</strong>：卷积核大小决定了 <strong>单个输出神经元能看到的输入区域范围</strong>，<strong>大卷积核</strong>（如 7×7）：感受野大，能捕获更大范围的上下文信息（适合检测大物体或全局结构），<strong>小卷积核</strong>（如 3×3）：感受野小，更关注局部细节（如边缘、角点）</li>
<li><strong>参数量</strong>：假设输入通道数为<script type="math/tex">C_{in}</script>，输出通道数为<script type="math/tex">C_{out}</script>，卷积核大小为<script type="math/tex">K</script> X <script type="math/tex">K</script>，则参数量为：</li>
</ul>
<script type="math/tex; mode=display">
K \times K\times C_{in} \times C_{out} + C_{out}+(偏置b)</script><ul>
<li><strong>stride</strong>：步长为1，代表卷积核每次在输入图像上移动的像素数，<code>stride=1</code> 表示卷积核每次向右（或向下）<strong>移动 1 个像素</strong>（<code>stride</code> 越大，输出越小，计算量越少，但会 <strong>丢失更多空间信息</strong>），其输出尺寸公式（高度方向）如下：</li>
</ul>
<script type="math/tex; mode=display">
H_{\text{out}} = \left\lfloor \frac{H_{\text{in}} + 2P - K}{S} \right\rfloor + 1\\
H_{in} ：输入高度\\
K ：卷积核高度（通常为奇数，如 3、5）\\
P：上下填充的像素数（padding）\\
S ：步长（stride）\\
⌊⋅⌋ ：向下取整（floor 函数）\\</script><ul>
<li><strong>padding</strong>：填充为1，在输入图像的 <strong>四周补 0</strong>（zero-padding），每边补 <code>padding</code> 个像素，<code>padding=1</code> 表示在 <strong>上下左右各补 1 行/列 0</strong></li>
<li><strong>stride=1,padding=1</strong>：输入为HxW，那么输出仍是HxW</li>
</ul>
<h4 id="损失函数、优化函数和训练"><a href="#损失函数、优化函数和训练" class="headerlink" title="损失函数、优化函数和训练"></a>损失函数、优化函数和训练</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 损失函数和优化器</span></span><br><span class="line">criterion = nn.CrossEntropyLoss()                                 <span class="comment"># 多分类交叉熵损失</span></span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">0.01</span>, momentum=<span class="number">0.9</span>)  <span class="comment"># 学习率和动量</span></span><br><span class="line"></span><br><span class="line">epochs = <span class="number">5</span></span><br><span class="line">model.train()</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    total_loss = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> images, labels <span class="keyword">in</span> train_loader:</span><br><span class="line">        outputs = model(images)</span><br><span class="line">        loss = criterion(outputs, labels)</span><br><span class="line"></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        total_loss += loss.item()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Epoch [<span class="subst">&#123;epoch+<span class="number">1</span>&#125;</span>/<span class="subst">&#123;epochs&#125;</span>], Loss: <span class="subst">&#123;total_loss / <span class="built_in">len</span>(train_loader):<span class="number">.4</span>f&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<h4 id="测试评估"><a href="#测试评估" class="headerlink" title="测试评估"></a>测试评估</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">model.<span class="built_in">eval</span>()  <span class="comment"># 设置为评估模式</span></span><br><span class="line">correct = <span class="number">0</span></span><br><span class="line">total = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():  <span class="comment"># 评估时不需要计算梯度</span></span><br><span class="line">    <span class="keyword">for</span> images, labels <span class="keyword">in</span> test_loader:</span><br><span class="line">        outputs = model(images)               <span class="comment"># 前向传播，输出形状 [batch_size, 10]</span></span><br><span class="line">        _, predicted = torch.<span class="built_in">max</span>(outputs, <span class="number">1</span>)  <span class="comment"># 取每行最大值的索引（预测类别）</span></span><br><span class="line">        total += labels.size(<span class="number">0</span>)               <span class="comment"># 累计总样本数</span></span><br><span class="line">        correct += (predicted == labels).<span class="built_in">sum</span>().item()  <span class="comment"># 累计正确预测数</span></span><br><span class="line"></span><br><span class="line">accuracy = <span class="number">100</span> * correct / total</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Test Accuracy: <span class="subst">&#123;accuracy:<span class="number">.2</span>f&#125;</span>%&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>其中：</p>
<ul>
<li>(predicted == labels) 返回布尔张量，sum()将True视为1，False视为0，item()将单元素张量转化为python数值</li>
</ul>
<h4 id="完整代码如下：（可视化部分可以不看）"><a href="#完整代码如下：（可视化部分可以不看）" class="headerlink" title="完整代码如下：（可视化部分可以不看）"></a>完整代码如下：（可视化部分可以不看）</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"><span class="keyword">import</span> torchvision.datasets <span class="keyword">as</span> datasets</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt </span><br><span class="line"></span><br><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.ToTensor(),                   <span class="comment"># 转为张量</span></span><br><span class="line">    transforms.Normalize((<span class="number">0.5</span>,), (<span class="number">0.5</span>,))     <span class="comment"># 归一化到 [-1, 1]</span></span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载 MNIST 数据集</span></span><br><span class="line">train_dataset = datasets.MNIST(root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">True</span>, transform=transform, download=<span class="literal">True</span>)</span><br><span class="line">test_dataset = datasets.MNIST(root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">False</span>, transform=transform, download=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=<span class="number">64</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line">test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=<span class="number">64</span>, shuffle=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定于一个CNN</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SimpleNN</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(SimpleNN, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="comment"># 定义卷积层：输入1通道，输出32通道，卷积核大小3x3</span></span><br><span class="line">        <span class="variable language_">self</span>.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">32</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 定义卷积层：输入32通道，输出64通道</span></span><br><span class="line">        <span class="variable language_">self</span>.conv2 = nn.Conv2d(<span class="number">32</span>, <span class="number">64</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 定义全连接层</span></span><br><span class="line">        <span class="variable language_">self</span>.fc1 = nn.Linear(<span class="number">64</span> * <span class="number">7</span> * <span class="number">7</span>, <span class="number">128</span>)</span><br><span class="line">        <span class="variable language_">self</span>.fc2 = nn.Linear(<span class="number">128</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = F.relu(<span class="variable language_">self</span>.conv1(x))   <span class="comment"># 第一层卷积 + ReLU</span></span><br><span class="line">        x = F.max_pool2d(x, <span class="number">2</span>)      <span class="comment"># 最大池化</span></span><br><span class="line">        x = F.relu(<span class="variable language_">self</span>.conv2(x))   <span class="comment"># 第二层卷积 + ReLU</span></span><br><span class="line">        x = F.max_pool2d(x, <span class="number">2</span>)      <span class="comment"># 最大池化</span></span><br><span class="line">        x = x.view(-<span class="number">1</span>, <span class="number">64</span> * <span class="number">7</span> * <span class="number">7</span>)  <span class="comment"># 展平操作 (改变张量形状)</span></span><br><span class="line">        x = F.relu(<span class="variable language_">self</span>.fc1(x))     <span class="comment"># 全连接层 + ReLU</span></span><br><span class="line">        x = <span class="variable language_">self</span>.fc2(x)             <span class="comment"># 全连接层输出</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建模型实例</span></span><br><span class="line">model = SimpleNN()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 损失函数和优化器</span></span><br><span class="line">criterion = nn.CrossEntropyLoss()                                 <span class="comment"># 多分类交叉熵损失</span></span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">0.01</span>, momentum=<span class="number">0.9</span>)  <span class="comment"># 学习率和动量</span></span><br><span class="line"></span><br><span class="line">epochs = <span class="number">5</span></span><br><span class="line">model.train()</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    total_loss = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> images, labels <span class="keyword">in</span> train_loader:</span><br><span class="line">        outputs = model(images)</span><br><span class="line">        loss = criterion(outputs, labels)</span><br><span class="line"></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        total_loss += loss.item()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Epoch [<span class="subst">&#123;epoch+<span class="number">1</span>&#125;</span>/<span class="subst">&#123;epochs&#125;</span>], Loss: <span class="subst">&#123;total_loss / <span class="built_in">len</span>(train_loader):<span class="number">.4</span>f&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line">correct = <span class="number">0</span></span><br><span class="line">total = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="keyword">for</span> images, labels <span class="keyword">in</span> test_loader:</span><br><span class="line">        outputs = model(images)</span><br><span class="line">        _, predictions = torch.<span class="built_in">max</span>(outputs, <span class="number">1</span>)</span><br><span class="line">        total += labels.size(<span class="number">0</span>)</span><br><span class="line">        correct += (predictions == labels).<span class="built_in">sum</span>().item()</span><br><span class="line"></span><br><span class="line">accuracy = <span class="number">100</span> * correct / total</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Test Accuracy: <span class="subst">&#123;accuracy:<span class="number">.2</span>f&#125;</span>%&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可视化测试结果</span></span><br><span class="line">dataiter = <span class="built_in">iter</span>(test_loader)</span><br><span class="line">images, labels = <span class="built_in">next</span>(dataiter)</span><br><span class="line">outputs = model(images)</span><br><span class="line">_, predictions = torch.<span class="built_in">max</span>(outputs, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">fig, axes = plt.subplots(<span class="number">1</span>, <span class="number">6</span>, figsize=(<span class="number">12</span>, <span class="number">4</span>))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">6</span>):</span><br><span class="line">    axes[i].imshow(images[i][<span class="number">0</span>], cmap=<span class="string">&#x27;gray&#x27;</span>)</span><br><span class="line">    axes[i].set_title(<span class="string">f&quot;Label: <span class="subst">&#123;labels[i]&#125;</span>\nPred: <span class="subst">&#123;predictions[i]&#125;</span>&quot;</span>)</span><br><span class="line">    axes[i].axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/img/lazy.gif" data-original="7.png" alt=""></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://PureStream108.github.io/project">Pure Stream</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://purestream108.github.io/project/2025/10/22/Pytorch/">https://purestream108.github.io/project/2025/10/22/Pytorch/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="https://PureStream108.github.io/project" target="_blank">PureStream & Marblue</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/AI/">AI</a><a class="post-meta__tags" href="/tags/Pytorch/">Pytorch</a></div><div class="post-share"><div class="social-share" data-image="/img/purestream.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related full-width" href="/2025/10/12/%E7%BE%8A%E5%9F%8E%E6%9D%AF2025Web-1345/" title="羊城杯2025Web 1345"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">羊城杯2025Web 1345</div></div><div class="info-2"><div class="info-item-1">羊城杯Web1345 wp我想说什么，但是也不懂说什么，那就不说了 ez_unserialize123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102&lt;?phperror_reporting(0);highlight_file(__FILE__);class A &#123;    public $first;    public $step;    public $next;    public function __construct() &#123;        $this-&gt;first = &quot;继续加油！&quot;;    &#125;    public function start() &#123;     ...</div></div></div></a></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/lazy.gif" data-original="/img/purestream.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">Pure Stream</div><div class="author-info-description">X1cT34m & Web</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">23</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">29</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">20</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://www.cnblogs.com/LAMENTXU"><i class="fab fa-github"></i><span>请支持LMTX喵</span></a><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/PureStream108" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="https://www.mihoyo.com/" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">推：知更鸟，芙宁娜，夏洛蒂，春日野穹，星（崩铁），希儿（崩铁），三无Marblue，朔夜观星，遐蝶，浮波柚叶，二阶堂希罗</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Pytorch%EF%BC%88%E6%8C%81%E7%BB%AD%E6%9B%B4%E6%96%B0ing%EF%BC%89"><span class="toc-number">1.</span> <span class="toc-text">Pytorch（持续更新ing）</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BC%80%E5%A7%8B"><span class="toc-number">1.1.</span> <span class="toc-text">开始</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BC%A0%E9%87%8F%EF%BC%88tensor%EF%BC%89"><span class="toc-number">1.2.</span> <span class="toc-text">张量（tensor）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BC%A0%E9%87%8F%E5%9F%BA%E6%9C%AC%E6%96%B9%E5%BC%8F"><span class="toc-number">1.2.1.</span> <span class="toc-text">张量基本方式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BC%A0%E9%87%8F%E5%9F%BA%E6%9C%AC%E5%B1%9E%E6%80%A7"><span class="toc-number">1.2.2.</span> <span class="toc-text">张量基本属性</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BC%A0%E9%87%8F%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C"><span class="toc-number">1.2.3.</span> <span class="toc-text">张量基本操作</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9F%BA%E7%A1%80%E6%93%8D%E4%BD%9C%EF%BC%9A"><span class="toc-number">1.2.3.1.</span> <span class="toc-text">基础操作：</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%BD%A2%E7%8A%B6%E6%93%8D%E4%BD%9C%EF%BC%9A"><span class="toc-number">1.2.3.2.</span> <span class="toc-text">形状操作：</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#GPU%E5%8A%A0%E9%80%9F"><span class="toc-number">1.2.4.</span> <span class="toc-text">GPU加速</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BC%A0%E9%87%8F%E4%B8%8ENumpy%E4%BA%92%E7%9B%B8%E6%93%8D%E4%BD%9C"><span class="toc-number">1.2.5.</span> <span class="toc-text">张量与Numpy互相操作</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80"><span class="toc-number">1.3.</span> <span class="toc-text">神经网络基础</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%A5%9E%E7%BB%8F%E5%85%83%EF%BC%88Neuron%EF%BC%89"><span class="toc-number">1.3.1.</span> <span class="toc-text">神经元（Neuron）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B1%82%EF%BC%88Layer%EF%BC%89"><span class="toc-number">1.3.2.</span> <span class="toc-text">层（Layer）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88FNN%EF%BC%89"><span class="toc-number">1.3.3.</span> <span class="toc-text">前馈神经网络（FNN）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88RNN%EF%BC%89"><span class="toc-number">1.3.4.</span> <span class="toc-text">循环神经网络（RNN）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88CNN%EF%BC%89"><span class="toc-number">1.3.5.</span> <span class="toc-text">卷积神经网络（CNN）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9E%E4%BE%8B"><span class="toc-number">1.3.6.</span> <span class="toc-text">实例</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%EF%BC%88Activation-Function%EF%BC%89"><span class="toc-number">1.3.7.</span> <span class="toc-text">激活函数（Activation Function）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%EF%BC%88Loss-Function%EF%BC%89"><span class="toc-number">1.3.8.</span> <span class="toc-text">损失函数（Loss Function）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BC%98%E5%8C%96%E5%99%A8%EF%BC%88Optimizer%EF%BC%89"><span class="toc-number">1.3.9.</span> <span class="toc-text">优化器（Optimizer）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B%EF%BC%88Training-Process%EF%BC%89"><span class="toc-number">1.3.10.</span> <span class="toc-text">训练过程（Training Process）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AC%AC%E4%B8%80%E4%B8%AA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">1.4.</span> <span class="toc-text">第一个神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9E%E4%BE%8B1"><span class="toc-number">1.4.1.</span> <span class="toc-text">实例1</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95"><span class="toc-number">1.4.2.</span> <span class="toc-text">梯度下降算法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9E%E4%BE%8B2"><span class="toc-number">1.4.3.</span> <span class="toc-text">实例2</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E4%B8%8E%E5%8A%A0%E8%BD%BD"><span class="toc-number">1.5.</span> <span class="toc-text">数据处理与加载</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%87%AA%E5%AE%9A%E4%B9%89Dataset"><span class="toc-number">1.5.1.</span> <span class="toc-text">自定义Dataset</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#DataLoader%E5%8A%A0%E8%BD%BD%E6%95%B0%E6%8D%AE"><span class="toc-number">1.5.2.</span> <span class="toc-text">DataLoader加载数据</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%A2%84%E5%A4%84%E7%90%86%E5%92%8C%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA"><span class="toc-number">1.5.3.</span> <span class="toc-text">预处理和数据增强</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8A%A0%E8%BD%BD%E5%9B%BE%E5%83%8F%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">1.5.4.</span> <span class="toc-text">加载图像数据集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%94%A8%E5%A4%9A%E4%B8%AA%E6%95%B0%E6%8D%AE%E6%BA%90%EF%BC%88Multi-source-Dataset%EF%BC%89"><span class="toc-number">1.5.5.</span> <span class="toc-text">用多个数据源（Multi-source Dataset）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="toc-number">1.6.</span> <span class="toc-text">线性回归</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9A%E4%B9%89%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.6.1.</span> <span class="toc-text">定义线性回归模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9A%E4%B9%89%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E4%B8%8E%E4%BC%98%E5%8C%96%E5%99%A8"><span class="toc-number">1.6.2.</span> <span class="toc-text">定义损失函数与优化器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.6.3.</span> <span class="toc-text">训练模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AF%84%E4%BC%B0"><span class="toc-number">1.6.4.</span> <span class="toc-text">评估</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-number">1.6.5.</span> <span class="toc-text">总结</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">1.7.</span> <span class="toc-text">卷积神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%84"><span class="toc-number">1.7.1.</span> <span class="toc-text">基本结构</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%BE%93%E5%85%A5%E5%B1%82%EF%BC%88Input-Layer%EF%BC%89"><span class="toc-number">1.7.1.1.</span> <span class="toc-text">输入层（Input Layer）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8D%B7%E7%A7%AF%E5%B1%82%EF%BC%88Convolutional-Layer%EF%BC%89"><span class="toc-number">1.7.1.2.</span> <span class="toc-text">卷积层（Convolutional Layer）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%EF%BC%88Activation-Function%EF%BC%89-1"><span class="toc-number">1.7.1.3.</span> <span class="toc-text">激活函数（Activation Function）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%B1%A0%E5%8C%96%E5%B1%82%EF%BC%88Pooling-Layer%EF%BC%89"><span class="toc-number">1.7.1.4.</span> <span class="toc-text">池化层（Pooling Layer）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%BD%92%E4%B8%80%E5%8C%96%E5%B1%82%EF%BC%88Normalization-Layer%EF%BC%8C-%E5%8F%AF%E9%80%89%EF%BC%89"><span class="toc-number">1.7.1.5.</span> <span class="toc-text">归一化层（Normalization Layer， 可选）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%B1%82%EF%BC%88Fully-Connected-Layer%EF%BC%89"><span class="toc-number">1.7.1.6.</span> <span class="toc-text">全连接层（Fully Connected Layer）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%BE%93%E5%87%BA%E5%B1%82%EF%BC%88Output-Layer%EF%BC%89"><span class="toc-number">1.7.1.7.</span> <span class="toc-text">输出层（Output Layer）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%EF%BC%88Loss-Function%EF%BC%89-1"><span class="toc-number">1.7.1.8.</span> <span class="toc-text">损失函数（Loss Function）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BC%98%E5%8C%96%E5%99%A8%EF%BC%88Optimizer%EF%BC%89-1"><span class="toc-number">1.7.1.9.</span> <span class="toc-text">优化器（Optimizer）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%AD%A3%E5%88%99%E5%8C%96%EF%BC%88Regularization%EF%BC%89"><span class="toc-number">1.7.1.10.</span> <span class="toc-text">正则化（Regularization）</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9E%E4%BE%8B-1"><span class="toc-number">1.7.2.</span> <span class="toc-text">实例</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%8A%A0%E8%BD%BD"><span class="toc-number">1.7.2.1.</span> <span class="toc-text">数据加载</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%9A%E4%B9%89CNN%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.7.2.2.</span> <span class="toc-text">定义CNN模型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E3%80%81%E4%BC%98%E5%8C%96%E5%87%BD%E6%95%B0%E5%92%8C%E8%AE%AD%E7%BB%83"><span class="toc-number">1.7.2.3.</span> <span class="toc-text">损失函数、优化函数和训练</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%B5%8B%E8%AF%95%E8%AF%84%E4%BC%B0"><span class="toc-number">1.7.2.4.</span> <span class="toc-text">测试评估</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%8C%E6%95%B4%E4%BB%A3%E7%A0%81%E5%A6%82%E4%B8%8B%EF%BC%9A%EF%BC%88%E5%8F%AF%E8%A7%86%E5%8C%96%E9%83%A8%E5%88%86%E5%8F%AF%E4%BB%A5%E4%B8%8D%E7%9C%8B%EF%BC%89"><span class="toc-number">1.7.2.5.</span> <span class="toc-text">完整代码如下：（可视化部分可以不看）</span></a></li></ol></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/10/22/Pytorch/" title="Pytorch">Pytorch</a><time datetime="2025-10-22T10:49:46.000Z" title="发表于 2025-10-22 18:49:46">2025-10-22</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/10/12/%E7%BE%8A%E5%9F%8E%E6%9D%AF2025Web-1345/" title="羊城杯2025Web 1345">羊城杯2025Web 1345</a><time datetime="2025-10-12T04:56:13.000Z" title="发表于 2025-10-12 12:56:13">2025-10-12</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/10/10/timeit-repeat%E4%BB%A3%E7%A0%81%E6%89%A7%E8%A1%8C/" title="timeit.repeat代码执行">timeit.repeat代码执行</a><time datetime="2025-10-10T13:34:21.000Z" title="发表于 2025-10-10 21:34:21">2025-10-10</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/09/08/2025%E6%B9%BE%E5%8C%BA%E6%9D%AFWeb-Wp/" title="2025湾区杯Web Wp">2025湾区杯Web Wp</a><time datetime="2025-09-08T07:17:51.000Z" title="发表于 2025-09-08 15:17:51">2025-09-08</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/08/25/0xGame2025-Official-Web/" title="0xGame2025 Official Web">0xGame2025 Official Web</a><time datetime="2025-08-25T12:52:51.000Z" title="发表于 2025-08-25 20:52:51">2025-08-25</time></div></div></div></div></div></div></main><footer id="footer"><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;&nbsp;2025 By Pure Stream</span><span class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 7.3.0</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.5.0-b1</a></span></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"><script>(() => {
  const loadMathjax = () => {
    if (!window.MathJax) {
      window.MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          tags: 'none',
        },
        chtml: {
          scale: 1.1
        },
        options: {
          enableMenu: true,
          renderActions: {
            findScript: [10, doc => {
              for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
                const display = !!node.type.match(/; *mode=display/)
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
                const text = document.createTextNode('')
                node.parentNode.replaceChild(text, node)
                math.start = {node: text, delim: '', n: 0}
                math.end = {node: text, delim: '', n: 0}
                doc.math.push(math)
              }
            }, '']
          }
        }
      }

      const script = document.createElement('script')
      script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
      script.id = 'MathJax-script'
      script.async = true
      document.head.appendChild(script)
    } else {
      MathJax.startup.document.state(0)
      MathJax.texReset()
      MathJax.typesetPromise()
    }
  }

  btf.addGlobalFn('encrypt', loadMathjax, 'mathjax')
  window.pjax ? loadMathjax() : window.addEventListener('load', loadMathjax)
})()</script></div><script defer="defer" id="fluttering_ribbon" mobile="true" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-fluttering-ribbon.min.js"></script><script id="click-show-text" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/click-show-text.min.js" data-mobile="true" data-text="CTF,Web,miHoYo,PureStream,Marblue" data-fontsize="15px" data-random="true" async="async"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div>
        <style>
            [bg-lazy] {
                background-image: none !important;
                background-color: #eee !important;
            }
        </style>
        <script>
            window.imageLazyLoadSetting = {
                isSPA: false,
                preloadRatio: 1,
                processImages: null,
            };
        </script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z\d\-\.\+]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(t.test(e.href)||r.test(e.href))&&(e.href=a.dataset.original)})});</script><script>(r=>{r.imageLazyLoadSetting.processImages=t;var a=r.imageLazyLoadSetting.isSPA,o=r.imageLazyLoadSetting.preloadRatio||1,d=i();function i(){var t=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")),e=Array.prototype.slice.call(document.querySelectorAll("[bg-lazy]"));return t.concat(e)}function t(t){(a||t)&&(d=i());for(var e,n=0;n<d.length;n++)0<=(e=(e=d[n]).getBoundingClientRect()).bottom&&0<=e.left&&e.top<=(r.innerHeight*o||document.documentElement.clientHeight*o)&&(()=>{var t,e,a,o,i=d[n];e=function(){d=d.filter(function(t){return i!==t}),r.imageLazyLoadSetting.onImageLoaded&&r.imageLazyLoadSetting.onImageLoaded(i)},(t=i).dataset.loaded||(t.hasAttribute("bg-lazy")?(t.removeAttribute("bg-lazy"),e&&e()):(a=new Image,o=t.getAttribute("data-original"),a.onload=function(){t.src=o,t.removeAttribute("data-original"),t.setAttribute("data-loaded",!0),e&&e()},a.onerror=function(){t.removeAttribute("data-original"),t.setAttribute("data-loaded",!1),t.src=o},t.src!==o&&(a.src=o)))})()}function e(){clearTimeout(t.tId),t.tId=setTimeout(t,500)}t(),document.addEventListener("scroll",e),r.addEventListener("resize",e),r.addEventListener("orientationchange",e)})(this);</script></body></html>