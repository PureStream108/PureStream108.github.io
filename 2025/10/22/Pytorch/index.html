<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Pytorch | PureStream &amp; Marblue</title><meta name="author" content="Pure Stream"><meta name="copyright" content="Pure Stream"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="AI安全学习(1)">
<meta property="og:type" content="article">
<meta property="og:title" content="Pytorch">
<meta property="og:url" content="https://purestream108.github.io/project/2025/10/22/Pytorch/index.html">
<meta property="og:site_name" content="PureStream &amp; Marblue">
<meta property="og:description" content="AI安全学习(1)">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://purestream108.github.io/project/img/purestream.jpg">
<meta property="article:published_time" content="2025-10-22T10:49:46.000Z">
<meta property="article:modified_time" content="2025-11-14T08:27:21.292Z">
<meta property="article:author" content="Pure Stream">
<meta property="article:tag" content="AI">
<meta property="article:tag" content="Pytorch">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://purestream108.github.io/project/img/purestream.jpg"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Pytorch",
  "url": "https://purestream108.github.io/project/2025/10/22/Pytorch/",
  "image": "https://purestream108.github.io/project/img/purestream.jpg",
  "datePublished": "2025-10-22T10:49:46.000Z",
  "dateModified": "2025-11-14T08:27:21.292Z",
  "author": [
    {
      "@type": "Person",
      "name": "Pure Stream",
      "url": "https://PureStream108.github.io/project"
    }
  ]
}</script><link rel="shortcut icon" href="/img/marblue.jpg"><link rel="canonical" href="https://purestream108.github.io/project/2025/10/22/Pytorch/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"limitCount":50,"languages":{"author":"作者: Pure Stream","link":"链接: ","source":"来源: PureStream & Marblue","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: true,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Pytorch',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><meta name="generator" content="Hexo 7.3.0"><link rel="stylesheet" href="/css/prism.css" type="text/css">
<link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/lazy.gif" data-original="/img/purestream.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">26</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">31</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">21</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/bangumis/"><i class="fa-fw fas fa-heart"></i><span> Animation</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(/img/w1.jpg);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">PureStream &amp; Marblue</span></a><a class="nav-page-title" href="/"><span class="site-name">Pytorch</span><span class="site-name"><i class="fa-solid fa-circle-arrow-left"></i><span>  返回首页</span></span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/bangumis/"><i class="fa-fw fas fa-heart"></i><span> Animation</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">Pytorch</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-10-22T10:49:46.000Z" title="发表于 2025-10-22 18:49:46">2025-10-22</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-11-14T08:27:21.292Z" title="更新于 2025-11-14 16:27:21">2025-11-14</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/AI%E5%AE%89%E5%85%A8/">AI安全</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h1 id="Pytorch（已完结）"><a href="#Pytorch（已完结）" class="headerlink" title="Pytorch（已完结）"></a>Pytorch（已完结）</h1><p>（本文档只是给我自己看的，大部分来源菜鸟教程，小部分我不理解的我会自己补充）</p>
<h2 id="开始"><a href="#开始" class="headerlink" title="开始"></a>开始</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip install torch</span><br><span class="line">pip install torchvision</span><br></pre></td></tr></table></figure>
<p>如果不想要GPU加速，可以换成</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu</span><br></pre></td></tr></table></figure>
<p>然后直接运行第一段代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">a = torch.randn(<span class="number">2</span>, <span class="number">3</span>) <span class="comment">#创建一个服从正态分布的随机张量，均值为 0，标准差为 1</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(a)</span><br></pre></td></tr></table></figure>
<h2 id="Pytorch-torch（API）"><a href="#Pytorch-torch（API）" class="headerlink" title="Pytorch torch（API）"></a>Pytorch torch（API）</h2><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">类别</th>
<th style="text-align:left">API</th>
<th style="text-align:left">描述</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">Tensors</td>
<td style="text-align:left"><code>is_tensor(obj)</code></td>
<td style="text-align:left">检查 <code>obj</code> 是否为 PyTorch 张量</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left"><code>is_storage(obj)</code></td>
<td style="text-align:left">检查 <code>obj</code> 是否为 PyTorch 存储对象</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left"><code>is_complex(input)</code></td>
<td style="text-align:left">检查 <code>input</code> 数据类型是否为复数数据类型</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left"><code>is_conj(input)</code></td>
<td style="text-align:left">检查 <code>input</code> 是否为共轭张量</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left"><code>is_floating_point(input)</code></td>
<td style="text-align:left">检查 <code>input</code> 数据类型是否为浮点数据类型</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left"><code>is_nonzero(input)</code></td>
<td style="text-align:left">检查 <code>input</code> 是否为非零单一元素张量</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left"><code>set_default_dtype(d)</code></td>
<td style="text-align:left">设置默认浮点数据类型为 <code>d</code></td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left"><code>get_default_dtype()</code></td>
<td style="text-align:left">获取当前默认浮点 <code>torch.dtype</code></td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left"><code>set_default_device(device)</code></td>
<td style="text-align:left">设置默认 <code>torch.Tensor</code> 分配的设备为 <code>device</code></td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left"><code>get_default_device()</code></td>
<td style="text-align:left">获取默认 <code>torch.Tensor</code> 分配的设备</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left"><code>numel(input)</code></td>
<td style="text-align:left">返回 <code>input</code> 张量中的元素总数</td>
</tr>
<tr>
<td style="text-align:left">Creation Ops</td>
<td style="text-align:left"><code>tensor(data)</code></td>
<td style="text-align:left">通过复制 <code>data</code> 构造无自动梯度历史的张量</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left"><code>sparse_coo_tensor(indices, values)</code></td>
<td style="text-align:left">在指定的 <code>indices</code> 处构造稀疏张量，具有指定的值</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left"><code>as_tensor(data)</code></td>
<td style="text-align:left">将 <code>data</code> 转换为张量，共享数据并尽可能保留自动梯度历史</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left"><code>zeros(size)</code></td>
<td style="text-align:left">返回一个用标量值 0 填充的张量，形状由 <code>size</code> 定义</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left"><code>ones(size)</code></td>
<td style="text-align:left">返回一个用标量值 1 填充的张量，形状由 <code>size</code> 定义</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left"><code>arange(start, end, step)</code></td>
<td style="text-align:left">返回一个 1-D 张量，包含从 <code>start</code> 到 <code>end</code> 的值，步长为 <code>step</code></td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left"><code>rand(size)</code></td>
<td style="text-align:left">返回一个从 [0, 1) 区间均匀分布的随机数填充的张量</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left"><code>randn(size)</code></td>
<td style="text-align:left">返回一个从标准正态分布填充的张量</td>
</tr>
<tr>
<td style="text-align:left">Math operations</td>
<td style="text-align:left"><code>add(input, other, alpha)</code></td>
<td style="text-align:left">将 <code>other</code>（由 <code>alpha</code> 缩放）加到 <code>input</code> 上</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left"><code>mul(input, other)</code></td>
<td style="text-align:left">将 <code>input</code> 与 <code>other</code> 相乘</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left"><code>matmul(input, other)</code></td>
<td style="text-align:left">执行 <code>input</code> 和 <code>other</code> 的矩阵乘法</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left"><code>mean(input, dim)</code></td>
<td style="text-align:left">计算 <code>input</code> 在维度 <code>dim</code> 上的均值</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left"><code>sum(input, dim)</code></td>
<td style="text-align:left">计算 <code>input</code> 在维度 <code>dim</code> 上的和</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left"><code>max(input, dim)</code></td>
<td style="text-align:left">返回 <code>input</code> 在维度 <code>dim</code> 上的最大值</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left"><code>min(input, dim)</code></td>
<td style="text-align:left">返回 <code>input</code> 在维度 <code>dim</code> 上的最小值</td>
</tr>
</tbody>
</table>
</div>
<h3 id="Tensor-创建"><a href="#Tensor-创建" class="headerlink" title="Tensor 创建"></a><strong>Tensor 创建</strong></h3><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left"><strong>函数</strong></th>
<th style="text-align:left"><strong>描述</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>torch.tensor(data, dtype, device, requires_grad)</code></td>
<td style="text-align:left">从数据创建张量</td>
</tr>
<tr>
<td style="text-align:left"><code>torch.as_tensor(data, dtype, device)</code></td>
<td style="text-align:left">将数据转换为张量（共享内存）</td>
</tr>
<tr>
<td style="text-align:left"><code>torch.from_numpy(ndarray)</code></td>
<td style="text-align:left">从 NumPy 数组创建张量（共享内存）</td>
</tr>
<tr>
<td style="text-align:left"><code>torch.zeros(*size, dtype, device, requires_grad)</code></td>
<td style="text-align:left">创建全零张量</td>
</tr>
<tr>
<td style="text-align:left"><code>torch.ones(*size, dtype, device, requires_grad)</code></td>
<td style="text-align:left">创建全一张量</td>
</tr>
<tr>
<td style="text-align:left"><code>torch.empty(*size, dtype, device, requires_grad)</code></td>
<td style="text-align:left">创建未初始化的张量</td>
</tr>
<tr>
<td style="text-align:left"><code>torch.arange(start, end, step, dtype, device, requires_grad)</code></td>
<td style="text-align:left">创建等差序列张量</td>
</tr>
<tr>
<td style="text-align:left"><code>torch.linspace(start, end, steps, dtype, device, requires_grad)</code></td>
<td style="text-align:left">创建等间隔序列张量</td>
</tr>
<tr>
<td style="text-align:left"><code>torch.logspace(start, end, steps, base, dtype, device, requires_grad)</code></td>
<td style="text-align:left">创建对数间隔序列张量</td>
</tr>
<tr>
<td style="text-align:left"><code>torch.eye(n, m, dtype, device, requires_grad)</code></td>
<td style="text-align:left">创建单位矩阵</td>
</tr>
<tr>
<td style="text-align:left"><code>torch.full(size, fill_value, dtype, device, requires_grad)</code></td>
<td style="text-align:left">创建填充指定值的张量</td>
</tr>
<tr>
<td style="text-align:left"><code>torch.rand(*size, dtype, device, requires_grad)</code></td>
<td style="text-align:left">创建均匀分布随机张量（范围 [0, 1)）</td>
</tr>
<tr>
<td style="text-align:left"><code>torch.randn(*size, dtype, device, requires_grad)</code></td>
<td style="text-align:left">创建标准正态分布随机张量</td>
</tr>
<tr>
<td style="text-align:left"><code>torch.randint(low, high, size, dtype, device, requires_grad)</code></td>
<td style="text-align:left">创建整数随机张量</td>
</tr>
<tr>
<td style="text-align:left"><code>torch.randperm(n, dtype, device, requires_grad)</code></td>
<td style="text-align:left">创建 0 到 n-1 的随机排列</td>
</tr>
</tbody>
</table>
</div>
<hr>
<h3 id="Tensor-操作"><a href="#Tensor-操作" class="headerlink" title="Tensor 操作"></a><strong>Tensor 操作</strong></h3><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left"><strong>函数</strong></th>
<th style="text-align:left"><strong>描述</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>torch.cat(tensors, dim)</code></td>
<td style="text-align:left">沿指定维度连接张量</td>
</tr>
<tr>
<td style="text-align:left"><code>torch.stack(tensors, dim)</code></td>
<td style="text-align:left">沿新维度堆叠张量</td>
</tr>
<tr>
<td style="text-align:left"><code>torch.split(tensor, split_size, dim)</code></td>
<td style="text-align:left">将张量沿指定维度分割</td>
</tr>
<tr>
<td style="text-align:left"><code>torch.chunk(tensor, chunks, dim)</code></td>
<td style="text-align:left">将张量沿指定维度分块</td>
</tr>
<tr>
<td style="text-align:left"><code>torch.reshape(input, shape)</code></td>
<td style="text-align:left">改变张量的形状</td>
</tr>
<tr>
<td style="text-align:left"><code>torch.transpose(input, dim0, dim1)</code></td>
<td style="text-align:left">交换张量的两个维度</td>
</tr>
<tr>
<td style="text-align:left"><code>torch.squeeze(input, dim)</code></td>
<td style="text-align:left">移除大小为 1 的维度</td>
</tr>
<tr>
<td style="text-align:left"><code>torch.unsqueeze(input, dim)</code></td>
<td style="text-align:left">在指定位置插入大小为 1 的维度</td>
</tr>
<tr>
<td style="text-align:left"><code>torch.expand(input, size)</code></td>
<td style="text-align:left">扩展张量的尺寸</td>
</tr>
<tr>
<td style="text-align:left"><code>torch.narrow(input, dim, start, length)</code></td>
<td style="text-align:left">返回张量的切片</td>
</tr>
<tr>
<td style="text-align:left"><code>torch.permute(input, dims)</code></td>
<td style="text-align:left">重新排列张量的维度</td>
</tr>
<tr>
<td style="text-align:left"><code>torch.masked_select(input, mask)</code></td>
<td style="text-align:left">根据布尔掩码选择元素</td>
</tr>
<tr>
<td style="text-align:left"><code>torch.index_select(input, dim, index)</code></td>
<td style="text-align:left">沿指定维度选择索引对应的元素</td>
</tr>
<tr>
<td style="text-align:left"><code>torch.gather(input, dim, index)</code></td>
<td style="text-align:left">沿指定维度收集指定索引的元素</td>
</tr>
<tr>
<td style="text-align:left"><code>torch.scatter(input, dim, index, src)</code></td>
<td style="text-align:left">将 <code>src</code> 的值散布到 <code>input</code> 的指定位置</td>
</tr>
<tr>
<td style="text-align:left"><code>torch.nonzero(input)</code></td>
<td style="text-align:left">返回非零元素的索引</td>
</tr>
</tbody>
</table>
</div>
<hr>
<h3 id="数学运算"><a href="#数学运算" class="headerlink" title="数学运算"></a><strong>数学运算</strong></h3><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left"><strong>函数</strong></th>
<th style="text-align:left"><strong>描述</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>torch.add(input, other)</code></td>
<td style="text-align:left">逐元素加法</td>
</tr>
<tr>
<td style="text-align:left"><code>torch.sub(input, other)</code></td>
<td style="text-align:left">逐元素减法</td>
</tr>
<tr>
<td style="text-align:left"><code>torch.mul(input, other)</code></td>
<td style="text-align:left">逐元素乘法</td>
</tr>
<tr>
<td style="text-align:left"><code>torch.div(input, other)</code></td>
<td style="text-align:left">逐元素除法</td>
</tr>
<tr>
<td style="text-align:left"><code>torch.matmul(input, other)</code></td>
<td style="text-align:left">矩阵乘法（matrix multiply）</td>
</tr>
<tr>
<td style="text-align:left"><code>torch.pow(input, exponent)</code></td>
<td style="text-align:left">逐元素幂运算</td>
</tr>
<tr>
<td style="text-align:left"><code>torch.sqrt(input)</code></td>
<td style="text-align:left">逐元素平方根</td>
</tr>
<tr>
<td style="text-align:left"><code>torch.exp(input)</code></td>
<td style="text-align:left">逐元素指数函数</td>
</tr>
<tr>
<td style="text-align:left"><code>torch.log(input)</code></td>
<td style="text-align:left">逐元素自然对数（e）</td>
</tr>
<tr>
<td style="text-align:left"><code>torch.sum(input, dim)</code></td>
<td style="text-align:left">沿指定维度求和</td>
</tr>
<tr>
<td style="text-align:left"><code>torch.mean(input, dim)</code></td>
<td style="text-align:left">沿指定维度求均值（average）</td>
</tr>
<tr>
<td style="text-align:left"><code>torch.max(input, dim)</code></td>
<td style="text-align:left">沿指定维度求最大值</td>
</tr>
<tr>
<td style="text-align:left"><code>torch.min(input, dim)</code></td>
<td style="text-align:left">沿指定维度求最小值</td>
</tr>
<tr>
<td style="text-align:left"><code>torch.abs(input)</code></td>
<td style="text-align:left">逐元素绝对值</td>
</tr>
<tr>
<td style="text-align:left"><code>torch.clamp(input, min, max)</code></td>
<td style="text-align:left">将张量值限制在指定范围内</td>
</tr>
<tr>
<td style="text-align:left"><code>torch.round(input)</code></td>
<td style="text-align:left">逐元素四舍五入</td>
</tr>
<tr>
<td style="text-align:left"><code>torch.floor(input)</code></td>
<td style="text-align:left">逐元素向下取整（地板这一块）</td>
</tr>
<tr>
<td style="text-align:left"><code>torch.ceil(input)</code></td>
<td style="text-align:left">逐元素向上取整（天花板这一块）</td>
</tr>
</tbody>
</table>
</div>
<hr>
<h3 id="随机数生成"><a href="#随机数生成" class="headerlink" title="随机数生成"></a><strong>随机数生成</strong></h3><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left"><strong>函数</strong></th>
<th style="text-align:left"><strong>描述</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>torch.manual_seed(seed)</code></td>
<td style="text-align:left">设置随机种子</td>
</tr>
<tr>
<td style="text-align:left"><code>torch.initial_seed()</code></td>
<td style="text-align:left">返回当前随机种子</td>
</tr>
<tr>
<td style="text-align:left"><code>torch.rand(*size)</code></td>
<td style="text-align:left">创建均匀分布随机张量（范围 [0, 1)）</td>
</tr>
<tr>
<td style="text-align:left"><code>torch.randn(*size)</code></td>
<td style="text-align:left">创建标准正态分布随机张量</td>
</tr>
<tr>
<td style="text-align:left"><code>torch.randint(low, high, size)</code></td>
<td style="text-align:left">创建整数随机张量</td>
</tr>
<tr>
<td style="text-align:left"><code>torch.randperm(n)</code></td>
<td style="text-align:left">返回 0 到 n-1 的随机排列</td>
</tr>
</tbody>
</table>
</div>
<hr>
<h3 id="线性代数"><a href="#线性代数" class="headerlink" title="线性代数"></a><strong>线性代数</strong></h3><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left"><strong>函数</strong></th>
<th style="text-align:left"><strong>描述</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>torch.dot(input, other)</code></td>
<td style="text-align:left">计算两个向量的点积</td>
</tr>
<tr>
<td style="text-align:left"><code>torch.mm(input, mat2)</code></td>
<td style="text-align:left">矩阵乘法（matrix multiply）</td>
</tr>
<tr>
<td style="text-align:left"><code>torch.bmm(input, mat2)</code></td>
<td style="text-align:left">批量矩阵乘法（batch mm）</td>
</tr>
<tr>
<td style="text-align:left"><code>torch.eig(input)</code></td>
<td style="text-align:left">计算矩阵的特征值和特征向量</td>
</tr>
<tr>
<td style="text-align:left"><code>torch.svd(input)</code></td>
<td style="text-align:left">计算矩阵的奇异值分解</td>
</tr>
<tr>
<td style="text-align:left"><code>torch.inverse(input)</code></td>
<td style="text-align:left">计算矩阵的逆</td>
</tr>
<tr>
<td style="text-align:left"><code>torch.det(input)</code></td>
<td style="text-align:left">计算矩阵的行列式</td>
</tr>
<tr>
<td style="text-align:left"><code>torch.trace(input)</code></td>
<td style="text-align:left">计算矩阵的迹</td>
</tr>
</tbody>
</table>
</div>
<hr>
<h3 id="设备管理"><a href="#设备管理" class="headerlink" title="设备管理"></a><strong>设备管理</strong></h3><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left"><strong>函数</strong></th>
<th style="text-align:left"><strong>描述</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>torch.cuda.is_available()</code></td>
<td style="text-align:left">检查 CUDA 是否可用</td>
</tr>
<tr>
<td style="text-align:left"><code>torch.device(device)</code></td>
<td style="text-align:left">创建一个设备对象（如 <code>&#39;cpu&#39;</code> 或 <code>&#39;cuda:0&#39;</code>）</td>
</tr>
<tr>
<td style="text-align:left"><code>torch.to(device)</code></td>
<td style="text-align:left">将张量移动到指定设备</td>
</tr>
</tbody>
</table>
</div>
<h3 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建张量</span></span><br><span class="line">x = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">y = torch.zeros(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数学运算</span></span><br><span class="line">z = torch.add(x, <span class="number">1</span>)  <span class="comment"># 逐元素加 1</span></span><br><span class="line"><span class="built_in">print</span>(z)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 索引和切片</span></span><br><span class="line">mask = x &gt; <span class="number">1</span></span><br><span class="line">selected = torch.masked_select(x, mask)</span><br><span class="line"><span class="built_in">print</span>(selected)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设备管理</span></span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    device = torch.device(<span class="string">&#x27;cuda&#x27;</span>)</span><br><span class="line">    x = x.to(device)</span><br><span class="line">    <span class="built_in">print</span>(x.device)</span><br></pre></td></tr></table></figure>
<p>其中：</p>
<ul>
<li><strong>布尔掩码</strong>：使用布尔值（<code>True</code> / <code>False</code>）数组来选择或过滤另一个数组中特定元素的技术</li>
</ul>
<p>比如实例里的 <code>mask = x &gt; 1</code> （True为保留的元素，False为忽略）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">arr = np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>])</span><br><span class="line">mask = arr &gt; <span class="number">3</span>  <span class="comment"># 生成布尔掩码：[False, False, False, True, True, True]</span></span><br><span class="line"></span><br><span class="line">filtered = arr[mask]  <span class="comment"># 使用掩码筛选</span></span><br><span class="line"><span class="built_in">print</span>(filtered)  <span class="comment"># 输出: [4 5 6]</span></span><br></pre></td></tr></table></figure>
<h2 id="Pytorch-torch-nn（API）"><a href="#Pytorch-torch-nn（API）" class="headerlink" title="Pytorch torch.nn（API）"></a>Pytorch torch.nn（API）</h2><p><strong>1、nn.Module 类</strong>：</p>
<ul>
<li><code>nn.Module</code> 是所有自定义神经网络模型的基类。用户通常会从这个类派生自己的模型类，并在其中定义网络层结构以及前向传播函数（forward pass）</li>
</ul>
<p><strong>2、预定义层（Modules）</strong>：</p>
<ul>
<li>包括各种类型的层组件，例如卷积层（<code>nn.Conv1d</code>, <code>nn.Conv2d</code>, <code>nn.Conv3d</code>）、全连接层（<code>nn.Linear</code>）、激活函数（<code>nn.ReLU</code>, <code>nn.Sigmoid</code>, <code>nn.Tanh</code>）等</li>
</ul>
<p><strong>3、容器类</strong>：</p>
<ul>
<li><code>nn.Sequential</code>：允许将多个层按顺序组合起来，形成简单的线性堆叠网络</li>
<li><code>nn.ModuleList</code> 和 <code>nn.ModuleDict</code>：可以动态地存储和访问子模块，支持可变长度或命名的模块集合</li>
</ul>
<p><strong>4、损失函数（Loss Functions）</strong>：</p>
<ul>
<li><code>torch.nn</code> 包含了一系列用于衡量模型预测与真实标签之间差异的损失函数，例如均方误差损失（<code>nn.MSELoss</code>）、交叉熵损失（<code>nn.CrossEntropyLoss</code>）等</li>
</ul>
<p><strong>5、实用函数接口（Functional Interface）</strong>：</p>
<ul>
<li><code>nn.functional</code>（通常简写为 <code>F</code>），包含了许多可以直接作用于张量上的函数，它们实现了与层对象相同的功能，但不具有参数保存和更新的能力。例如，可以使用 <code>F.relu()</code> 直接进行 ReLU 操作，或者 <code>F.conv2d()</code> 进行卷积操作</li>
</ul>
<p><strong>6、初始化方法</strong>：</p>
<ul>
<li><code>torch.nn.init</code> 提供了一些常用的权重初始化策略，比如 Xavier 初始化 (<code>nn.init.xavier_uniform_()</code>) 和 Kaiming 初始化 (<code>nn.init.kaiming_uniform_()</code>)，这些对于成功训练神经网络至关重要</li>
</ul>
<h3 id="神经网络容器"><a href="#神经网络容器" class="headerlink" title="神经网络容器"></a><strong>神经网络容器</strong></h3><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left"><strong>类/函数</strong></th>
<th style="text-align:left"><strong>描述</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>torch.nn.Module</code></td>
<td style="text-align:left">所有神经网络模块的基类</td>
</tr>
<tr>
<td style="text-align:left"><code>torch.nn.Sequential(*args)</code></td>
<td style="text-align:left">按顺序组合多个模块</td>
</tr>
<tr>
<td style="text-align:left"><code>torch.nn.ModuleList(modules)</code></td>
<td style="text-align:left">将子模块存储在列表中</td>
</tr>
<tr>
<td style="text-align:left"><code>torch.nn.ModuleDict(modules)</code></td>
<td style="text-align:left">将子模块存储在字典中</td>
</tr>
<tr>
<td style="text-align:left"><code>torch.nn.ParameterList(parameters)</code></td>
<td style="text-align:left">将参数存储在列表中</td>
</tr>
<tr>
<td style="text-align:left"><code>torch.nn.ParameterDict(parameters)</code></td>
<td style="text-align:left">将参数存储在字典中</td>
</tr>
</tbody>
</table>
</div>
<hr>
<h3 id="线性层"><a href="#线性层" class="headerlink" title="线性层"></a><strong>线性层</strong></h3><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left"><strong>类/函数</strong></th>
<th style="text-align:left"><strong>描述</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>torch.nn.Linear(in_features, out_features)</code></td>
<td style="text-align:left">全连接层</td>
</tr>
<tr>
<td style="text-align:left"><code>torch.nn.Bilinear(in1_features, in2_features, out_features)</code></td>
<td style="text-align:left">双线性层</td>
</tr>
</tbody>
</table>
</div>
<hr>
<h3 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a><strong>卷积层</strong></h3><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left"><strong>类/函数</strong></th>
<th style="text-align:left"><strong>描述</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>torch.nn.Conv1d(in_channels, out_channels, kernel_size)</code></td>
<td style="text-align:left">一维卷积层</td>
</tr>
<tr>
<td style="text-align:left"><code>torch.nn.Conv2d(in_channels, out_channels, kernel_size)</code></td>
<td style="text-align:left">二维卷积层</td>
</tr>
<tr>
<td style="text-align:left"><code>torch.nn.Conv3d(in_channels, out_channels, kernel_size)</code></td>
<td style="text-align:left">三维卷积层</td>
</tr>
<tr>
<td style="text-align:left"><code>torch.nn.ConvTranspose1d(in_channels, out_channels, kernel_size)</code></td>
<td style="text-align:left">一维转置卷积层</td>
</tr>
<tr>
<td style="text-align:left"><code>torch.nn.ConvTranspose2d(in_channels, out_channels, kernel_size)</code></td>
<td style="text-align:left">二维转置卷积层</td>
</tr>
<tr>
<td style="text-align:left"><code>torch.nn.ConvTranspose3d(in_channels, out_channels, kernel_size)</code></td>
<td style="text-align:left">三维转置卷积层</td>
</tr>
</tbody>
</table>
</div>
<hr>
<h3 id="池化层"><a href="#池化层" class="headerlink" title="池化层"></a><strong>池化层</strong></h3><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left"><strong>类/函数</strong></th>
<th style="text-align:left"><strong>描述</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>torch.nn.MaxPool1d(kernel_size)</code></td>
<td style="text-align:left">一维最大池化层</td>
</tr>
<tr>
<td style="text-align:left"><code>torch.nn.MaxPool2d(kernel_size)</code></td>
<td style="text-align:left">二维最大池化层</td>
</tr>
<tr>
<td style="text-align:left"><code>torch.nn.MaxPool3d(kernel_size)</code></td>
<td style="text-align:left">三维最大池化层</td>
</tr>
<tr>
<td style="text-align:left"><code>torch.nn.AvgPool1d(kernel_size)</code></td>
<td style="text-align:left">一维平均池化层</td>
</tr>
<tr>
<td style="text-align:left"><code>torch.nn.AvgPool2d(kernel_size)</code></td>
<td style="text-align:left">二维平均池化层</td>
</tr>
<tr>
<td style="text-align:left"><code>torch.nn.AvgPool3d(kernel_size)</code></td>
<td style="text-align:left">三维平均池化层</td>
</tr>
<tr>
<td style="text-align:left"><code>torch.nn.AdaptiveMaxPool1d(output_size)</code></td>
<td style="text-align:left">一维自适应最大池化层</td>
</tr>
<tr>
<td style="text-align:left"><code>torch.nn.AdaptiveAvgPool1d(output_size)</code></td>
<td style="text-align:left">一维自适应平均池化层</td>
</tr>
<tr>
<td style="text-align:left"><code>torch.nn.AdaptiveMaxPool2d(output_size)</code></td>
<td style="text-align:left">二维自适应最大池化层</td>
</tr>
<tr>
<td style="text-align:left"><code>torch.nn.AdaptiveAvgPool2d(output_size)</code></td>
<td style="text-align:left">二维自适应平均池化层</td>
</tr>
<tr>
<td style="text-align:left"><code>torch.nn.AdaptiveMaxPool3d(output_size)</code></td>
<td style="text-align:left">三维自适应最大池化层</td>
</tr>
<tr>
<td style="text-align:left"><code>torch.nn.AdaptiveAvgPool3d(output_size)</code></td>
<td style="text-align:left">三维自适应平均池化层</td>
</tr>
</tbody>
</table>
</div>
<hr>
<h3 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a><strong>激活函数</strong></h3><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left"><strong>类/函数</strong></th>
<th style="text-align:left"><strong>描述</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>torch.nn.ReLU()</code></td>
<td style="text-align:left">ReLU 激活函数</td>
</tr>
<tr>
<td style="text-align:left"><code>torch.nn.Sigmoid()</code></td>
<td style="text-align:left">Sigmoid 激活函数</td>
</tr>
<tr>
<td style="text-align:left"><code>torch.nn.Tanh()</code></td>
<td style="text-align:left">Tanh 激活函数</td>
</tr>
<tr>
<td style="text-align:left"><code>torch.nn.Softmax(dim)</code></td>
<td style="text-align:left">Softmax 激活函数</td>
</tr>
<tr>
<td style="text-align:left"><code>torch.nn.LogSoftmax(dim)</code></td>
<td style="text-align:left">LogSoftmax 激活函数</td>
</tr>
<tr>
<td style="text-align:left"><code>torch.nn.LeakyReLU(negative_slope)</code></td>
<td style="text-align:left">LeakyReLU 激活函数</td>
</tr>
<tr>
<td style="text-align:left"><code>torch.nn.ELU(alpha)</code></td>
<td style="text-align:left">ELU 激活函数</td>
</tr>
<tr>
<td style="text-align:left"><code>torch.nn.SELU()</code></td>
<td style="text-align:left">SELU 激活函数</td>
</tr>
<tr>
<td style="text-align:left"><code>torch.nn.GELU()</code></td>
<td style="text-align:left">GELU 激活函数</td>
</tr>
</tbody>
</table>
</div>
<hr>
<h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a><strong>损失函数</strong></h3><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left"><strong>类/函数</strong></th>
<th style="text-align:left"><strong>描述</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>torch.nn.MSELoss()</code></td>
<td style="text-align:left">均方误差损失</td>
</tr>
<tr>
<td style="text-align:left"><code>torch.nn.L1Loss()</code></td>
<td style="text-align:left">L1 损失</td>
</tr>
<tr>
<td style="text-align:left"><code>torch.nn.CrossEntropyLoss()</code></td>
<td style="text-align:left">交叉熵损失</td>
</tr>
<tr>
<td style="text-align:left"><code>torch.nn.NLLLoss()</code></td>
<td style="text-align:left">负对数似然损失</td>
</tr>
<tr>
<td style="text-align:left"><code>torch.nn.BCELoss()</code></td>
<td style="text-align:left">二分类交叉熵损失</td>
</tr>
<tr>
<td style="text-align:left"><code>torch.nn.BCEWithLogitsLoss()</code></td>
<td style="text-align:left">带 Sigmoid 的二分类交叉熵损失</td>
</tr>
<tr>
<td style="text-align:left"><code>torch.nn.KLDivLoss()</code></td>
<td style="text-align:left">KL 散度损失</td>
</tr>
<tr>
<td style="text-align:left"><code>torch.nn.HingeEmbeddingLoss()</code></td>
<td style="text-align:left">铰链嵌入损失</td>
</tr>
<tr>
<td style="text-align:left"><code>torch.nn.MultiMarginLoss()</code></td>
<td style="text-align:left">多分类间隔损失</td>
</tr>
<tr>
<td style="text-align:left"><code>torch.nn.SmoothL1Loss()</code></td>
<td style="text-align:left">平滑 L1 损失</td>
</tr>
</tbody>
</table>
</div>
<hr>
<h3 id="归一化层"><a href="#归一化层" class="headerlink" title="归一化层"></a><strong>归一化层</strong></h3><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left"><strong>类/函数</strong></th>
<th style="text-align:left"><strong>描述</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>torch.nn.BatchNorm1d(num_features)</code></td>
<td style="text-align:left">一维批归一化层</td>
</tr>
<tr>
<td style="text-align:left"><code>torch.nn.BatchNorm2d(num_features)</code></td>
<td style="text-align:left">二维批归一化层</td>
</tr>
<tr>
<td style="text-align:left"><code>torch.nn.BatchNorm3d(num_features)</code></td>
<td style="text-align:left">三维批归一化层</td>
</tr>
<tr>
<td style="text-align:left"><code>torch.nn.LayerNorm(normalized_shape)</code></td>
<td style="text-align:left">层归一化</td>
</tr>
<tr>
<td style="text-align:left"><code>torch.nn.InstanceNorm1d(num_features)</code></td>
<td style="text-align:left">一维实例归一化层</td>
</tr>
<tr>
<td style="text-align:left"><code>torch.nn.InstanceNorm2d(num_features)</code></td>
<td style="text-align:left">二维实例归一化层</td>
</tr>
<tr>
<td style="text-align:left"><code>torch.nn.InstanceNorm3d(num_features)</code></td>
<td style="text-align:left">三维实例归一化层</td>
</tr>
<tr>
<td style="text-align:left"><code>torch.nn.GroupNorm(num_groups, num_channels)</code></td>
<td style="text-align:left">组归一化</td>
</tr>
</tbody>
</table>
</div>
<hr>
<h3 id="循环神经网络层"><a href="#循环神经网络层" class="headerlink" title="循环神经网络层"></a><strong>循环神经网络层</strong></h3><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left"><strong>类/函数</strong></th>
<th style="text-align:left"><strong>描述</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>torch.nn.RNN(input_size, hidden_size)</code></td>
<td style="text-align:left">简单 RNN 层</td>
</tr>
<tr>
<td style="text-align:left"><code>torch.nn.LSTM(input_size, hidden_size)</code></td>
<td style="text-align:left">LSTM 层</td>
</tr>
<tr>
<td style="text-align:left"><code>torch.nn.GRU(input_size, hidden_size)</code></td>
<td style="text-align:left">GRU 层</td>
</tr>
<tr>
<td style="text-align:left"><code>torch.nn.RNNCell(input_size, hidden_size)</code></td>
<td style="text-align:left">简单 RNN 单元</td>
</tr>
<tr>
<td style="text-align:left"><code>torch.nn.LSTMCell(input_size, hidden_size)</code></td>
<td style="text-align:left">LSTM 单元</td>
</tr>
<tr>
<td style="text-align:left"><code>torch.nn.GRUCell(input_size, hidden_size)</code></td>
<td style="text-align:left">GRU 单元</td>
</tr>
</tbody>
</table>
</div>
<hr>
<h3 id="嵌入层"><a href="#嵌入层" class="headerlink" title="嵌入层"></a><strong>嵌入层</strong></h3><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left"><strong>类/函数</strong></th>
<th style="text-align:left"><strong>描述</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>torch.nn.Embedding(num_embeddings, embedding_dim)</code></td>
<td style="text-align:left">嵌入层</td>
</tr>
</tbody>
</table>
</div>
<hr>
<h3 id="Dropout-层"><a href="#Dropout-层" class="headerlink" title="Dropout 层"></a><strong>Dropout 层</strong></h3><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left"><strong>类/函数</strong></th>
<th style="text-align:left"><strong>描述</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>torch.nn.Dropout(p)</code></td>
<td style="text-align:left">Dropout 层</td>
</tr>
<tr>
<td style="text-align:left"><code>torch.nn.Dropout2d(p)</code></td>
<td style="text-align:left">2D Dropout 层</td>
</tr>
<tr>
<td style="text-align:left"><code>torch.nn.Dropout3d(p)</code></td>
<td style="text-align:left">3D Dropout 层</td>
</tr>
</tbody>
</table>
</div>
<hr>
<h3 id="实用函数"><a href="#实用函数" class="headerlink" title="实用函数"></a><strong>实用函数</strong></h3><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left"><strong>函数</strong></th>
<th style="text-align:left"><strong>描述</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>torch.nn.functional.relu(input)</code></td>
<td style="text-align:left">应用 ReLU 激活函数</td>
</tr>
<tr>
<td style="text-align:left"><code>torch.nn.functional.sigmoid(input)</code></td>
<td style="text-align:left">应用 Sigmoid 激活函数</td>
</tr>
<tr>
<td style="text-align:left"><code>torch.nn.functional.softmax(input, dim)</code></td>
<td style="text-align:left">应用 Softmax 激活函数</td>
</tr>
<tr>
<td style="text-align:left"><code>torch.nn.functional.cross_entropy(input, target)</code></td>
<td style="text-align:left">计算交叉熵损失</td>
</tr>
<tr>
<td style="text-align:left"><code>torch.nn.functional.mse_loss(input, target)</code></td>
<td style="text-align:left">计算均方误差损失</td>
</tr>
</tbody>
</table>
</div>
<hr>
<h2 id="张量（tensor）"><a href="#张量（tensor）" class="headerlink" title="张量（tensor）"></a>张量（tensor）</h2><p>张量（Tensor）是 PyTorch 中的核心数据结构，用于存储和操作多维数组</p>
<p>张量可以视为一个多维数组，支持加速计算的操作</p>
<p>在 PyTorch 中，张量的概念类似于 NumPy 中的数组，但是 PyTorch 的张量可以运行在不同的设备上，比如 CPU 和 GPU，这使得它们非常适合于进行大规模并行计算，特别是在深度学习领域。</p>
<ul>
<li><strong>维度（Dimensionality）</strong>：张量的维度指的是数据的多维数组结构。例如，一个标量（0维张量）是一个单独的数字，一个向量（1维张量）是一个一维数组，一个矩阵（2维张量）是一个二维数组，以此类推</li>
<li><strong>形状（Shape）</strong>：张量的形状是指每个维度上的大小。例如，一个形状为<code>(3, 4)</code>的张量意味着它有3行4列</li>
<li><strong>数据类型（Dtype）</strong>：张量中的数据类型定义了存储每个元素所需的内存大小和解释方式。PyTorch支持多种数据类型，包括整数型（如<code>torch.int8</code>、<code>torch.int32</code>）、浮点型（如<code>torch.float32</code>、<code>torch.float64</code>）和布尔型（<code>torch.bool</code>）</li>
</ul>
<p>张量可以存储在 CPU 或 GPU 中，GPU 张量可显著加速计算，先放张菜鸟教程的图：</p>
<p><img src="/img/lazy.gif" data-original="1.jpg" alt=""></p>
<h3 id="张量基本方式"><a href="#张量基本方式" class="headerlink" title="张量基本方式"></a>张量基本方式</h3><p>但感觉直接说概念有点太过于抽象，不如先看看几个基本方式：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left"><strong>方法</strong></th>
<th style="text-align:left"><strong>说明</strong></th>
<th style="text-align:left"><strong>示例代码</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>torch.tensor(data)</code></td>
<td style="text-align:left">从 Python 列表或 NumPy 数组创建张量</td>
<td style="text-align:left"><code>x = torch.tensor([[1, 2], [3, 4]])</code></td>
</tr>
<tr>
<td style="text-align:left"><code>torch.zeros(size)</code></td>
<td style="text-align:left">创建一个全为零的张量</td>
<td style="text-align:left"><code>x = torch.zeros((2, 3))</code></td>
</tr>
<tr>
<td style="text-align:left"><code>torch.ones(size)</code></td>
<td style="text-align:left">创建一个全为 1 的张量</td>
<td style="text-align:left"><code>x = torch.ones((2, 3))</code></td>
</tr>
<tr>
<td style="text-align:left"><code>torch.empty(size)</code></td>
<td style="text-align:left">创建一个未初始化的张量</td>
<td style="text-align:left"><code>x = torch.empty((2, 3))</code></td>
</tr>
<tr>
<td style="text-align:left"><code>torch.rand(size)</code></td>
<td style="text-align:left">创建一个服从均匀分布的随机张量，值在 <code>[0, 1)</code></td>
<td style="text-align:left"><code>x = torch.rand((2, 3))</code></td>
</tr>
<tr>
<td style="text-align:left"><code>torch.randn(size)</code></td>
<td style="text-align:left">创建一个服从正态分布的随机张量，均值为 0，标准差为 1</td>
<td style="text-align:left"><code>x = torch.randn((2, 3))</code></td>
</tr>
<tr>
<td style="text-align:left"><code>torch.arange(start, end, step)</code></td>
<td style="text-align:left">创建一个一维序列张量，类似于 Python 的 <code>range</code></td>
<td style="text-align:left"><code>x = torch.arange(0, 10, 2)</code></td>
</tr>
<tr>
<td style="text-align:left"><code>torch.linspace(start, end, steps)</code></td>
<td style="text-align:left">创建一个在指定范围内等间隔的序列张量</td>
<td style="text-align:left"><code>x = torch.linspace(0, 1, 5)</code></td>
</tr>
<tr>
<td style="text-align:left"><code>torch.eye(size)</code></td>
<td style="text-align:left">创建一个单位矩阵（对角线为 1，其他为 0）</td>
<td style="text-align:left"><code>x = torch.eye(3)</code></td>
</tr>
<tr>
<td style="text-align:left"><code>torch.from_numpy(ndarray)</code></td>
<td style="text-align:left">将 NumPy 数组转换为张量</td>
<td style="text-align:left"><code>x = torch.from_numpy(np.array([1, 2, 3]))</code></td>
</tr>
</tbody>
</table>
</div>
<h3 id="张量基本属性"><a href="#张量基本属性" class="headerlink" title="张量基本属性"></a>张量基本属性</h3><p>然后张量的属性：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left"><strong>属性</strong></th>
<th style="text-align:left"><strong>说明</strong></th>
<th style="text-align:left"><strong>示例</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>.shape</code></td>
<td style="text-align:left">获取张量的形状</td>
<td style="text-align:left"><code>tensor.shape</code></td>
</tr>
<tr>
<td style="text-align:left"><code>.size()</code></td>
<td style="text-align:left">获取张量的形状</td>
<td style="text-align:left"><code>tensor.size()</code></td>
</tr>
<tr>
<td style="text-align:left"><code>.dtype</code></td>
<td style="text-align:left">获取张量的数据类型</td>
<td style="text-align:left"><code>tensor.dtype</code></td>
</tr>
<tr>
<td style="text-align:left"><code>.device</code></td>
<td style="text-align:left">查看张量所在的设备 (CPU/GPU)</td>
<td style="text-align:left"><code>tensor.device</code></td>
</tr>
<tr>
<td style="text-align:left"><code>.dim()</code></td>
<td style="text-align:left">获取张量的维度数</td>
<td style="text-align:left"><code>tensor.dim()</code></td>
</tr>
<tr>
<td style="text-align:left"><code>.requires_grad</code></td>
<td style="text-align:left">是否启用梯度计算</td>
<td style="text-align:left"><code>tensor.requires_grad</code></td>
</tr>
<tr>
<td style="text-align:left"><code>.numel()</code></td>
<td style="text-align:left">获取张量中的元素总数</td>
<td style="text-align:left"><code>tensor.numel()</code></td>
</tr>
<tr>
<td style="text-align:left"><code>.is_cuda</code></td>
<td style="text-align:left">检查张量是否在 GPU 上</td>
<td style="text-align:left"><code>tensor.is_cuda</code></td>
</tr>
<tr>
<td style="text-align:left"><code>.T</code></td>
<td style="text-align:left">获取张量的转置（适用于 2D 张量）</td>
<td style="text-align:left"><code>tensor.T</code></td>
</tr>
<tr>
<td style="text-align:left"><code>.item()</code></td>
<td style="text-align:left">获取单元素张量的值</td>
<td style="text-align:left"><code>tensor.item()</code></td>
</tr>
<tr>
<td style="text-align:left"><code>.is_contiguous()</code></td>
<td style="text-align:left">检查张量是否连续存储</td>
<td style="text-align:left"><code>tensor.is_contiguous()</code></td>
</tr>
</tbody>
</table>
</div>
<p>看了上面之后，结合下面示例可以更加理解：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个 2D 张量</span></span><br><span class="line">tensor = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]], dtype=torch.float32)  </span><br><span class="line"><span class="comment">#指定为浮点数，也可以自己尝试 torch. 然后可以看到其他的变量类型</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 张量的属性</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Tensor:\n&quot;</span>, tensor)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Shape:&quot;</span>, tensor.shape)  <span class="comment"># 获取形状</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Size:&quot;</span>, tensor.size())  <span class="comment"># 获取形状（另一种方法）</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Data Type:&quot;</span>, tensor.dtype)  <span class="comment"># 数据类型</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Device:&quot;</span>, tensor.device)  <span class="comment"># 设备</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Dimensions:&quot;</span>, tensor.dim())  <span class="comment"># 维度数</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Total Elements:&quot;</span>, tensor.numel())  <span class="comment"># 元素总数</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Requires Grad:&quot;</span>, tensor.requires_grad)  <span class="comment"># 是否启用梯度</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Is CUDA:&quot;</span>, tensor.is_cuda)  <span class="comment"># 是否在 GPU 上</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Is Contiguous:&quot;</span>, tensor.is_contiguous())  <span class="comment"># 是否连续存储</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取单元素值</span></span><br><span class="line">single_value = torch.tensor(<span class="number">42</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Single Element Value:&quot;</span>, single_value.item())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 转置张量</span></span><br><span class="line">tensor_T = tensor.T</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Transposed Tensor:\n&quot;</span>, tensor_T)</span><br></pre></td></tr></table></figure>
<p>输出结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">Tensor:</span><br><span class="line"> tensor([[<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>],</span><br><span class="line">         [<span class="number">4.</span>, <span class="number">5.</span>, <span class="number">6.</span>]])</span><br><span class="line">Shape: torch.Size([<span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">Size: torch.Size([<span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">Data <span class="type">Type</span>: torch.float32</span><br><span class="line">Device: cpu</span><br><span class="line">Dimensions: <span class="number">2</span></span><br><span class="line">Total Elements: <span class="number">6</span></span><br><span class="line">Requires Grad: <span class="literal">False</span></span><br><span class="line">Is CUDA: <span class="literal">False</span></span><br><span class="line">Is Contiguous: <span class="literal">True</span></span><br><span class="line">Single Element Value: <span class="number">42</span></span><br><span class="line">Transposed Tensor:</span><br><span class="line"> tensor([[<span class="number">1.</span>, <span class="number">4.</span>],</span><br><span class="line">         [<span class="number">2.</span>, <span class="number">5.</span>],</span><br><span class="line">         [<span class="number">3.</span>, <span class="number">6.</span>]])</span><br></pre></td></tr></table></figure>
<p>除此之外还有个是<code>torch.stack((x, x), dim=?)</code>，这里表示将张量x和x在第几维度（dim）进行堆叠，比如说dim=0,1,2，这边类比到坐标轴上，那么</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#dim=0 Z轴  层数+1（层）</span><br><span class="line">#dim=1 Y轴  行数+1（行）</span><br><span class="line">#dim=2 X轴  列数+1（列）</span><br></pre></td></tr></table></figure>
<p>可以具体看个例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">a = torch.randn(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"><span class="comment">#b = torch.randn(2, 3)</span></span><br><span class="line"><span class="comment">#c = torch.eye(3)</span></span><br><span class="line"><span class="comment">#d = torch.from_numpy(np.array([[1, 2], [3, 4]]))</span></span><br><span class="line">e = torch.stack((a, a), dim=<span class="number">0</span>)</span><br><span class="line">f = torch.stack((a, a), dim=<span class="number">1</span>)</span><br><span class="line">g = torch.stack((a, a), dim=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot; &quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;===== a =====&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot; &quot;</span>)</span><br><span class="line"><span class="comment">#print(b)</span></span><br><span class="line"><span class="comment">#print(c)</span></span><br><span class="line"><span class="comment">#print(c.shape)</span></span><br><span class="line"><span class="comment">#print(d)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;===== dim=0 =====&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(e)</span><br><span class="line"><span class="built_in">print</span>(e.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot; &quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;===== dim=1 =====&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(f)</span><br><span class="line"><span class="built_in">print</span>(f.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot; &quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;===== dim=2 =====&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(g)</span><br><span class="line"><span class="built_in">print</span>(g.shape)</span><br></pre></td></tr></table></figure>
<p>运行后结果是：（size里的[2, 3, 4]就分别对应0,1,2）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">===== a =====</span><br><span class="line">tensor([[<span class="number">0.3607</span>, <span class="number">0.9021</span>, <span class="number">0.0285</span>, <span class="number">0.0186</span>],</span><br><span class="line">        [<span class="number">0.1229</span>, <span class="number">0.5829</span>, <span class="number">0.8991</span>, <span class="number">0.4807</span>],</span><br><span class="line">        [<span class="number">0.0543</span>, <span class="number">0.2008</span>, <span class="number">0.0578</span>, <span class="number">0.4865</span>]])</span><br><span class="line"> </span><br><span class="line">===== dim=<span class="number">0</span> =====                                 // 直接层数 + <span class="number">1</span>（原本层数为<span class="number">1</span>，在dim=<span class="number">0</span>进行两个x的堆叠所以层数为<span class="number">2</span>）</span><br><span class="line">tensor([[[<span class="number">0.3607</span>, <span class="number">0.9021</span>, <span class="number">0.0285</span>, <span class="number">0.0186</span>],</span><br><span class="line">         [<span class="number">0.1229</span>, <span class="number">0.5829</span>, <span class="number">0.8991</span>, <span class="number">0.4807</span>],</span><br><span class="line">         [<span class="number">0.0543</span>, <span class="number">0.2008</span>, <span class="number">0.0578</span>, <span class="number">0.4865</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">0.3607</span>, <span class="number">0.9021</span>, <span class="number">0.0285</span>, <span class="number">0.0186</span>],</span><br><span class="line">         [<span class="number">0.1229</span>, <span class="number">0.5829</span>, <span class="number">0.8991</span>, <span class="number">0.4807</span>],</span><br><span class="line">         [<span class="number">0.0543</span>, <span class="number">0.2008</span>, <span class="number">0.0578</span>, <span class="number">0.4865</span>]]])</span><br><span class="line">torch.Size([<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line"> </span><br><span class="line">===== dim=<span class="number">1</span> =====                                // 直接行数 + <span class="number">1</span>（原本层数为<span class="number">1</span>，在dim=<span class="number">1</span>进行两个x的堆叠）</span><br><span class="line">tensor([[[<span class="number">0.3607</span>, <span class="number">0.9021</span>, <span class="number">0.0285</span>, <span class="number">0.0186</span>],       // 此时由于<span class="number">1</span>对应行，所以层变为<span class="number">3</span>，每层函数变为<span class="number">2</span></span><br><span class="line">         [<span class="number">0.3607</span>, <span class="number">0.9021</span>, <span class="number">0.0285</span>, <span class="number">0.0186</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">0.1229</span>, <span class="number">0.5829</span>, <span class="number">0.8991</span>, <span class="number">0.4807</span>],</span><br><span class="line">         [<span class="number">0.1229</span>, <span class="number">0.5829</span>, <span class="number">0.8991</span>, <span class="number">0.4807</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">0.0543</span>, <span class="number">0.2008</span>, <span class="number">0.0578</span>, <span class="number">0.4865</span>],</span><br><span class="line">         [<span class="number">0.0543</span>, <span class="number">0.2008</span>, <span class="number">0.0578</span>, <span class="number">0.4865</span>]]])</span><br><span class="line">torch.Size([<span class="number">3</span>, <span class="number">2</span>, <span class="number">4</span>])</span><br><span class="line"> </span><br><span class="line">===== dim=<span class="number">2</span> =====                                // 直接列数 + <span class="number">1</span></span><br><span class="line">tensor([[[<span class="number">0.3607</span>, <span class="number">0.3607</span>],                       // 将原本a的列转换为行然后copy一份（原理与上相同）</span><br><span class="line">         [<span class="number">0.9021</span>, <span class="number">0.9021</span>],</span><br><span class="line">         [<span class="number">0.0285</span>, <span class="number">0.0285</span>],</span><br><span class="line">         [<span class="number">0.0186</span>, <span class="number">0.0186</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">0.1229</span>, <span class="number">0.1229</span>],</span><br><span class="line">         [<span class="number">0.5829</span>, <span class="number">0.5829</span>],</span><br><span class="line">         [<span class="number">0.8991</span>, <span class="number">0.8991</span>],</span><br><span class="line">         [<span class="number">0.4807</span>, <span class="number">0.4807</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">0.0543</span>, <span class="number">0.0543</span>],</span><br><span class="line">         [<span class="number">0.2008</span>, <span class="number">0.2008</span>],</span><br><span class="line">         [<span class="number">0.0578</span>, <span class="number">0.0578</span>],</span><br><span class="line">         [<span class="number">0.4865</span>, <span class="number">0.4865</span>]]])</span><br><span class="line">torch.Size([<span class="number">3</span>, <span class="number">4</span>, <span class="number">2</span>])</span><br></pre></td></tr></table></figure>
<p>这样映射到XYZ轴后相比于直接写<strong>dim在新的维度上进行拼接</strong>，这个<strong>新的维度出现在第1维（从0开始计数）</strong>这样子好理解的多</p>
<h3 id="张量基本操作"><a href="#张量基本操作" class="headerlink" title="张量基本操作"></a>张量基本操作</h3><h4 id="基础操作："><a href="#基础操作：" class="headerlink" title="基础操作："></a>基础操作：</h4><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left"><strong>操作</strong></th>
<th style="text-align:left"><strong>说明</strong></th>
<th style="text-align:left"><strong>示例代码</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>+</code>, <code>-</code>, <code>*</code>, <code>/</code></td>
<td style="text-align:left">元素级加法、减法、乘法、除法</td>
<td style="text-align:left"><code>z = x + y</code></td>
</tr>
<tr>
<td style="text-align:left"><code>torch.matmul(x, y)</code></td>
<td style="text-align:left">矩阵乘法</td>
<td style="text-align:left"><code>z = torch.matmul(x, y)</code></td>
</tr>
<tr>
<td style="text-align:left"><code>torch.dot(x, y)</code></td>
<td style="text-align:left">向量点积（仅适用于 1D 张量）</td>
<td style="text-align:left"><code>z = torch.dot(x, y)</code></td>
</tr>
<tr>
<td style="text-align:left"><code>torch.sum(x)</code></td>
<td style="text-align:left">求和</td>
<td style="text-align:left"><code>z = torch.sum(x)</code></td>
</tr>
<tr>
<td style="text-align:left"><code>torch.mean(x)</code></td>
<td style="text-align:left">求均值</td>
<td style="text-align:left"><code>z = torch.mean(x)</code></td>
</tr>
<tr>
<td style="text-align:left"><code>torch.max(x)</code></td>
<td style="text-align:left">求最大值</td>
<td style="text-align:left"><code>z = torch.max(x)</code></td>
</tr>
<tr>
<td style="text-align:left"><code>torch.min(x)</code></td>
<td style="text-align:left">求最小值</td>
<td style="text-align:left"><code>z = torch.min(x)</code></td>
</tr>
<tr>
<td style="text-align:left"><code>torch.argmax(x, dim)</code></td>
<td style="text-align:left">返回最大值的索引（指定维度）</td>
<td style="text-align:left"><code>z = torch.argmax(x, dim=1)</code></td>
</tr>
<tr>
<td style="text-align:left"><code>torch.softmax(x, dim)</code></td>
<td style="text-align:left">计算 softmax（指定维度）</td>
<td style="text-align:left"><code>z = torch.softmax(x, dim=1)</code></td>
</tr>
</tbody>
</table>
</div>
<p> 这里大部分如果学过线性代数或者加减法的一般都很好理解，然后这里比较需要注意的就是torch.dot()、argmax()和softmax()，这里我们先看 <code>torch.dot()</code> ，这里的点积（适用于一维向量），其实就是我们数学中学的向量的点乘，具体公式为：</p>
<script type="math/tex; mode=display">
\mathbf{a} \cdot \mathbf{b} = a_1 b_1 + a_2 b_2 + \dots + a_n b_n = \sum_{i=1}^n a_i b_i</script><p>用torch来表示就是：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">a = torch.tensor([<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>])</span><br><span class="line">b = torch.tensor([<span class="number">4.0</span>, <span class="number">5.0</span>, <span class="number">6.0</span>])</span><br><span class="line"></span><br><span class="line">dot_product = torch.dot(a, b)</span><br><span class="line"><span class="built_in">print</span>(dot_product)  <span class="comment"># 输出: 1*4 + 2*5 + 3*6 = 32</span></span><br></pre></td></tr></table></figure>
<p>然后就是 <code>torch.argmax()</code>，返回最大值的索引，不过这个最大值的索引是指将数组展开来后的，比如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">a = torch.randn(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"><span class="built_in">print</span>(a.<span class="built_in">min</span>())</span><br><span class="line"><span class="built_in">print</span>(a.<span class="built_in">max</span>())</span><br><span class="line"><span class="built_in">print</span>(a.mean())</span><br><span class="line"><span class="built_in">print</span>(a.argmax())</span><br><span class="line"></span><br><span class="line"><span class="comment">#tensor([[-0.0678, -0.9911, -1.4649,  0.9039],</span></span><br><span class="line"><span class="comment">#        [ 1.1405,  0.7636, -0.0447, -0.4907],</span></span><br><span class="line"><span class="comment">#        [-0.8256, -1.1506,  0.7874, -0.5046]])</span></span><br><span class="line"><span class="comment">#tensor(-1.4649)</span></span><br><span class="line"><span class="comment">#tensor(1.1405)</span></span><br><span class="line"><span class="comment">#tensor(-0.1620)</span></span><br><span class="line"><span class="comment">#tensor(4)         &lt;--- 指的是1.1405，最开始从0开始数</span></span><br></pre></td></tr></table></figure>
<p>最后就是 <code>torch.softmax()</code> ，他是种常用的<strong>激活函数</strong>，主要用于<strong>多分类问题</strong>的输出层。它的作用是将一个 <strong>任意实数向量</strong> 转换成一个 <strong>概率分布</strong>，总结来说，就是每个数值的概率密度，对于输入如下向量：</p>
<script type="math/tex; mode=display">
\mathbf{z} = [z_1, z_2, \dots, z_n]</script><p>公式如下：</p>
<script type="math/tex; mode=display">
\text{softmax}(z_i) = \frac{e^{z_i}}{\sum_{j=1}^{n} e^{z_j}}</script><p>为了避免整数溢出，实际通常减去最大值：</p>
<script type="math/tex; mode=display">
\text{softmax}(z_i) = \frac{e^{z_i - \max(\mathbf{z})}}{\sum_{j=1}^{n} e^{z_j - \max(\mathbf{z})}}</script><p>用torch表示就是：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line">a = torch.tensor([[<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>]])</span><br><span class="line"></span><br><span class="line">probs = F.softmax(a, dim=<span class="number">1</span>)      <span class="comment"># dim=0是行方向，dim=1是列方向</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(probs)</span><br><span class="line"></span><br><span class="line"><span class="comment">#tensor([[0.0900, 0.2447, 0.6652]])</span></span><br></pre></td></tr></table></figure>
<h4 id="形状操作："><a href="#形状操作：" class="headerlink" title="形状操作："></a>形状操作：</h4><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left"><strong>操作</strong></th>
<th style="text-align:left"><strong>说明</strong></th>
<th style="text-align:left"><strong>示例代码</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>x.view(shape)</code></td>
<td style="text-align:left">改变张量的形状（不改变数据）</td>
<td style="text-align:left"><code>z = x.view(3, 4)</code></td>
</tr>
<tr>
<td style="text-align:left"><code>x.reshape(shape)</code></td>
<td style="text-align:left">类似于 <code>view</code>，但更灵活</td>
<td style="text-align:left"><code>z = x.reshape(3, 4)</code></td>
</tr>
<tr>
<td style="text-align:left"><code>x.t()</code></td>
<td style="text-align:left">转置矩阵</td>
<td style="text-align:left"><code>z = x.t()</code></td>
</tr>
<tr>
<td style="text-align:left"><code>x.unsqueeze(dim)</code></td>
<td style="text-align:left">在指定维度添加一个维度</td>
<td style="text-align:left"><code>z = x.unsqueeze(0)</code></td>
</tr>
<tr>
<td style="text-align:left"><code>x.squeeze(dim)</code></td>
<td style="text-align:left">去掉指定维度为 1 的维度</td>
<td style="text-align:left"><code>z = x.squeeze(0)</code></td>
</tr>
<tr>
<td style="text-align:left"><code>torch.cat((x, y), dim)</code></td>
<td style="text-align:left">按指定维度连接多个张量</td>
<td style="text-align:left"><code>z = torch.cat((x, y), dim=1)</code></td>
</tr>
<tr>
<td style="text-align:left"><code>x.flatten()</code></td>
<td style="text-align:left">将张量展平成一维</td>
<td style="text-align:left"><code>z = x.flatten()</code></td>
</tr>
</tbody>
</table>
</div>
<p>同样地，这里只介绍<code>unsqueez</code>，<code>squeeze</code> 和 <code>cat</code></p>
<p>先看个示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">a = torch.tensor([[<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>]])</span><br><span class="line">b = torch.tensor([[<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>]])</span><br><span class="line"></span><br><span class="line">c = torch.cat((a, b), dim=<span class="number">0</span>)</span><br><span class="line">d = torch.cat((a, b), dim=<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(c)</span><br><span class="line"><span class="built_in">print</span>(d)</span><br><span class="line"><span class="comment">#tensor([[1.0000, 2.0000, 3.0000],</span></span><br><span class="line"><span class="comment">#       [0.5000, 0.5000, 0.5000]])</span></span><br><span class="line"><span class="comment">#tensor([[1.0000, 2.0000, 3.0000, 0.5000, 0.5000, 0.5000]])</span></span><br></pre></td></tr></table></figure>
<p><code>dim=0</code>依旧是行方向，然后<code>dim=1</code>是列方向，用cat连接之后可以发现，c变为2行，而d变为6列</p>
<p>然后就是<code>squeeze</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">c = torch.randn(<span class="number">1</span>, <span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">e = c.squeeze(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(c)</span><br><span class="line"><span class="built_in">print</span>(e)</span><br><span class="line"></span><br><span class="line"><span class="comment">#tensor([[[-0.9388,  0.3443,  0.7916, -1.5823],</span></span><br><span class="line"><span class="comment">#         [-1.8765, -0.2879,  0.8101,  0.2462],</span></span><br><span class="line"><span class="comment">#         [-1.5106,  0.7375, -0.7615,  0.1300]]])</span></span><br><span class="line"><span class="comment">#tensor([[-0.9388,  0.3443,  0.7916, -1.5823],</span></span><br><span class="line"><span class="comment">#        [-1.8765, -0.2879,  0.8101,  0.2462],</span></span><br><span class="line"><span class="comment">#        [-1.5106,  0.7375, -0.7615,  0.1300]])</span></span><br></pre></td></tr></table></figure>
<p>很明显的可以看到，e被删去了0维（此时dim=0的大小是1，也就是c只有1层）</p>
<p>那么的，<code>unsqueeze</code> 同理</p>
<h3 id="GPU加速"><a href="#GPU加速" class="headerlink" title="GPU加速"></a>GPU加速</h3><p>将张量转移到 GPU：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">device = torch.device(<span class="string">&#x27;cuda&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line">x = torch.tensor([<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>], device=device)</span><br></pre></td></tr></table></figure>
<p>检查 GPU 是否可用：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.cuda.is_available()  <span class="comment"># 返回 True 或 False</span></span><br></pre></td></tr></table></figure>
<h3 id="张量与Numpy互相操作"><a href="#张量与Numpy互相操作" class="headerlink" title="张量与Numpy互相操作"></a>张量与Numpy互相操作</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">a = np.array([[<span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">2.0</span>, <span class="number">3.0</span>]])</span><br><span class="line">b = torch.from_numpy(a)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"><span class="built_in">print</span>(b)</span><br><span class="line"></span><br><span class="line"><span class="comment">#[[1. 2.]</span></span><br><span class="line"><span class="comment"># [2. 3.]]</span></span><br><span class="line"><span class="comment">#tensor([[1., 2.],</span></span><br><span class="line"><span class="comment">#        [2., 3.]], dtype=torch.float64)</span></span><br></pre></td></tr></table></figure>
<h2 id="神经网络基础"><a href="#神经网络基础" class="headerlink" title="神经网络基础"></a>神经网络基础</h2><p>此节点只做基本介绍，后面会有详细的单独介绍（公式部分也仅做了解）</p>
<h3 id="神经元（Neuron）"><a href="#神经元（Neuron）" class="headerlink" title="神经元（Neuron）"></a>神经元（Neuron）</h3><p>神经元是神经网络的基本单元，它接收输入信号，通过加权求和后与偏置（bias）相加，然后通过激活函数处理以产生输出</p>
<p>神经元的权重和偏置是网络学习过程中需要调整的参数，其公式如下：</p>
<script type="math/tex; mode=display">
z = \sum_{i=1}^{n} w_ix_i + Bias</script><p>其中 <script type="math/tex">w</script> 表示权重，<script type="math/tex">x</script> 表示输入，<script type="math/tex">Bias</script> 表示偏置</p>
<p>神经元接收多个输入（例如x1, x2, …, xn），如果输入的加权和大于激活阈值（activation potential），则产生二进制输出：</p>
<p><img src="/img/lazy.gif" data-original="1.png" alt=""></p>
<h3 id="层（Layer）"><a href="#层（Layer）" class="headerlink" title="层（Layer）"></a>层（Layer）</h3><p>输入层和输出层之间的层被称为隐藏层，层与层之间的连接密度和类型构成了网络的配置</p>
<p>神经网络由多个层组成，包括：</p>
<ul>
<li><strong>输入层（Input Layer）</strong>：接收原始输入数据</li>
<li><strong>隐藏层（Hidden Layer）</strong>：对输入数据进行处理，可以有多个隐藏层</li>
<li><strong>输出层（Output Layer）</strong>：产生最终的输出结果</li>
</ul>
<p>经典神经网络如下图：</p>
<p><img src="/img/lazy.gif" data-original="2.png" alt=""></p>
<p>这也是前馈神经网络的基本结构</p>
<h3 id="前馈神经网络（FNN）"><a href="#前馈神经网络（FNN）" class="headerlink" title="前馈神经网络（FNN）"></a>前馈神经网络（FNN）</h3><p>前馈神经网络（Feedforward Neural Network，FNN）是神经网络家族中的基本单元</p>
<p>前馈神经网络特点是数据从输入层开始，经过一个或多个隐藏层，最后到达输出层，全过程没有循环或反馈</p>
<p><strong>前馈神经网络的基本结构：</strong></p>
<ul>
<li><strong>输入层：</strong> 数据进入网络的入口点。输入层的每个节点代表一个输入特征</li>
<li><strong>隐藏层：</strong>一个或多个层，用于捕获数据的非线性特征。每个隐藏层由多个神经元组成，每个神经元通过激活函数增加非线性能力</li>
<li><strong>输出层：</strong>输出网络的预测结果。节点数和问题类型相关，例如分类问题的输出节点数等于类别数</li>
<li><strong>连接权重与偏置：</strong>每个神经元的输入通过权重进行加权求和，并加上偏置值，然后通过激活函数传递</li>
</ul>
<h3 id="循环神经网络（RNN）"><a href="#循环神经网络（RNN）" class="headerlink" title="循环神经网络（RNN）"></a>循环神经网络（RNN）</h3><p>循环神经网络（Recurrent Neural Network, RNN）络是一类专门处理序列数据的神经网络，能够捕获输入数据中时间或顺序信息的依赖关系</p>
<p>RNN 的特别之处在于它具有”记忆能力”，可以在网络的隐藏状态中保存之前时间步的信息</p>
<p>循环神经网络用于处理随时间变化的数据模式</p>
<p>在 RNN 中，相同的层被用来接收输入参数，并在指定的神经网络中显示输出参数</p>
<p><img src="/img/lazy.gif" data-original="3.png" alt=""></p>
<h3 id="卷积神经网络（CNN）"><a href="#卷积神经网络（CNN）" class="headerlink" title="卷积神经网络（CNN）"></a>卷积神经网络（CNN）</h3><p><strong>卷积神经网络</strong>（Convolutional Neural Network, CNN）是一种专门用于处理<strong>具有网格结构数据</strong>（如图像、视频、语音频谱图）的深度学习模型，通过<strong>局部连接、权值共享和池化</strong>等机制，自动学习空间层次特征，在计算机视觉任务中取得了巨大成功</p>
<p><img src="/img/lazy.gif" data-original="4.png" alt=""></p>
<h3 id="实例-1"><a href="#实例-1" class="headerlink" title="实例"></a>实例</h3><p>在 PyTorch 中，构建神经网络通常需要继承 nn.Module 类</p>
<p>nn.Module 是所有神经网络模块的基类，你需要定义以下两个部分：</p>
<ul>
<li><strong><code>__init__()</code></strong>：定义网络层</li>
<li><strong><code>forward()</code></strong>：定义数据的前向传播过程</li>
</ul>
<p>简单的全连接神经网络（Fully Connected Network）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SimpleNN</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(SimpleNN, <span class="variable language_">self</span>).__init__()  </span><br><span class="line">        <span class="comment"># 定义一个输入层到隐藏层的全连接层</span></span><br><span class="line">        <span class="variable language_">self</span>.fc1 = nn.Linear(<span class="number">2</span>, <span class="number">2</span>)      <span class="comment"># 输入两个特征，输出两个特征</span></span><br><span class="line">        <span class="variable language_">self</span>.fc2 = nn.Linear(<span class="number">2</span>, <span class="number">1</span>)      <span class="comment"># 输入两个特征，输出一个特征</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = torch.relu(<span class="variable language_">self</span>.fc1(x))     <span class="comment"># 使用relu激活函数</span></span><br><span class="line">        x = <span class="variable language_">self</span>.fc2(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    </span><br><span class="line">model = SimpleNN()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(model)</span><br><span class="line"></span><br><span class="line"><span class="comment">#SimpleNN(</span></span><br><span class="line"><span class="comment">#  (fc1): Linear(in_features=2, out_features=2, bias=True)</span></span><br><span class="line"><span class="comment">#  (fc2): Linear(in_features=2, out_features=1, bias=True)</span></span><br><span class="line"><span class="comment">#)</span></span><br></pre></td></tr></table></figure>
<p>初次看到可能有些地方会不理解，下面介绍几个：</p>
<p><code>super(SimpleNN, self).__init__()</code>  <strong>调用父类的初始化方法</strong></p>
<p><code>relu</code> 是最常用的激活函数之一，其定义为：</p>
<script type="math/tex; mode=display">
Relu(x) = \max(0,x)</script><p>relu计算简单，只需判断正负，而且正区梯度恒为1，利于训练</p>
<p>PyTorch 提供了许多常见的神经网络层，以下是几个常见的：</p>
<ul>
<li><strong><code>nn.Linear(in_features, out_features)</code></strong>：全连接层，输入 <code>in_features</code> 个特征，输出 <code>out_features</code> 个特征</li>
<li><strong><code>nn.Conv2d(in_channels, out_channels, kernel_size)</code></strong>：2D 卷积层，用于图像处理</li>
<li><strong><code>nn.MaxPool2d(kernel_size)</code></strong>：2D 最大池化层，用于降维</li>
<li><strong><code>nn.ReLU()</code></strong>：ReLU 激活函数，常用于隐藏层</li>
<li><strong><code>nn.Softmax(dim)</code></strong>：Softmax 激活函数，通常用于输出层，适用于多类分类问</li>
</ul>
<h3 id="激活函数（Activation-Function）"><a href="#激活函数（Activation-Function）" class="headerlink" title="激活函数（Activation Function）"></a>激活函数（Activation Function）</h3><p>激活函数决定了神经元是否应该被激活。它们是非线性函数，使得神经网络能够学习和执行更复杂的任务。常见的激活函数包括：</p>
<p><strong>Sigmoid</strong>： 用于二分类问题，输出值在 0 和 1 之间</p>
<script type="math/tex; mode=display">
\sigma(x) = \frac{1}{1 + e^{-x}}</script><p><strong>Tanh：</strong> 输出值在 -1 和 1 之间，常用于输出层之前</p>
<script type="math/tex; mode=display">
\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}</script><p><strong>ReLU（Rectified Linear Unit）：</strong>目前最流行的激活函数之一，定义为 <code>f(x) = max(0, x)</code>，有助于解决梯度消失问题</p>
<p><strong>Softmax： </strong>常用于多分类问题的输出层，将输出转换为概率分布</p>
<p>各激活方式如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn.fuctional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">output1 = F.relu(<span class="built_in">input</span>)</span><br><span class="line"></span><br><span class="line">output2 = torch.sigmoid(<span class="built_in">input</span>)</span><br><span class="line"></span><br><span class="line">output3 = torch.tanh(<span class="built_in">input</span>)</span><br><span class="line"></span><br><span class="line">output4 = F.softmax(<span class="built_in">input</span>, dim=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<h3 id="损失函数（Loss-Function）"><a href="#损失函数（Loss-Function）" class="headerlink" title="损失函数（Loss Function）"></a>损失函数（Loss Function）</h3><p>logit（对数几率）是<strong>未经过归一化或激活函数</strong>（如 Softmax、Sigmoid）处理的线性输出，其表达式如下：</p>
<script type="math/tex; mode=display">
logit(p) = log(\frac{p}{1-p})</script><p>损失函数用于衡量模型的预测值与真实值之间的差异</p>
<p>常见的损失函数包括：</p>
<p><strong>均方误差（MSELoss）</strong>：回归问题常用，计算输出与目标值的平方差</p>
<script type="math/tex; mode=display">
\mathcal{L}_{\text{MSE}} = \frac{1}{N} \sum_{i=1}^{N} (y_i - \hat{y}_i)^2</script><p><strong>交叉熵损失（CrossEntropyLoss）</strong>：分类问题常用，计算输出和真实标签之间的交叉熵</p>
<script type="math/tex; mode=display">
\mathcal{L}_{\text{CE}} = \frac{1}{N} \sum_{i=1}^{N} \left( -z_{i, y_i} + \log \left( \sum_{j=1}^{C} e^{z_{i,j}} \right) \right)</script><script type="math/tex; mode=display">
C:类别总数</script><script type="math/tex; mode=display">
z_i,_y:第i个样本在正式类别y_i上的logit值</script><p><strong>BCEWithLogitsLoss</strong>：二分类问题，结合了 Sigmoid 激活和二元交叉熵损失</p>
<script type="math/tex; mode=display">
\mathcal{L} = \frac{1}{N \cdot H} \sum_{i=1}^{N} \sum_{k=1}^{H} \left[ -y_{i,k} \cdot z_{i,k} + \log(1 + e^{z_{i,k}}) \right]</script><script type="math/tex; mode=display">
H : 输出维度（二分类时 H=1 ，多标签时H>1 ）</script><script type="math/tex; mode=display">
y_i,_k∈[0,1] 真实标签（可为软标签）</script><script type="math/tex; mode=display">
z_i,_k : logits</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 均方误差损失</span></span><br><span class="line">criterion = nn.MSELoss()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 交叉熵损失</span></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 二分类交叉熵损失</span></span><br><span class="line">criterion = nn.BCEWithLogitsLoss()</span><br></pre></td></tr></table></figure>
<h3 id="优化器（Optimizer）"><a href="#优化器（Optimizer）" class="headerlink" title="优化器（Optimizer）"></a>优化器（Optimizer）</h3><p>优化器负责在训练过程中更新网络的权重和偏置</p>
<p>常见的优化器包括：</p>
<p><strong>SGD（随机梯度下降）</strong></p>
<script type="math/tex; mode=display">
\theta_{t+1} = \theta_t - \eta \nabla_\theta \mathcal{L}(\theta_t)</script><script type="math/tex; mode=display">
θ 
t
​
  ：第 t 步的模型参数</script><script type="math/tex; mode=display">
η ：学习率（learning rate）</script><script type="math/tex; mode=display">
∇_θL(θ_t) ：损失函数 L 对 θ 的梯度（基于当前 batch）</script><p><strong>Adam（自适应矩估计）</strong></p>
<script type="math/tex; mode=display">
\begin{aligned}
s_{t+1} &= \beta s_t + (1 - \beta) \left( \nabla_\theta \mathcal{L}(\theta_t) \right)^2 \\
\theta_{t+1} &= \theta_t - \frac{\eta}{\sqrt{s_{t+1} + \epsilon}} \nabla_\theta \mathcal{L}(\theta_t)
\end{aligned}</script><script type="math/tex; mode=display">
s_t
  ：梯度平方的指数移动平均（二阶矩估计）</script><script type="math/tex; mode=display">
β ：衰减率（通常取 0.9）</script><script type="math/tex; mode=display">
ϵ ：小常数（如 10^{−8}），防止除零（参数为eps=1e-9）</script><p><strong>RMSprop（均方根传播）</strong></p>
<script type="math/tex; mode=display">
\begin{aligned}
g_t &= \nabla_\theta \mathcal{L}(\theta_t) \\
m_t &= \beta_1 m_{t-1} + (1 - \beta_1) g_t \\
s_t &= \beta_2 s_{t-1} + (1 - \beta_2) g_t^2 \\
\hat{m}_t &= \frac{m_t}{1 - \beta_1^t} \\
\hat{s}_t &= \frac{s_t}{1 - \beta_2^t} \\
\theta_{t+1} &= \theta_t - \frac{\eta}{\sqrt{\hat{s}_t} + \epsilon} \hat{m}_t
\end{aligned}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用 SGD 优化器</span></span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用 Adam 优化器</span></span><br><span class="line">optimizer = optim.Adam(model.parameters(), lr=<span class="number">0.001</span>)</span><br></pre></td></tr></table></figure>
<h3 id="训练过程（Training-Process）"><a href="#训练过程（Training-Process）" class="headerlink" title="训练过程（Training Process）"></a>训练过程（Training Process）</h3><p>训练神经网络涉及以下步骤：</p>
<ol>
<li><strong>准备数据</strong>：通过 <code>DataLoader</code> 加载数据</li>
<li><strong>定义损失函数和优化器</strong></li>
<li><strong>前向传播</strong>：计算模型的输出</li>
<li><strong>计算损失</strong>：与目标进行比较，得到损失值</li>
<li><strong>反向传播</strong>：通过 <code>loss.backward()</code> 计算梯度</li>
<li><strong>更新参数</strong>：通过 <code>optimizer.step()</code> 更新模型的参数</li>
<li><strong>重复上述步骤</strong>，直到达到预定的训练轮数</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 假设已经定义好了模型、损失函数和优化器</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练数据示例</span></span><br><span class="line">X = torch.randn(<span class="number">10</span>, <span class="number">2</span>) <span class="comment"># 10 个样本，每个样本有 2 个特征</span></span><br><span class="line">Y = torch.randn(<span class="number">10</span>, <span class="number">1</span>) <span class="comment"># 10 个目标标签</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练过程</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):  <span class="comment"># 训练 100 轮</span></span><br><span class="line">  model.train() <span class="comment"># 设置模型为训练模式</span></span><br><span class="line">  optimizer.zero_grad() <span class="comment"># 清除梯度</span></span><br><span class="line">  output = model(X) <span class="comment"># 前向传播</span></span><br><span class="line">  loss = criterion(output, Y) <span class="comment"># 计算损失</span></span><br><span class="line">  loss.backward() <span class="comment"># 反向传播</span></span><br><span class="line">  optimizer.step() <span class="comment"># 更新权重</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (epoch + <span class="number">1</span>) % <span class="number">10</span> == <span class="number">0</span>:  <span class="comment"># 每 10 轮输出一次损失</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;Epoch [<span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>/100], Loss: <span class="subst">&#123;loss.item():<span class="number">.4</span>f&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h2 id="Pytorch-torch-optim"><a href="#Pytorch-torch-optim" class="headerlink" title="Pytorch torch.optim"></a>Pytorch torch.optim</h2><p><img src="/img/lazy.gif" data-original="18.png" alt=""></p>
<p>优化器是深度学习中的核心组件，负责根据损失函数的梯度调整模型参数，使模型能够逐步逼近最优解</p>
<p>在PyTorch中，<code>torch.optim</code> 模块提供了多种优化算法的实现</p>
<h3 id="为什么需要优化器"><a href="#为什么需要优化器" class="headerlink" title="为什么需要优化器"></a>为什么需要优化器</h3><ul>
<li><strong>自动化参数更新</strong>：手动计算和更新每个参数非常繁琐</li>
<li><strong>加速收敛</strong>：使用优化算法比普通梯度下降更快找到最优解</li>
<li><strong>避免局部最优</strong>：某些优化器具有跳出局部最优的能力</li>
</ul>
<h3 id="常见优化器类型"><a href="#常见优化器类型" class="headerlink" title="常见优化器类型"></a>常见优化器类型</h3><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">优化器名称</th>
<th style="text-align:left">主要特点</th>
<th style="text-align:left">适用场景</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">SGD</td>
<td style="text-align:left">简单基础</td>
<td style="text-align:left">基础教学、简单模型</td>
</tr>
<tr>
<td style="text-align:left">Adam</td>
<td style="text-align:left">自适应学习率</td>
<td style="text-align:left">大多数深度学习任务</td>
</tr>
<tr>
<td style="text-align:left">RMSprop</td>
<td style="text-align:left">适应学习率</td>
<td style="text-align:left">RNN网络</td>
</tr>
<tr>
<td style="text-align:left">Adagrad</td>
<td style="text-align:left">参数独立学习率</td>
<td style="text-align:left">稀疏数据</td>
</tr>
</tbody>
</table>
</div>
<h3 id="常用优化器"><a href="#常用优化器" class="headerlink" title="常用优化器"></a>常用优化器</h3><h4 id="SGD-随机梯度下降"><a href="#SGD-随机梯度下降" class="headerlink" title="SGD (随机梯度下降)"></a>SGD (随机梯度下降)</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line">optimizer = optim.SGD(params, lr=<span class="number">0.01</span>, momentum=<span class="number">0</span>, dampening=<span class="number">0</span>, weight_decay=<span class="number">0</span>, nesterov=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<p><strong>核心参数</strong>：</p>
<ul>
<li><code>lr</code> (float)：学习率（默认0.01）</li>
<li><code>momentum</code> (float)：动量因子（默认0）</li>
<li><code>weight_decay</code> (float)：L2正则化系数（默认0）</li>
</ul>
<p><strong>特点</strong>：</p>
<ul>
<li>最简单的优化算法</li>
<li>可以添加动量项加速收敛</li>
<li>适合作为基准比较</li>
</ul>
<h4 id="Adam-自适应矩估计"><a href="#Adam-自适应矩估计" class="headerlink" title="Adam (自适应矩估计)"></a>Adam (自适应矩估计)</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line">optimizer = optim.Adam(params, lr=<span class="number">0.001</span>, betas=(<span class="number">0.9</span>, <span class="number">0.999</span>), eps=<span class="number">1e-08</span>, weight_decay=<span class="number">0</span>, amsgrad=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<p><strong>核心参数</strong>：</p>
<ul>
<li><code>betas</code> (Tuple[float, float])：用于计算梯度和梯度平方的移动平均系数</li>
<li><code>eps</code> (float)：数值稳定项（默认1e-8）</li>
<li><code>amsgrad</code> (bool)：是否使用AMSGrad变体（默认False）</li>
</ul>
<p><strong>特点</strong>：</p>
<ul>
<li>自适应学习率</li>
<li>结合了动量概念</li>
<li>大多数情况下的默认选择</li>
</ul>
<h3 id="高级技巧"><a href="#高级技巧" class="headerlink" title="高级技巧"></a>高级技巧</h3><h4 id="学习率调度"><a href="#学习率调度" class="headerlink" title="学习率调度"></a>学习率调度</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.optim.lr_scheduler <span class="keyword">as</span> StepLR</span><br><span class="line"></span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">0.1</span>)</span><br><span class="line">scheduler = StepLR(optimizer, step_size=<span class="number">30</span>, gamma=<span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">	train(..)</span><br><span class="line">	validate(..)</span><br><span class="line">	scheduler.step()   <span class="comment"># 更新学习率</span></span><br></pre></td></tr></table></figure>
<p><strong>学习率调度</strong>就是：</p>
<blockquote>
<p>按照一定规则，随着训练的进行，<strong>自动改变 lr</strong>，而不是一直用同一个值</p>
</blockquote>
<p><code>StepLR(optimizer, step_size=30, gamma=0.1)</code> 表示优化器每经过30轮，就将当前的学习率乘以gamma</p>
<p>总结下来就是：</p>
<ul>
<li><strong>前 30 轮</strong>：lr = 0.1<ul>
<li>让模型快速从一堆随机参数中粗略找到一个“还行”的区域</li>
</ul>
</li>
<li><strong>后 70 轮</strong>：逐渐把 lr 调小到 0.01 → 0.001 → 0.0001<ul>
<li>让模型在已经不错的区域里，<strong>慢慢微调</strong>、稳定收敛，避免 loss 抖动太厉害</li>
</ul>
</li>
</ul>
<h4 id="参数分组优化"><a href="#参数分组优化" class="headerlink" title="参数分组优化"></a>参数分组优化</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">optim.SGD([</span><br><span class="line">    &#123;<span class="string">&#x27;params&#x27;</span>: model.base.parameters()&#125;,  <span class="comment"># 基础层</span></span><br><span class="line">    &#123;<span class="string">&#x27;params&#x27;</span>: model.classifier.parameters(), <span class="string">&#x27;lr&#x27;</span>: <span class="number">1e-3</span>&#125;  <span class="comment"># 分类层</span></span><br><span class="line">], lr=<span class="number">1e-2</span>)</span><br></pre></td></tr></table></figure>
<p>整个模型分成两大块：</p>
<ul>
<li><code>model.base</code>：基础特征提取部分（比如 ResNet 的 backbone）</li>
<li><code>model.classifier</code>：最后的分类头</li>
</ul>
<p>很明显的是，基础层使用外面的学习率（lr=1e-2），然后分类头单独使用1e-3的学习率</p>
<h4 id="梯度裁剪"><a href="#梯度裁剪" class="headerlink" title="梯度裁剪"></a>梯度裁剪</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=<span class="number">1.0</span>)</span><br></pre></td></tr></table></figure>
<p>先把 <code>model.parameters()</code> 里所有参数的梯度拿出来</p>
<p>算一个“总的梯度范数”（就可以理解成“所有梯度的大小合在一起”的长度）</p>
<p>如果这个总大小 <code>&lt;= 1.0</code>：不动</p>
<p>如果 <code>&gt; 1.0</code>：<strong>统一把所有梯度等比例缩小</strong>，缩小到“总大小 = 1.0”为止</p>
<p>所以可以知道，并不是我们理解上的裁剪，而是等比例缩放</p>
<p>假设某个模型只有两个参数，它们的梯度拼成一个向量可以理解为：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grad = [3, 4]</span><br></pre></td></tr></table></figure>
<ul>
<li>它的 L2 范数是：<br> <code>total_norm = sqrt(3^2 + 4^2) = 5</code></li>
<li>我们的 <code>max_norm = 1.0</code></li>
</ul>
<p>那缩放系数就是：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">clip_coef = max_norm / total_norm = 1.0 / 5 = 0.2</span><br></pre></td></tr></table></figure>
<p>于是裁剪后：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">原梯度: [3, 4]</span><br><span class="line">裁剪后: [3 * 0.2, 4 * 0.2] = [0.6, 0.8]</span><br></pre></td></tr></table></figure>
<p><strong>运用示例：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">optimizer.zero_grad()</span><br><span class="line">loss.backward()  <span class="comment"># 先算出梯度</span></span><br><span class="line"></span><br><span class="line">torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=<span class="number">1.0</span>)  <span class="comment"># 再裁剪梯度</span></span><br><span class="line"></span><br><span class="line">optimizer.step()  <span class="comment"># 最后用裁剪后的梯度更新参数</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<hr>
<h2 id="第一个神经网络"><a href="#第一个神经网络" class="headerlink" title="第一个神经网络"></a>第一个神经网络</h2><h3 id="实例1"><a href="#实例1" class="headerlink" title="实例1"></a>实例1</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义输入层大小、隐藏层大小、输出层大小和批量大小</span></span><br><span class="line">inp, hid, outp, size = <span class="number">10</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">10</span></span><br><span class="line"></span><br><span class="line">x = torch.randn(size, inp)</span><br><span class="line">y = torch.tensor([[<span class="number">1.0</span>], [<span class="number">0.0</span>], [<span class="number">0.0</span>],</span><br><span class="line">                 [<span class="number">1.0</span>], [<span class="number">1.0</span>], [<span class="number">1.0</span>], [<span class="number">0.0</span>], [<span class="number">0.0</span>], [<span class="number">1.0</span>], [<span class="number">1.0</span>]])  <span class="comment"># 目标数据</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建顺序模型，包含线性层、ReLU激活函数和Sigmoid激活函数</span></span><br><span class="line">model = nn.Sequential(</span><br><span class="line">    nn.Linear(inp, hid),        <span class="comment"># 输入层 -&gt; 隐藏层的线性变化</span></span><br><span class="line">    nn.ReLU(),                  <span class="comment"># 隐藏层的激活函数</span></span><br><span class="line">    nn.Linear(hid, outp),       <span class="comment"># 隐藏层 -&gt; 输出层的线性变化</span></span><br><span class="line">    nn.Sigmoid()                <span class="comment"># 输出层的激活函数</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义均方误差损失函数和随机梯度下降优化器</span></span><br><span class="line">criterion = nn.MSELoss()</span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">0.01</span>) <span class="comment">#学习率为0.01</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 执行梯度下降算法进行模型训练</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">50</span>):</span><br><span class="line">    y_pred = model(x)</span><br><span class="line">    loss = criterion(y_pred, y)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;epoch: &quot;</span>, epoch, <span class="string">&quot; loss: &quot;</span>, loss)</span><br><span class="line"></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br></pre></td></tr></table></figure>
<ul>
<li><code>inp</code>：输入层大小为 10，即每个数据点有 10 个特征</li>
<li><code>hid</code>：隐藏层大小为 5，即隐藏层包含 5 个神经元</li>
<li><code>outp</code>：输出层大小为 1，即输出一个标量，表示二分类结果（0 或 1）</li>
<li><code>size</code>：每个批次包含 10 个样本</li>
</ul>
<p><code>nn.Sequential</code> 用于按顺序定义网络层</p>
<ul>
<li><code>nn.Linear(inp, hid)</code>：定义输入层到隐藏层的线性变换，输入特征是 10 个，隐藏层有 5 个神经元</li>
<li><code>nn.ReLU()</code>：在隐藏层后添加 ReLU 激活函数，增加非线性</li>
<li><code>nn.Linear(hid, outp)</code>：定义隐藏层到输出层的线性变换，输出为 1 个神经元</li>
<li><code>nn.Sigmoid()</code>：输出层使用 Sigmoid 激活函数，将结果映射到 0 到 1 之间，用于二分类任务</li>
</ul>
<h3 id="梯度下降算法"><a href="#梯度下降算法" class="headerlink" title="梯度下降算法"></a><strong>梯度下降算法</strong></h3><p>假设我们有一个可微的损失函数 <script type="math/tex">L(θ)</script> ，其中 <em>θ</em> 是模型的参数（例如神经网络的权重），我们希望找到使 <script type="math/tex">L(θ)</script> 最小的参数值 <em>θ</em></p>
<p>梯度下降利用这样一个事实：<strong>函数在某点处的梯度方向是函数值增长最快的方向</strong>。因此，<strong>负梯度方向就是函数值下降最快的方向</strong></p>
<p>于是，我们迭代地更新参数：</p>
<script type="math/tex; mode=display">
θ ← θ - η∇_θL(θ)</script><p>其中：</p>
<script type="math/tex; mode=display">
∇_θL(θ) 是损失函数对参数的梯度（偏导数组成的向量）</script><script type="math/tex; mode=display">
η>0 是学习率，控制每次更新的步长</script><p>那么问题来了，现在我们找到了下降最快的方向时候，需要做什么？</p>
<p>答案就是，往下降最快的地方迈一步，这样的话迈完之后可以继续找下一个最小的地方，其中，迈多少就是步长，也就是学习率(lr)，如果步子太大（学习率过高）：可能一步跨过最低点，甚至越走越高（发散），如果步子太小（学习率过低）：收敛太慢，训练效率低，所以，我们更新公式如下：</p>
<script type="math/tex; mode=display">
θ_{new} = θ_{old} - η∇_θL(θ_{old})</script><p>比如上面代码所写的那样：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">optimizer.zero_grad()</span><br><span class="line">loss.backward()</span><br><span class="line">optimizer.step()</span><br></pre></td></tr></table></figure>
<p>先清空旧梯度，然后计算当前梯度（下降最快的方向），然后沿着负梯度方向走一步，那会不会一直走下去呢？答案是不会，当下面几种情况下停止：</p>
<ul>
<li>损失函数变化非常小（收敛）</li>
<li>达到最大训练轮数（epochs）</li>
<li>验证集性能不再提升（防止过拟合）</li>
</ul>
<p>下面举个例子：</p>
<script type="math/tex; mode=display">
假设损失函数是 \\L(w)=w^2 ，当前w=3</script><script type="math/tex; mode=display">
计算梯度：\frac{dw}{dL}=2w=6 → 上升最快方向是 +6</script><script type="math/tex; mode=display">
所以下降最快方向是 -6</script><script type="math/tex; mode=display">
设学习率 η=0.1 ，则更新：

w_{new}=3−0.1×6=2.4</script><script type="math/tex; mode=display">
下一轮再算梯度：2×2.4=4.8 ，继续更新……</script><script type="math/tex; mode=display">
最终 w→0 ，损失 →0 ，达到最小值</script><p>Q1：为什么 “最快下降方向” ≠ “直达最低点”</p>
<p>A1：因为 <strong>梯度只反映局部信息</strong>（一阶导数），它假设函数在附近是线性的。但实际损失函数可能是弯曲的（非线性）。所以你只能“走一小步”，然后重新评估方向——这正是迭代优化的本质</p>
<h3 id="实例2"><a href="#实例2" class="headerlink" title="实例2"></a>实例2</h3><p>你也可以自定义一个神经网络：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line">n_samples = <span class="number">100</span></span><br><span class="line">data = torch.randn(n_samples, <span class="number">2</span>)</span><br><span class="line">labels = (data[:, <span class="number">0</span>]**<span class="number">2</span> + data[:, <span class="number">1</span>]**<span class="number">2</span> &lt; <span class="number">1</span>).<span class="built_in">float</span>().unsqueeze(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#定义神经网络</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SimpleNN</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(SimpleNN, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.fc1 = nn.Linear(<span class="number">2</span>, <span class="number">4</span>)</span><br><span class="line">        <span class="variable language_">self</span>.fc2 = nn.Linear(<span class="number">4</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="variable language_">self</span>.sigmoid = nn.Sigmoid()  <span class="comment"># 二分类激活函数</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = torch.relu(<span class="variable language_">self</span>.fc1(x))    <span class="comment"># 输入层用relu激活函数</span></span><br><span class="line">        x = torch.sigmoid(<span class="variable language_">self</span>.fc2(x)) <span class="comment"># 输出层用sigmoid激活函数</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    </span><br><span class="line">model = SimpleNN()</span><br><span class="line"></span><br><span class="line"><span class="comment">#定义损失函数和优化器</span></span><br><span class="line">criterion = nn.BCELoss()  <span class="comment"># 二元交叉熵损失</span></span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">0.1</span>) <span class="comment"># 随机梯度下降熵优化器</span></span><br><span class="line"></span><br><span class="line">epochs = <span class="number">100</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    outputs = model(data)</span><br><span class="line">    loss = criterion(outputs, labels)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#反向传播</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (epoch + <span class="number">1</span>) % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;Epoch [<span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>/<span class="subst">&#123;epochs&#125;</span>], Loss: <span class="subst">&#123;loss.item():<span class="number">.4</span>f&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h2 id="数据处理与加载"><a href="#数据处理与加载" class="headerlink" title="数据处理与加载"></a>数据处理与加载</h2><p>PyTorch 数据处理与加载的介绍：</p>
<ul>
<li><strong>自定义 Dataset</strong>：通过继承 <code>torch.utils.data.Dataset</code> 来加载自己的数据集</li>
<li><strong>DataLoader</strong>：<code>DataLoader</code> 按批次加载数据，支持多线程加载并进行数据打乱</li>
<li><strong>数据预处理与增强</strong>：使用 <code>torchvision.transforms</code> 进行常见的图像预处理和增强操作，提高模型的泛化能力</li>
<li><strong>加载标准数据集</strong>：<code>torchvision.datasets</code> 提供了许多常见的数据集，简化了数据加载过程</li>
<li><strong>多个数据源</strong>：通过组合多个 <code>Dataset</code> 实例来处理来自不同来源的数据</li>
</ul>
<h3 id="自定义Dataset"><a href="#自定义Dataset" class="headerlink" title="自定义Dataset"></a>自定义Dataset</h3><p><strong>torch.utils.data.Dataset</strong> 是一个抽象类，允许你从自己的数据源中创建数据集。</p>
<p>我们需要继承该类并实现以下两个方法：</p>
<ul>
<li><code>__len__(self)</code>：返回数据集中的样本数量</li>
<li><code>__getitem__(self, idx)</code>：通过索引返回一个样本</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, X_Data, Y_Data</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        初始化数据 X_data 和 Y_data 是两个列表或数组</span></span><br><span class="line"><span class="string">        X_data: 输入特征</span></span><br><span class="line"><span class="string">        Y_data: 目标标签</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="variable language_">self</span>.X_Data = X_Data</span><br><span class="line">        <span class="variable language_">self</span>.Y_Data = Y_Data</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(<span class="variable language_">self</span>.X_Data)   <span class="comment"># 返回数据集大小</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, index</span>):</span><br><span class="line">        <span class="comment">#返回指定的索引数据</span></span><br><span class="line">        x = torch.tensor(<span class="variable language_">self</span>.X_Data[index], dtype=torch.float32)</span><br><span class="line">        y = torch.tensor(<span class="variable language_">self</span>.Y_Data[index], dtype=torch.float32)</span><br><span class="line">        <span class="keyword">return</span> x, y</span><br><span class="line"></span><br><span class="line">X_Data = [[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>], [<span class="number">5</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>]] <span class="comment"># 输入特征</span></span><br><span class="line">Y_Data = [<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]  <span class="comment"># 输出特征</span></span><br><span class="line"></span><br><span class="line">dataset = MyDataset(X_Data, Y_Data)</span><br><span class="line"></span><br><span class="line"><span class="comment">#打印数据</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(dataset)):</span><br><span class="line">    <span class="built_in">print</span>(dataset[i])</span><br><span class="line">    </span><br><span class="line"><span class="comment">#(tensor([1., 2.]), tensor(1.))</span></span><br><span class="line"><span class="comment">#(tensor([3., 4.]), tensor(0.))</span></span><br><span class="line"><span class="comment">#(tensor([5., 6.]), tensor(1.))</span></span><br><span class="line"><span class="comment">#(tensor([7., 8.]), tensor(0.))</span></span><br></pre></td></tr></table></figure>
<h3 id="DataLoader加载数据"><a href="#DataLoader加载数据" class="headerlink" title="DataLoader加载数据"></a>DataLoader加载数据</h3><p><code>DataLoader</code> 用于在pytorch中从 Dataset 中按批次（batch）加载数据</p>
<p>以上面的Dataset为例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset, DataLoader</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, X_Data, Y_Data</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        初始化数据 X_data 和 Y_data 是两个列表或数组</span></span><br><span class="line"><span class="string">        X_data: 输入特征</span></span><br><span class="line"><span class="string">        Y_data: 目标标签</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="variable language_">self</span>.X_Data = X_Data</span><br><span class="line">        <span class="variable language_">self</span>.Y_Data = Y_Data</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(<span class="variable language_">self</span>.X_Data)   <span class="comment"># 返回数据集大小</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, index</span>):</span><br><span class="line">        <span class="comment">#返回指定的索引数据</span></span><br><span class="line">        x = torch.tensor(<span class="variable language_">self</span>.X_Data[index], dtype=torch.float32)</span><br><span class="line">        y = torch.tensor(<span class="variable language_">self</span>.Y_Data[index], dtype=torch.float32)</span><br><span class="line">        <span class="keyword">return</span> x, y</span><br><span class="line"></span><br><span class="line">X_Data = [[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>], [<span class="number">5</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>]] <span class="comment"># 输入特征</span></span><br><span class="line">Y_Data = [<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]  <span class="comment"># 输出特征</span></span><br><span class="line"></span><br><span class="line">dataset = MyDataset(X_Data, Y_Data)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(dataset)):</span><br><span class="line">    <span class="built_in">print</span>(dataset[i])</span><br><span class="line"></span><br><span class="line">dataLoader = DataLoader(dataset, batch_size=<span class="number">2</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>):</span><br><span class="line">    <span class="keyword">for</span> index, (inputs, labels) <span class="keyword">in</span> <span class="built_in">enumerate</span>(dataLoader):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;Batch <span class="subst">&#123;index + <span class="number">1</span>&#125;</span>:&#x27;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;Inputs: <span class="subst">&#123;inputs&#125;</span>&#x27;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;Labels: <span class="subst">&#123;labels&#125;</span>&#x27;</span>)</span><br><span class="line">        </span><br><span class="line"><span class="comment">#(tensor([1., 2.]), tensor(1.))</span></span><br><span class="line"><span class="comment">#(tensor([3., 4.]), tensor(0.))</span></span><br><span class="line"><span class="comment">#(tensor([5., 6.]), tensor(1.))</span></span><br><span class="line"><span class="comment">#(tensor([7., 8.]), tensor(0.))</span></span><br><span class="line"><span class="comment">#Batch 1:</span></span><br><span class="line"><span class="comment">#Inputs: tensor([[1., 2.],</span></span><br><span class="line"><span class="comment">#        [5., 6.]])</span></span><br><span class="line"><span class="comment">#Labels: tensor([1., 1.])</span></span><br><span class="line"><span class="comment">#Batch 2:</span></span><br><span class="line"><span class="comment">#Inputs: tensor([[7., 8.],</span></span><br><span class="line"><span class="comment">#        [3., 4.]])</span></span><br><span class="line"><span class="comment">#Labels: tensor([0., 0.])</span></span><br></pre></td></tr></table></figure>
<ul>
<li><strong>batch_size</strong> 为每次加载的样本量</li>
<li><strong>shuffle</strong> 意为洗牌，也就是将数据打乱（通常训练时都需要打乱）</li>
<li><strong>drop_last</strong> 如果数据集中的样本数不能被 <code>batch_size</code> 整除，设置为 <code>True</code> 时，丢弃最后一个不完整的 batch </li>
</ul>
<p>每次循环都会返回每个批次与每个批次所包含的输入特征和目标标签</p>
<h3 id="预处理和数据增强"><a href="#预处理和数据增强" class="headerlink" title="预处理和数据增强"></a>预处理和数据增强</h3><p><code>torchvision.transforms</code> 模块来进行常见的图像预处理和增强操作，如旋转、裁剪、归一化等</p>
<p>常见预处理如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"></span><br><span class="line"><span class="comment">#定义数据预处理的流水线</span></span><br><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.Resize((<span class="number">128</span>, <span class="number">128</span>)),  <span class="comment"># 将图像调整为 128x128</span></span><br><span class="line">    transforms.ToTensor(),  <span class="comment"># 将图像转换为张量</span></span><br><span class="line">    transforms.Normalize(mean=[<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], std=[<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])  <span class="comment"># 标准化</span></span><br><span class="line">])</span><br><span class="line">image = Image.<span class="built_in">open</span>(<span class="string">&#x27;image.jpg&#x27;</span>)</span><br><span class="line"></span><br><span class="line">image_tensor = transform(image)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(image_tensor.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment">#torch.Size([3, 128, 128])</span></span><br></pre></td></tr></table></figure>
<ul>
<li><strong>transforms.Compose()</strong>：将多个变换操作组合在一起</li>
<li><strong>transforms.Resize()</strong>：调整图像大小</li>
<li><strong>transforms.ToTensor()</strong>：将图像转换为 PyTorch 张量，值会被归一化到 <code>[0, 1]</code> 范围</li>
<li><strong>transforms.Normalize()</strong>：标准化图像数据，通常使用预训练模型时需要进行标准化处理</li>
</ul>
<p>另外，transforms也提供了图片增强的功能</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.RandomHorizontalFlip(),  <span class="comment"># 随机水平翻转</span></span><br><span class="line">    transforms.RandomRotation(<span class="number">30</span>),  <span class="comment"># 随机旋转 30 度</span></span><br><span class="line">    transforms.RandomResizedCrop(<span class="number">128</span>),  <span class="comment"># 随机裁剪并调整为 128x128</span></span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize(mean=[<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], std=[<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])</span><br><span class="line">])</span><br></pre></td></tr></table></figure>
<p>其中这里的mean和std分别为RGB 三个通道的<strong>经验均值（mean）和标准差（std）</strong>，在Normalize()中分别在R，G，B三种通道中做以下操作：</p>
<script type="math/tex; mode=display">
output = \frac{input - mean}{std}</script><ul>
<li><code>mean = [0.485, 0.456, 0.406]</code> 对应 <strong>R、G、B 通道的平均像素值</strong>（归一化到 [0,1] 后的均值）</li>
<li><code>std = [0.229, 0.224, 0.225]</code> 对应 <strong>R、G、B 通道的标准差</strong></li>
</ul>
<h3 id="加载图像数据集"><a href="#加载图像数据集" class="headerlink" title="加载图像数据集"></a>加载图像数据集</h3><p>对于图像数据集，torchvision.datasets 提供了许多常见数据集（如 CIFAR-10、ImageNet、MNIST 等）以及用于加载图像数据的工具</p>
<p>加载 MNIST 数据集:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision.datasets <span class="keyword">as</span> datasets</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义预处理操作</span></span><br><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize((<span class="number">0.5</span>,), (<span class="number">0.5</span>,))  <span class="comment"># 对灰度图像进行标准化</span></span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 下载并加载 MNIST 数据集</span></span><br><span class="line">train_dataset = datasets.MNIST(root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>, transform=transform)</span><br><span class="line">test_dataset = datasets.MNIST(root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">False</span>, download=<span class="literal">True</span>, transform=transform)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建 DataLoader</span></span><br><span class="line">train_loader = DataLoader(train_dataset, batch_size=<span class="number">64</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line">test_loader = DataLoader(test_dataset, batch_size=<span class="number">64</span>, shuffle=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 迭代训练数据</span></span><br><span class="line"><span class="keyword">for</span> inputs, labels <span class="keyword">in</span> train_loader:</span><br><span class="line">    <span class="built_in">print</span>(inputs.shape)  <span class="comment"># 每个批次的输入数据形状</span></span><br><span class="line">    <span class="built_in">print</span>(labels.shape)  <span class="comment"># 每个批次的标签形状</span></span><br></pre></td></tr></table></figure>
<ul>
<li><code>datasets.MNIST()</code> 会自动下载 MNIST 数据集并加载</li>
<li><code>transform</code> 参数允许我们对数据进行预处理</li>
<li><code>train=True</code> 和 <code>train=False</code> 分别表示训练集和测试集</li>
</ul>
<h3 id="用多个数据源（Multi-source-Dataset）"><a href="#用多个数据源（Multi-source-Dataset）" class="headerlink" title="用多个数据源（Multi-source Dataset）"></a>用多个数据源（Multi-source Dataset）</h3><p>如果你的数据集由多个文件、多个来源（例如多个图像文件夹）组成，可以通过继承 Dataset 类自定义加载多个数据源</p>
<p>PyTorch 提供了 ConcatDataset 和 ChainDataset 等类来连接多个数据集</p>
<p>例如，假设我们有多个图像文件夹的数据，可以将它们合并为一个数据集</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> ConcatDataset</span><br><span class="line"></span><br><span class="line"><span class="comment"># 假设 dataset1 和 dataset2 是两个 Dataset 对象</span></span><br><span class="line">combined_dataset = ConcatDataset([dataset1, dataset2])</span><br><span class="line">combined_loader = DataLoader(combined_dataset, batch_size=<span class="number">64</span>, shuffle=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<h2 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h2><p>线性回归是最基本的机器学习算法之一，用于预测一个连续值。</p>
<p>线性回归是一种简单且常见的回归分析方法，目的是通过拟合一个线性函数来预测输出。</p>
<p>对于一个简单的线性回归问题，模型可以表示为：</p>
<script type="math/tex; mode=display">
y = w_1x_1+w_2x_2+...+w_nx_n+b</script><p>其中：</p>
<ul>
<li><script type="math/tex">y</script> 是预测值（目标值）</li>
<li><script type="math/tex">x_1,x_2..x_n</script> 是输入特征</li>
<li><script type="math/tex">w_1,w_2...w_n</script> 是每个输入特征所对应的权重（待学习的权重）</li>
<li><script type="math/tex">b</script> 是偏置</li>
</ul>
<p>现使用Pytorch来实现一个简单的线性回归</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机种子，确保每次结果一致</span></span><br><span class="line">torch.manual_seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line">X = torch.randn(<span class="number">100</span> ,<span class="number">2</span>)</span><br><span class="line">true_w = torch.tensor([<span class="number">2.0</span>, <span class="number">3.0</span>])</span><br><span class="line">true_b = <span class="number">4.0</span>     <span class="comment"># 偏置</span></span><br><span class="line">Y = X @ true_w + true_b + torch.randn(<span class="number">100</span>) * <span class="number">0.1</span> <span class="comment"># 加入一点噪声</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(X[:<span class="number">5</span>])</span><br><span class="line"><span class="built_in">print</span>(Y[:<span class="number">5</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment">#tensor([[ 1.9269,  1.4873],</span></span><br><span class="line"><span class="comment">#        [ 0.9007, -2.1055],</span></span><br><span class="line"><span class="comment">#        [ 0.6784, -1.2345],</span></span><br><span class="line"><span class="comment">#        [-0.0431, -1.6047],</span></span><br><span class="line"><span class="comment">#        [-0.7521,  1.6487]])</span></span><br><span class="line"><span class="comment">#tensor([12.4460, -0.4663,  1.7666, -0.9357,  7.4781])</span></span><br></pre></td></tr></table></figure>
<p>其中：</p>
<ul>
<li>@表示的是矩阵乘法运算符，<code>X @ true_w</code>就是对每一行做点积，即<code>X[i, :] @ true_w = X[i, 0] * 2.0 + X[i, 1] * 3.0</code></li>
<li>噪声：<code>torch.randn(100) * 0.1</code> 生成了 100 个服从 <strong>标准正态分布</strong>（均值为 0，标准差为 1）的随机数，再乘以 0.1，变成 <strong>均值为 0、标准差为 0.1 的小扰动</strong></li>
<li>为什么要加入噪声？因为现实中数据存在测量误差、环境干扰等，如果不加入噪声，模型将完全由 <code>X @ true_w + true_b</code> 来决定，会出现百分百拟合，不符合实际</li>
</ul>
<h3 id="定义线性回归模型"><a href="#定义线性回归模型" class="headerlink" title="定义线性回归模型"></a>定义线性回归模型</h3><p>可以通过继承 <code>nn.Module</code> 来定义一个简单的线性回归模型</p>
<p>在 PyTorch 中，线性回归的核心是 <code>nn.Linear()</code> 层，它会自动处理权重和偏置的初始化</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SimpleNN</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(SimpleNN, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.linear = nn.Linear(<span class="number">2</span>, <span class="number">1</span>)  <span class="comment"># 定义线性层，输入两个特征，输出一个特征</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.linear(x)</span><br><span class="line"></span><br><span class="line">model = SimpleNN()</span><br></pre></td></tr></table></figure>
<h3 id="定义损失函数与优化器"><a href="#定义损失函数与优化器" class="headerlink" title="定义损失函数与优化器"></a>定义损失函数与优化器</h3><p>线性回归的常见损失函数是 <strong>均方误差损失（MSELoss）</strong>，用于衡量预测值与真实值之间的差异</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">criterion = nn.MSELoss()</span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">0.01</span>)</span><br></pre></td></tr></table></figure>
<h3 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h3><p>依旧按照前面训练过程，即前向传播 -&gt; 计算损失 -&gt; 反向传播 -&gt; 计算梯度并清空 -&gt; 更新/优化参数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">epochs = <span class="number">1000</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    model.train()</span><br><span class="line">    data = model(X)</span><br><span class="line"></span><br><span class="line">    loss = criterion = (data.squeeze(), Y)  <span class="comment"># 预测值data需要压缩成1维</span></span><br><span class="line"></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (epoch + <span class="number">1</span>) % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;Epoch [<span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>/1000], Loss: <span class="subst">&#123;loss.item():<span class="number">.4</span>f&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>其中：</p>
<ul>
<li>“训练模式”：对于<strong>包含某些特定层</strong>的模型（如 <code>Dropout</code>、<code>BatchNorm</code> 等），PyTorch 需要知道当前是在 <strong>训练</strong> 还是 <strong>推理/评估</strong> 阶段</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th>层类型</th>
<th>训练模式 model.train()</th>
<th>评估模式 model.eval()</th>
</tr>
</thead>
<tbody>
<tr>
<td>Dropout</td>
<td>随机“丢弃”一部分神经元（防止过拟合）</td>
<td><strong>不丢弃</strong>，所有神经元都参与，但权重会缩放</td>
</tr>
<tr>
<td>BathNorm</td>
<td>使用当前 batch 的均值和方差做归一化</td>
<td>使用训练时累计的 <strong>全局均值和方差</strong></td>
</tr>
</tbody>
</table>
</div>
<ul>
<li>不过为什么之前的示例中没用到 <code>train()</code> ？因为对于只包含简单线性层的，其实在 <code>train()</code> 和 <code>eval()</code> 下的工作行为完全一样，所以理论上可以不用写（Dropout 默认处于 <code>eval</code> 模式）</li>
<li>为什么需要压缩成一维？因为目标值Y就是一维</li>
</ul>
<h3 id="评估"><a href="#评估" class="headerlink" title="评估"></a>评估</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Predicted Weight: <span class="subst">&#123;model.linear.weight.data.numpy()&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Predicted Bias: <span class="subst">&#123;model.linear.bias.data.numpy()&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    predictions = model(X)</span><br><span class="line"></span><br><span class="line">plt.scatter(X[:, <span class="number">0</span>], Y, color=<span class="string">&#x27;blue&#x27;</span>, label=<span class="string">&#x27;True values&#x27;</span>)</span><br><span class="line">plt.scatter(X[:, <span class="number">0</span>], predictions, color=<span class="string">&#x27;red&#x27;</span>, label=<span class="string">&#x27;Predictions&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>其中：</p>
<p><code>model.linear.weight.data.numpy()</code> 和 <code>model.linear.bias.data.numpy()</code> 是什么？</p>
<p>假设你的模型定义如下（这是最常见的情况）：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model = torch.nn.Linear(2, 1)</span><br></pre></td></tr></table></figure>
<p>那么 <code>model</code> 内部有两个可学习的参数：</p>
<ul>
<li><code>weight</code>：形状为 <code>(1, 2)</code> 的张量，对应两个输入特征的权重</li>
<li><code>bias</code>：形状为 <code>(1,)</code> 的张量，对应偏置项</li>
</ul>
<p><strong>各部分含义：</strong></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>.data</th>
<th>获取张量底层的<strong>数据（Tensor）</strong>，不包含梯度信息</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>.numpy()</strong></td>
<td><strong>将 PyTorch 张量（CPU 上）转换为 NumPy 数组，便于打印或绘图</strong></td>
</tr>
</tbody>
</table>
</div>
<p><code>with torch.no_grad():</code> 是什么？</p>
<p>到代码的结尾，模型已经训练完了，现在要进行绘图操作或者对比真实值与预测值，那么就不需要梯度，如果不关闭自动求导（autograd）的话，只会浪费内存（保存中间结果）和浪费时间</p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>以上的整体代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机种子，确保每次结果一致</span></span><br><span class="line">torch.manual_seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line">X = torch.randn(<span class="number">100</span> ,<span class="number">2</span>)</span><br><span class="line">true_w = torch.tensor([<span class="number">2.0</span>, <span class="number">3.0</span>])</span><br><span class="line">true_b = <span class="number">4.0</span>     <span class="comment"># 偏置</span></span><br><span class="line">Y = X @ true_w + true_b + torch.randn(<span class="number">100</span>) * <span class="number">0.1</span> <span class="comment"># 加入一点噪声</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SimpleNN</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(SimpleNN, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.linear = nn.Linear(<span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.linear(x)</span><br><span class="line"></span><br><span class="line">model = SimpleNN()</span><br><span class="line"></span><br><span class="line">criterion = nn.MSELoss()</span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">epochs = <span class="number">1000</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    model.train()</span><br><span class="line">    data = model(X)</span><br><span class="line"></span><br><span class="line">    loss = criterion(data.squeeze(), Y)  <span class="comment"># 预测值data需要压缩成1维</span></span><br><span class="line"></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (epoch + <span class="number">1</span>) % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;Epoch [<span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>/1000], Loss: <span class="subst">&#123;loss.item():<span class="number">.4</span>f&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Predicted Weight: <span class="subst">&#123;model.linear.weight.data.numpy()&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Predicted Bias: <span class="subst">&#123;model.linear.bias.data.numpy()&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    predictions = model(X)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘图</span></span><br><span class="line">plt.scatter(X[:, <span class="number">0</span>], Y, color=<span class="string">&#x27;blue&#x27;</span>, label=<span class="string">&#x27;True values&#x27;</span>)</span><br><span class="line">plt.scatter(X[:, <span class="number">0</span>], predictions, color=<span class="string">&#x27;red&#x27;</span>, label=<span class="string">&#x27;Predictions&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>结果如下：</p>
<p><img src="/img/lazy.gif" data-original="5.png" alt=""></p>
<h2 id="卷积神经网络"><a href="#卷积神经网络" class="headerlink" title="卷积神经网络"></a>卷积神经网络</h2><p><strong>卷积神经网络 (Convolutional Neural Networks, CNN)</strong> 是一类专门用于处理具有网格状拓扑结构数据（如图像）的深度学习模型</p>
<p><img src="/img/lazy.gif" data-original="6.png" alt=""></p>
<p>在图中，CNN 的输出层给出了三个类别的概率：Donald（0.2）、Goofy（0.1）和Tweety（0.7），这表明网络认为输入图像最有可能是 Tweety</p>
<h3 id="基本结构"><a href="#基本结构" class="headerlink" title="基本结构"></a>基本结构</h3><h4 id="输入层（Input-Layer）"><a href="#输入层（Input-Layer）" class="headerlink" title="输入层（Input Layer）"></a>输入层（Input Layer）</h4><p>接收原始图像数据，图像通常被表示为一个三维数组，其中两个维度代表图像的宽度和高度，第三个维度代表颜色通道（例如，RGB图像有三个通道）</p>
<h4 id="卷积层（Convolutional-Layer）"><a href="#卷积层（Convolutional-Layer）" class="headerlink" title="卷积层（Convolutional Layer）"></a>卷积层（Convolutional Layer）</h4><p>用卷积核提取局部特征，如边缘、纹理等，公式如下：</p>
<script type="math/tex; mode=display">
y[i,j]=\sum_{m}\sum_{n}x[i+m,j+n]·k[m,n]+b</script><ul>
<li><script type="math/tex">x</script>：输入图像</li>
<li><script type="math/tex">k</script>：卷积核（权重矩阵）</li>
<li><script type="math/tex">b</script>：偏置</li>
</ul>
<p>应用一组可学习的滤波器（或卷积核）在输入图像上进行卷积操作，以提取局部特征</p>
<p>每个滤波器在输入图像上滑动，生成一个特征图（Feature Map），表示滤波器在不同位置的激活</p>
<p>卷积层可以有多个滤波器，每个滤波器生成一个特征图，所有特征图组成一个特征图集合</p>
<h4 id="激活函数（Activation-Function）-1"><a href="#激活函数（Activation-Function）-1" class="headerlink" title="激活函数（Activation Function）"></a>激活函数（Activation Function）</h4><p>通常在卷积层之后应用非线性激活函数，如 ReLU（Rectified Linear Unit），以引入非线性特性，使网络能够学习更复杂的模式</p>
<h4 id="池化层（Pooling-Layer）"><a href="#池化层（Pooling-Layer）" class="headerlink" title="池化层（Pooling Layer）"></a>池化层（Pooling Layer）</h4><ul>
<li>用于降低特征图的空间维度，减少计算量和参数数量，同时保留最重要的特征信息</li>
<li>最常见的池化操作是 <strong>最大池化（Max Pooling）</strong> 和 <strong>平均池化（Average Pooling）</strong></li>
<li>最大池化选择区域内的最大值，而平均池化计算区域内的平均值</li>
</ul>
<h4 id="归一化层（Normalization-Layer，-可选）"><a href="#归一化层（Normalization-Layer，-可选）" class="headerlink" title="归一化层（Normalization Layer， 可选）"></a>归一化层（Normalization Layer， 可选）</h4><p>例如，<strong>局部响应归一化（Local Response Normalization, LRN）</strong> 或 <strong>批归一化（Batch Normalization）</strong></p>
<p>这些层有助于加速训练过程，提高模型的稳定性</p>
<p>归一化：</p>
<script type="math/tex; mode=display">
\hat{x}_i = \frac{x_i - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}}</script><p>其中 <script type="math/tex">\epsilon</script> 是一个极小常数，防止除以0</p>
<h4 id="全连接层（Fully-Connected-Layer）"><a href="#全连接层（Fully-Connected-Layer）" class="headerlink" title="全连接层（Fully Connected Layer）"></a>全连接层（Fully Connected Layer）</h4><p>在 CNN 的末端，将前面层提取的特征图展平（Flatten）成一维向量，然后输入到全连接层</p>
<p>全连接层的每个神经元都与前一层的所有神经元相连，用于综合特征并进行最终的分类或回归</p>
<h4 id="输出层（Output-Layer）"><a href="#输出层（Output-Layer）" class="headerlink" title="输出层（Output Layer）"></a>输出层（Output Layer）</h4><p>根据任务的不同，输出层可以有不同的形式</p>
<p>对于分类任务，通常使用 Softmax 函数将输出转换为概率分布，表示输入属于各个类别的概率</p>
<h4 id="损失函数（Loss-Function）-1"><a href="#损失函数（Loss-Function）-1" class="headerlink" title="损失函数（Loss Function）"></a>损失函数（Loss Function）</h4><p>用于衡量模型预测与真实标签之间的差异</p>
<p>常见的损失函数包括交叉熵损失（Cross-Entropy Loss）用于多分类任务，均方误差（Mean Squared Error, MSE）用于回归任务</p>
<h4 id="优化器（Optimizer）-1"><a href="#优化器（Optimizer）-1" class="headerlink" title="优化器（Optimizer）"></a>优化器（Optimizer）</h4><p>用于根据损失函数的梯度更新网络的权重。常见的优化器包括随机梯度下降（SGD）、Adam、RMSprop等</p>
<h4 id="正则化（Regularization）"><a href="#正则化（Regularization）" class="headerlink" title="正则化（Regularization）"></a>正则化（Regularization）</h4><p>包括 Dropout、L1/L2 正则化等技术，用于防止模型过拟合</p>
<p>这些层可以堆叠形成更深的网络结构，以提高模型的学习能力</p>
<p>CNN 的深度和复杂性可以根据任务的需求进行调整</p>
<h3 id="实例-2"><a href="#实例-2" class="headerlink" title="实例"></a>实例</h3><p>主要步骤：</p>
<ul>
<li><strong>数据加载与预处理</strong>：使用 <code>torchvision</code> 加载和预处理 MNIST 数据</li>
<li><strong>模型构建</strong>：定义卷积层、池化层和全连接层</li>
<li><strong>训练</strong>：通过损失函数和优化器进行模型训练</li>
<li><strong>评估</strong>：测试集上计算模型的准确率</li>
<li><strong>可视化</strong>：展示部分测试样本及其预测结果</li>
</ul>
<h4 id="数据加载"><a href="#数据加载" class="headerlink" title="数据加载"></a>数据加载</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.ToTensor(),                   <span class="comment"># 转为张量</span></span><br><span class="line">    transforms.Normalize((<span class="number">0.5</span>,), (<span class="number">0.5</span>,))     <span class="comment"># 归一化到 [-1, 1]</span></span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载 MNIST 数据集</span></span><br><span class="line">train_dataset = datasets.MNIST(root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">True</span>, transform=transform, download=<span class="literal">True</span>)</span><br><span class="line">test_dataset = datasets.MNIST(root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">False</span>, transform=transform, download=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=<span class="number">64</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line">test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=<span class="number">64</span>, shuffle=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<h4 id="定义CNN模型"><a href="#定义CNN模型" class="headerlink" title="定义CNN模型"></a>定义CNN模型</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定于一个CNN</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SimpleNN</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(SimpleNN, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="comment"># 定义卷积层：输入1通道，输出32通道，卷积核大小3x3</span></span><br><span class="line">        <span class="variable language_">self</span>.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">32</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 定义卷积层：输入32通道，输出64通道</span></span><br><span class="line">        <span class="variable language_">self</span>.conv2 = nn.Conv2d(<span class="number">32</span>, <span class="number">64</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 定义全连接层</span></span><br><span class="line">        <span class="variable language_">self</span>.fc1 = nn.Linear(<span class="number">64</span> * <span class="number">7</span> * <span class="number">7</span>, <span class="number">128</span>)</span><br><span class="line">        <span class="variable language_">self</span>.fc2 = nn.Linear(<span class="number">128</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = F.relu(<span class="variable language_">self</span>.conv1(x))   <span class="comment"># 第一层卷积 + ReLU</span></span><br><span class="line">        x = F.max_pool2d(x, <span class="number">2</span>)      <span class="comment"># 最大池化</span></span><br><span class="line">        x = F.relu(<span class="variable language_">self</span>.conv2(x))   <span class="comment"># 第二层卷积 + ReLU</span></span><br><span class="line">        x = F.max_pool2d(x, <span class="number">2</span>)      <span class="comment"># 最大池化</span></span><br><span class="line">        x = x.view(-<span class="number">1</span>, <span class="number">64</span> * <span class="number">7</span> * <span class="number">7</span>)  <span class="comment"># 展平操作 (改变张量形状)</span></span><br><span class="line">        x = F.relu(<span class="variable language_">self</span>.fc1(x))     <span class="comment"># 全连接层 + ReLU</span></span><br><span class="line">        x = <span class="variable language_">self</span>.fc2(x)             <span class="comment"># 全连接层输出</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建模型实例</span></span><br><span class="line">model = SimpleNN()</span><br></pre></td></tr></table></figure>
<p>其中：</p>
<ul>
<li><strong>卷积核大小</strong>（kernel size）对卷积神经网络（CNN）的性能、感受野、参数量和特征提取能力有 <strong>显著影响</strong> （现代CNN普遍使用3x3小卷积核）</li>
<li><strong>感受野</strong>：卷积核大小决定了 <strong>单个输出神经元能看到的输入区域范围</strong>，<strong>大卷积核</strong>（如 7×7）：感受野大，能捕获更大范围的上下文信息（适合检测大物体或全局结构），<strong>小卷积核</strong>（如 3×3）：感受野小，更关注局部细节（如边缘、角点）</li>
<li><strong>参数量</strong>：假设输入通道数为<script type="math/tex">C_{in}</script>，输出通道数为<script type="math/tex">C_{out}</script>，卷积核大小为<script type="math/tex">K</script> X <script type="math/tex">K</script>，则参数量为：</li>
</ul>
<script type="math/tex; mode=display">
K \times K\times C_{in} \times C_{out} + C_{out}+(偏置b)</script><ul>
<li><strong>stride</strong>：步长为1，代表卷积核每次在输入图像上移动的像素数，<code>stride=1</code> 表示卷积核每次向右（或向下）<strong>移动 1 个像素</strong>（<code>stride</code> 越大，输出越小，计算量越少，但会 <strong>丢失更多空间信息</strong>），其输出尺寸公式（高度方向）如下：</li>
</ul>
<script type="math/tex; mode=display">
H_{\text{out}} = \left\lfloor \frac{H_{\text{in}} + 2P - K}{S} \right\rfloor + 1\\
H_{in} ：输入高度\\
K ：卷积核高度（通常为奇数，如 3、5）\\
P：上下填充的像素数（padding）\\
S ：步长（stride）\\
⌊⋅⌋ ：向下取整（floor 函数）\\</script><ul>
<li><strong>padding</strong>：填充为1，在输入图像的 <strong>四周补 0</strong>（zero-padding），每边补 <code>padding</code> 个像素，<code>padding=1</code> 表示在 <strong>上下左右各补 1 行/列 0</strong></li>
<li><strong>stride=1,padding=1</strong>：输入为HxW，那么输出仍是HxW</li>
</ul>
<h4 id="损失函数、优化函数和训练"><a href="#损失函数、优化函数和训练" class="headerlink" title="损失函数、优化函数和训练"></a>损失函数、优化函数和训练</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 损失函数和优化器</span></span><br><span class="line">criterion = nn.CrossEntropyLoss()                                 <span class="comment"># 多分类交叉熵损失</span></span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">0.01</span>, momentum=<span class="number">0.9</span>)  <span class="comment"># 学习率和动量</span></span><br><span class="line"></span><br><span class="line">epochs = <span class="number">5</span></span><br><span class="line">model.train()</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    total_loss = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> images, labels <span class="keyword">in</span> train_loader:</span><br><span class="line">        outputs = model(images)</span><br><span class="line">        loss = criterion(outputs, labels)</span><br><span class="line"></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        total_loss += loss.item()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Epoch [<span class="subst">&#123;epoch+<span class="number">1</span>&#125;</span>/<span class="subst">&#123;epochs&#125;</span>], Loss: <span class="subst">&#123;total_loss / <span class="built_in">len</span>(train_loader):<span class="number">.4</span>f&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<h4 id="测试评估"><a href="#测试评估" class="headerlink" title="测试评估"></a>测试评估</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">model.<span class="built_in">eval</span>()  <span class="comment"># 设置为评估模式</span></span><br><span class="line">correct = <span class="number">0</span></span><br><span class="line">total = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():  <span class="comment"># 评估时不需要计算梯度</span></span><br><span class="line">    <span class="keyword">for</span> images, labels <span class="keyword">in</span> test_loader:</span><br><span class="line">        outputs = model(images)               <span class="comment"># 前向传播，输出形状 [batch_size, 10]</span></span><br><span class="line">        _, predicted = torch.<span class="built_in">max</span>(outputs, <span class="number">1</span>)  <span class="comment"># 取每行最大值的索引（预测类别）</span></span><br><span class="line">        total += labels.size(<span class="number">0</span>)               <span class="comment"># 累计总样本数</span></span><br><span class="line">        correct += (predicted == labels).<span class="built_in">sum</span>().item()  <span class="comment"># 累计正确预测数</span></span><br><span class="line"></span><br><span class="line">accuracy = <span class="number">100</span> * correct / total</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Test Accuracy: <span class="subst">&#123;accuracy:<span class="number">.2</span>f&#125;</span>%&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>其中：</p>
<ul>
<li>(predicted == labels) 返回布尔张量，sum()将True视为1，False视为0，item()将单元素张量转化为python数值</li>
</ul>
<h4 id="完整代码如下：（可视化部分可以不看）"><a href="#完整代码如下：（可视化部分可以不看）" class="headerlink" title="完整代码如下：（可视化部分可以不看）"></a>完整代码如下：（可视化部分可以不看）</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"><span class="keyword">import</span> torchvision.datasets <span class="keyword">as</span> datasets</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt </span><br><span class="line"></span><br><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.ToTensor(),                   <span class="comment"># 转为张量</span></span><br><span class="line">    transforms.Normalize((<span class="number">0.5</span>,), (<span class="number">0.5</span>,))     <span class="comment"># 归一化到 [-1, 1]</span></span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载 MNIST 数据集</span></span><br><span class="line">train_dataset = datasets.MNIST(root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">True</span>, transform=transform, download=<span class="literal">True</span>)</span><br><span class="line">test_dataset = datasets.MNIST(root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">False</span>, transform=transform, download=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=<span class="number">64</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line">test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=<span class="number">64</span>, shuffle=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定于一个CNN</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SimpleNN</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(SimpleNN, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="comment"># 定义卷积层：输入1通道，输出32通道，卷积核大小3x3</span></span><br><span class="line">        <span class="variable language_">self</span>.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">32</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 定义卷积层：输入32通道，输出64通道</span></span><br><span class="line">        <span class="variable language_">self</span>.conv2 = nn.Conv2d(<span class="number">32</span>, <span class="number">64</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 定义全连接层</span></span><br><span class="line">        <span class="variable language_">self</span>.fc1 = nn.Linear(<span class="number">64</span> * <span class="number">7</span> * <span class="number">7</span>, <span class="number">128</span>)</span><br><span class="line">        <span class="variable language_">self</span>.fc2 = nn.Linear(<span class="number">128</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = F.relu(<span class="variable language_">self</span>.conv1(x))   <span class="comment"># 第一层卷积 + ReLU</span></span><br><span class="line">        x = F.max_pool2d(x, <span class="number">2</span>)      <span class="comment"># 最大池化</span></span><br><span class="line">        x = F.relu(<span class="variable language_">self</span>.conv2(x))   <span class="comment"># 第二层卷积 + ReLU</span></span><br><span class="line">        x = F.max_pool2d(x, <span class="number">2</span>)      <span class="comment"># 最大池化</span></span><br><span class="line">        x = x.view(-<span class="number">1</span>, <span class="number">64</span> * <span class="number">7</span> * <span class="number">7</span>)  <span class="comment"># 展平操作 (改变张量形状)</span></span><br><span class="line">        x = F.relu(<span class="variable language_">self</span>.fc1(x))     <span class="comment"># 全连接层 + ReLU</span></span><br><span class="line">        x = <span class="variable language_">self</span>.fc2(x)             <span class="comment"># 全连接层输出</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建模型实例</span></span><br><span class="line">model = SimpleNN()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 损失函数和优化器</span></span><br><span class="line">criterion = nn.CrossEntropyLoss()                                 <span class="comment"># 多分类交叉熵损失</span></span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">0.01</span>, momentum=<span class="number">0.9</span>)  <span class="comment"># 学习率和动量</span></span><br><span class="line"></span><br><span class="line">epochs = <span class="number">5</span></span><br><span class="line">model.train()</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    total_loss = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> images, labels <span class="keyword">in</span> train_loader:</span><br><span class="line">        outputs = model(images)</span><br><span class="line">        loss = criterion(outputs, labels)</span><br><span class="line"></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        total_loss += loss.item()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Epoch [<span class="subst">&#123;epoch+<span class="number">1</span>&#125;</span>/<span class="subst">&#123;epochs&#125;</span>], Loss: <span class="subst">&#123;total_loss / <span class="built_in">len</span>(train_loader):<span class="number">.4</span>f&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line">correct = <span class="number">0</span></span><br><span class="line">total = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="keyword">for</span> images, labels <span class="keyword">in</span> test_loader:</span><br><span class="line">        outputs = model(images)</span><br><span class="line">        _, predictions = torch.<span class="built_in">max</span>(outputs, <span class="number">1</span>)</span><br><span class="line">        total += labels.size(<span class="number">0</span>)</span><br><span class="line">        correct += (predictions == labels).<span class="built_in">sum</span>().item()</span><br><span class="line"></span><br><span class="line">accuracy = <span class="number">100</span> * correct / total</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Test Accuracy: <span class="subst">&#123;accuracy:<span class="number">.2</span>f&#125;</span>%&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可视化测试结果</span></span><br><span class="line">dataiter = <span class="built_in">iter</span>(test_loader)</span><br><span class="line">images, labels = <span class="built_in">next</span>(dataiter)</span><br><span class="line">outputs = model(images)</span><br><span class="line">_, predictions = torch.<span class="built_in">max</span>(outputs, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">fig, axes = plt.subplots(<span class="number">1</span>, <span class="number">6</span>, figsize=(<span class="number">12</span>, <span class="number">4</span>))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">6</span>):</span><br><span class="line">    axes[i].imshow(images[i][<span class="number">0</span>], cmap=<span class="string">&#x27;gray&#x27;</span>)</span><br><span class="line">    axes[i].set_title(<span class="string">f&quot;Label: <span class="subst">&#123;labels[i]&#125;</span>\nPred: <span class="subst">&#123;predictions[i]&#125;</span>&quot;</span>)</span><br><span class="line">    axes[i].axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/img/lazy.gif" data-original="7.png" alt=""></p>
<h2 id="循环神经网络"><a href="#循环神经网络" class="headerlink" title="循环神经网络"></a>循环神经网络</h2><p><strong>循环神经网络（Recurrent Neural Networks, RNN）</strong> 是一类神经网络架构，专门用于处理序列数据，能够捕捉时间序列或有序数据的动态信息，能够处理序列数据，如文本、时间序列或音频</p>
<p>RNN 的关键特性是其能够保持隐状态（hidden state），使得网络能够记住先前时间步的信息</p>
<h3 id="基本结构-1"><a href="#基本结构-1" class="headerlink" title="基本结构"></a>基本结构</h3><p>在传统的前馈神经网络中，数据是从输入层流向输出层的，而在 RNN 中，数据不仅沿着网络层级流动，还会在每个时间步骤上传播到当前的隐层状态，从而将之前的信息传递到下一个时间步骤</p>
<p>RNN公式如下：</p>
<script type="math/tex; mode=display">
h_t=tanh(W_{hx}x_t+W_{hh}h_{t-1}+b_{h})</script><p>其中：</p>
<ul>
<li><script type="math/tex">h_t</script>：时间步 <script type="math/tex">t</script> 的隐状态</li>
<li><script type="math/tex">x_t</script>：时间步 <script type="math/tex">t</script> 的输入</li>
<li><script type="math/tex">W_{hx}</script>：输入到隐状态的权重矩阵</li>
<li><script type="math/tex">W_{hh}</script>：隐状态到隐状态的权重矩阵（循环连接）</li>
<li><script type="math/tex">b_h</script>：偏置项</li>
<li><script type="math/tex">tanh</script>：激活函数（或者用ReLU）</li>
</ul>
<p>其输出为：</p>
<script type="math/tex; mode=display">
y_t=W_{yh}h_t+b_y</script><p>其中 <script type="math/tex">y_t</script> 是模型在时间步 <script type="math/tex">t</script> 的输出（如词汇表的概率分布）</p>
<p><strong>隐状态：</strong></p>
<ul>
<li><strong>网络在时间步 <script type="math/tex">t</script> 对过去所有输入 <script type="math/tex">x_1,x_2,…,x_t</script> 的“记忆”或“摘要”</strong></li>
<li>它编码了到当前时刻为止的上下文信息</li>
<li>在每个时间步，隐状态都会被更新，并传递给下一个时间步</li>
</ul>
<p>初始隐状态 <em>h</em>0 通常被初始化为零向量（zero initialization），即：</p>
<script type="math/tex; mode=display">
h_0=0</script><p><img src="/img/lazy.gif" data-original="8.png" alt=""></p>
<ul>
<li><strong>输入序列（Xt, Xt-1, Xt+1, …）</strong>：图中的粉色圆圈代表输入序列中的各个元素，如Xt表示当前时间步的输入，Xt-1表示前一个时间步的输入，以此类推</li>
<li><strong>隐藏状态（ht, ht-1, ht+1, …）</strong>：绿色矩形代表RNN的隐藏状态，它在每个时间步存储有关序列的信息。ht是当前时间步的隐藏状态，ht-1是前一个时间步的隐藏状态</li>
<li><strong>权重矩阵（U, W, V）</strong>：<ul>
<li><code>U</code>：输入到隐藏状态的权重矩阵，用于将输入<code>Xt</code>转换为隐藏状态的一部分</li>
<li><code>W</code>：隐藏状态到隐藏状态的权重矩阵，用于将前一时间步的隐藏状态<code>ht-1</code>转换为当前时间步隐藏状态的一部分</li>
<li><code>V</code>：隐藏状态到输出的权重矩阵，用于将隐藏状态<code>ht</code>转换为输出<code>Yt</code></li>
</ul>
</li>
<li><strong>输出序列（Yt, Yt-1, Yt+1, …）</strong>：蓝色圆圈代表RNN在每个时间步的输出，如Yt是当前时间步的输出</li>
<li><strong>循环连接</strong>：RNN的特点是隐藏状态的循环连接，这允许网络在处理当前时间步的输入时考虑到之前时间步的信息</li>
<li><strong>展开（Unfold）</strong>：图中展示了RNN在序列上的展开过程，这有助于理解RNN如何在时间上处理序列数据。在实际的RNN实现中，这些步骤是并行处理的，但在概念上，我们可以将其展开来理解信息是如何流动的</li>
<li><strong>信息流动</strong>：信息从输入序列通过权重矩阵U传递到隐藏状态，然后通过权重矩阵W在时间步之间传递，最后通过权重矩阵V从隐藏状态传递到输出序列</li>
</ul>
<h3 id="模块"><a href="#模块" class="headerlink" title="模块"></a>模块</h3><p>PyTorch 提供了几种 RNN 模块，包括：</p>
<ul>
<li><code>torch.nn.RNN</code>：基本的RNN单元</li>
<li><code>torch.nn.LSTM</code>：长短期记忆单元，能够学习长期依赖关系</li>
<li><code>torch.nn.GRU</code>：门控循环单元，是LSTM的简化版本，但通常更容易训练</li>
</ul>
<p>使用 RNN 类时，您需要指定输入的维度、隐藏层的维度以及其他一些超参数</p>
<h3 id="实例1-1"><a href="#实例1-1" class="headerlink" title="实例1"></a>实例1</h3><h4 id="定义RNN模型"><a href="#定义RNN模型" class="headerlink" title="定义RNN模型"></a>定义RNN模型</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader, TensorDataset</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SimpleRNN</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_size, hidden_size, output_size</span>):</span><br><span class="line">        <span class="built_in">super</span>(SimpleRNN, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="comment"># 定义RNN层</span></span><br><span class="line">        <span class="variable language_">self</span>.rnn = nn.RNN(input_size, hidden_size, batch_first=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment"># 定义全连接层</span></span><br><span class="line">        <span class="variable language_">self</span>.fc = nn.Linear(hidden_size, output_size)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># x: (batch_size, seq_len, input_size)</span></span><br><span class="line">        out, _ = <span class="variable language_">self</span>.rnn(x)  <span class="comment"># out: (batch_size, seq_len, hidden_size)</span></span><br><span class="line">        <span class="comment"># 取序列最后一个时间步的输出作为模型的输出</span></span><br><span class="line">        out = out[:, -<span class="number">1</span>, :]  <span class="comment"># (batch_size, hidden_size)</span></span><br><span class="line">        out = <span class="variable language_">self</span>.fc(out)  <span class="comment"># 全连接层</span></span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>
<p>其中：</p>
<ul>
<li>Pytorch中，nn.RNN中输入张量的默认形状是：（seq_len，batch_size，input_size），但很多时候（尤其是在处理实际数据集时），我们习惯把 <strong>batch 维度放在最前面</strong>（batch_size，seq_len，input_size），所以我们需要设置 <code>batch_first = True</code>，如果不设 <code>batch_first=True</code>，你传入 <code>(32, 10, 5)</code>（32个样本，每条序列长10，每个时间步特征5维）会被误认为是“序列长度=32”，导致错误</li>
<li><code>out, _ = self.rnn(x)</code> 中的 <code>_</code> 是什么意思？可以查看RNN的返回值，发现返回了一个output（所有时间步的隐状态输出）和一个hidden（最后一个时间步的隐状态），在这个实例中我们只需要output而不需要hidden，就使用 <code>_</code>，表示这个变量我不想用</li>
</ul>
<p><img src="/img/lazy.gif" data-original="9.png" alt=""></p>
<ul>
<li><code>out = out[:, -1, :]</code> 是什么意思？<code>:</code> 表示取 <strong>所有 batch 样本</strong>，<code>-1</code> 表示取 <strong>最后一个时间步</strong>（即第 <code>seq_len - 1</code> 个位置），<code>:</code> 表示取该时间步的 <strong>全部 hidden 特征</strong></li>
</ul>
<h4 id="训练数据"><a href="#训练数据" class="headerlink" title="训练数据"></a>训练数据</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 生成一些随机序列数据</span></span><br><span class="line">num_samples = <span class="number">1000</span></span><br><span class="line">seq_len = <span class="number">10</span></span><br><span class="line">input_size = <span class="number">5</span></span><br><span class="line">output_size = <span class="number">2</span>  <span class="comment"># 假设二分类问题</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机生成输入数据 (batch_size, seq_len, input_size)</span></span><br><span class="line">X = torch.randn(num_samples, seq_len, input_size)  <span class="comment"># 生成3维数据，尺寸分别为 bxsxi</span></span><br><span class="line"><span class="comment"># 随机生成目标标签 (batch_size, output_size)</span></span><br><span class="line">Y = torch.randint(<span class="number">0</span>, output_size, (num_samples,))  <span class="comment"># 数据在0~output_size之间，共num_samples列，1行</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建数据加载器</span></span><br><span class="line">dataset = TensorDataset(X, Y)</span><br><span class="line">train_loader = DataLoader(dataset, batch_size=<span class="number">32</span>, shuffle=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<h4 id="损失函数，优化器和训练"><a href="#损失函数，优化器和训练" class="headerlink" title="损失函数，优化器和训练"></a>损失函数，优化器和训练</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># hidden_size：每个 token 或时间步的特征向量有多少个元素</span></span><br><span class="line">model = SimpleRNN(input_size=input_size, hidden_size=<span class="number">64</span>, output_size=output_size)</span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = optim.Adam(model.parameters(), lr=<span class="number">0.001</span>)</span><br><span class="line"></span><br><span class="line">num_epochs = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    model.train()</span><br><span class="line">    total_loss = <span class="number">0</span></span><br><span class="line">    correct = <span class="number">0</span></span><br><span class="line">    total = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> inputs, labels <span class="keyword">in</span> train_loader:</span><br><span class="line">        outputs = model(inputs)</span><br><span class="line">        loss = criterion(outputs, labels)</span><br><span class="line">        </span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        </span><br><span class="line">        total_loss += loss.item()</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Epoch [<span class="subst">&#123;epoch+<span class="number">1</span>&#125;</span>/<span class="subst">&#123;num_epochs&#125;</span>], Loss: <span class="subst">&#123;total_loss / <span class="built_in">len</span>(train_loader):<span class="number">.4</span>f&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<h4 id="评估模型"><a href="#评估模型" class="headerlink" title="评估模型"></a>评估模型</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 测试模型</span></span><br><span class="line">model.<span class="built_in">eval</span>()  <span class="comment"># 设置模型为评估模式</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    total = <span class="number">0</span></span><br><span class="line">    correct = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> inputs, labels <span class="keyword">in</span> train_loader:</span><br><span class="line">        outputs = model(inputs)</span><br><span class="line">        _, predicted = torch.<span class="built_in">max</span>(outputs, <span class="number">1</span>)</span><br><span class="line">        total += labels.size(<span class="number">0</span>)</span><br><span class="line">        correct += (predicted == labels).<span class="built_in">sum</span>().item()</span><br><span class="line"></span><br><span class="line">    accuracy = <span class="number">100</span> * correct / total</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Test Accuracy: <span class="subst">&#123;accuracy:<span class="number">.2</span>f&#125;</span>%&quot;</span>)</span><br></pre></td></tr></table></figure>
<h3 id="实例2-1"><a href="#实例2-1" class="headerlink" title="实例2"></a>实例2</h3><p>也可以使用RNN来预测字符</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader, TensorDataset</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据集：字符序列预测</span></span><br><span class="line">char_set = <span class="built_in">list</span>(<span class="string">&quot;TechOtakusSaveTheWorld&quot;</span>)</span><br><span class="line">char_to_idx = &#123;c: i <span class="keyword">for</span> i, c <span class="keyword">in</span> <span class="built_in">enumerate</span>(char_set)&#125;</span><br><span class="line">idx_to_char = &#123;i: c <span class="keyword">for</span> i, c <span class="keyword">in</span> <span class="built_in">enumerate</span>(char_set)&#125;</span><br><span class="line"></span><br><span class="line">input_str = <span class="string">&quot;TechOtakusSaveTheWorld&quot;</span></span><br><span class="line">target_str = <span class="string">&quot;OtakusSaveTheWorldTech&quot;</span></span><br><span class="line">input_data = [char_to_idx[c] <span class="keyword">for</span> c <span class="keyword">in</span> input_str]</span><br><span class="line">target_data = [char_to_idx[c] <span class="keyword">for</span> c <span class="keyword">in</span> target_str]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 转换为独热编码</span></span><br><span class="line">input_ont_hot = np.eye(<span class="built_in">len</span>(char_set))[input_data]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 转换为张量</span></span><br><span class="line">inputs = torch.tensor(input_ont_hot, dtype=torch.float32)</span><br><span class="line">targets = torch.tensor(target_data, dtype=torch.long)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型超参数</span></span><br><span class="line">input_size = <span class="built_in">len</span>(char_set)</span><br><span class="line">hidden_size = <span class="number">8</span></span><br><span class="line">output_size = <span class="built_in">len</span>(char_set)</span><br><span class="line">num_epochs = <span class="number">500</span></span><br><span class="line">lr = <span class="number">0.1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义RNN模型</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SimpleRNN</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_size, hidden_size, output_size</span>):</span><br><span class="line">        <span class="built_in">super</span>(SimpleRNN, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.rnn = nn.RNN(input_size, hidden_size, batch_first=<span class="literal">True</span>)</span><br><span class="line">        <span class="variable language_">self</span>.fc = nn.Linear(hidden_size, output_size)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, hidden</span>):</span><br><span class="line">        out, hidden = <span class="variable language_">self</span>.rnn(x, hidden)</span><br><span class="line">        out = <span class="variable language_">self</span>.fc(out)</span><br><span class="line">        <span class="keyword">return</span> out, hidden</span><br><span class="line">    </span><br><span class="line">model = SimpleRNN(input_size=input_size, hidden_size=hidden_size, output_size=output_size)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义损失函数和优化器</span></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = optim.Adam(model.parameters(), lr=lr)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练</span></span><br><span class="line">losses =[]</span><br><span class="line">hidden = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">    outputs, hidden = model(inputs.unsqueeze(<span class="number">0</span>), hidden)</span><br><span class="line">    hidden = hidden.detach() <span class="comment"># 防止梯度爆炸</span></span><br><span class="line"></span><br><span class="line">    loss = criterion(outputs.view(-<span class="number">1</span>, output_size), targets)</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line">    losses.append(loss.item())</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> (epoch + <span class="number">1</span>) % <span class="number">20</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Epoch [<span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>/<span class="subst">&#123;num_epochs&#125;</span>], Loss: <span class="subst">&#123;loss.item():<span class="number">.4</span>f&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试RNN</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    test_hidden = <span class="literal">None</span></span><br><span class="line">    test_output, _ = model(inputs.unsqueeze(<span class="number">0</span>), test_hidden)</span><br><span class="line">    predicted = torch.argmax(test_output, dim=<span class="number">2</span>).squeeze().numpy()</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Input sequence: &quot;</span>, <span class="string">&#x27;&#x27;</span>.join([idx_to_char[i] <span class="keyword">for</span> i <span class="keyword">in</span> input_data]))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Predicted sequence: &quot;</span>, <span class="string">&#x27;&#x27;</span>.join([idx_to_char[i] <span class="keyword">for</span> i <span class="keyword">in</span> predicted]))</span><br></pre></td></tr></table></figure>
<p><img src="/img/lazy.gif" data-original="10.png" alt=""></p>
<p>其中：</p>
<ul>
<li><code>char_to_idx = &#123;c: i for i, c in enumerate(char_set)&#125;</code> 表示构建 <strong>字符 → 索引</strong> 的字典，比如 <code>&#39;T&#39; → 0</code>, <code>&#39;e&#39; → 1</code>, <code>&#39;c&#39; → 2</code>, …, <code>&#39;d&#39; → 21</code>，后面的 <code>idx_to_char</code> 同理，类似 <code>0 -&gt; &#39;T&#39;</code></li>
<li><strong>独热编码：</strong>一种将 <strong>类别型变量（categorical variable）</strong> 转换为 <strong>机器学习模型可理解的数值形式</strong> 的常用方法，向量中<strong>只有一个位置是 1</strong>（表示“激活”或“选中”），其余都是 0，“1” 的位置对应原始类别，举个例子，比如说我们有颜色为 <code>[&quot;红&quot;,&quot;绿&quot;,&quot;蓝&quot;]</code>，那么红的独热编码为 <code>[1,0,0]</code>，而绿色是 <code>[0,1,0]</code>，以此类推，但实际上这些类别<strong>没有大小或顺序关系</strong>（称为<strong>名义变量</strong>）（适用于不能直接处理字符串场景，但是不适用于类别太多的场景）</li>
<li><code>input_ont_hot = np.eye(len(char_set))[input_data]</code> ，这里的 <code>input_data</code> 是整数数组的索引，然后<code>np.eye()</code>的作用就是构造一个NxN的矩阵</li>
<li><code>hidden</code>为什么设置为None？当你传入 <code>None</code>，PyTorch 会根据输入自动创建合适形状的零张量，这是标准做法，尤其在训练开始时没有历史状态</li>
<li>为什么需要 <code>inputs.unsqueeze(0)</code>？<strong>RNN 要求输入是 3D 张量</strong>，但原始 <code>inputs</code> 是 <code>(seq_len, input_size)</code>（2D），<code>unsqueeze(0)</code> 在第 0 维增加一个 batch 维度 → <code>(1, seq_len, input_size)</code></li>
<li>为什么需要 <code>hidden.detach()</code>？RNN 是按时间步展开的，理论上梯度会从当前 epoch 回传到训练开始，但实际上，我们通常只在<strong>当前 batch 内</strong>计算梯度，<code>detach()</code> 会<strong>切断 hidden state 的历史计算图</strong>，使梯度不会回传到上一个 epoch</li>
<li>为什么需要 <code>view(-1)</code>？因为 <code>criterion *=* nn.CrossEntropyLoss()</code> 要求输入形状为（N,C），但是由上面可知我们为了可以正常进行RNN我们手动添加了一个维度，所以这里给他去除</li>
</ul>
<h2 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h2><p>PyTorch 提供了强大的数据加载和处理工具，主要包括：</p>
<ul>
<li><strong><code>torch.utils.data.Dataset</code></strong>：数据集的抽象类，需要自定义并实现 <code>__len__</code>（数据集大小）和 <code>__getitem__</code>（按索引获取样本）</li>
<li><strong><code>torch.utils.data.TensorDataset</code></strong>：基于张量的数据集，适合处理数据-标签对，直接支持批处理和迭代</li>
<li><strong><code>torch.utils.data.DataLoader</code></strong>：封装 Dataset 的迭代器，提供批处理、数据打乱、多线程加载等功能，便于数据输入模型训练</li>
<li><strong><code>torchvision.datasets.ImageFolder</code></strong>：从文件夹加载图像数据，每个子文件夹代表一个类别，适用于图像分类任务</li>
</ul>
<h3 id="PyTorch-内置数据集"><a href="#PyTorch-内置数据集" class="headerlink" title="PyTorch 内置数据集"></a>PyTorch 内置数据集</h3><p>PyTorch 通过 torchvision.datasets 模块提供了许多常用的数据集，例如：</p>
<ul>
<li><strong>MNIST</strong>：手写数字图像数据集，用于图像分类任务</li>
<li><strong>CIFAR</strong>：包含 10 个类别、60000 张 32x32 的彩色图像数据集，用于图像分类任务</li>
<li><strong>COCO</strong>：通用物体检测、分割、关键点检测数据集，包含超过 330k 个图像和 2.5M 个目标实例的大规模数据集</li>
<li><strong>ImageNet</strong>：包含超过 1400 万张图像，用于图像分类和物体检测等任务</li>
<li><strong>STL-10</strong>：包含 100k 张 96x96 的彩色图像数据集，用于图像分类任务</li>
<li><strong>Cityscapes</strong>：包含 5000 张精细注释的城市街道场景图像，用于语义分割任务</li>
<li><strong>SQUAD</strong>：用于机器阅读理解任务的数据集</li>
</ul>
<p>以上数据集可以通过 torchvision.datasets 模块中的函数进行加载，也可以通过自定义的方式加载其他数据集</p>
<h3 id="torchvision-和-torchtext"><a href="#torchvision-和-torchtext" class="headerlink" title="torchvision 和 torchtext"></a>torchvision 和 torchtext</h3><ul>
<li><strong>torchvision</strong>： 一个图形库，提供了图片数据处理相关的 API 和数据集接口，包括数据集加载函数和常用的图像变换</li>
<li><strong>torchtext</strong>： 自然语言处理工具包，提供了文本数据处理和建模的工具，包括数据预处理和数据加载的方式</li>
</ul>
<h3 id="torch-utils-data-Dataset"><a href="#torch-utils-data-Dataset" class="headerlink" title="torch.utils.data.Dataset"></a>torch.utils.data.Dataset</h3><p>自定义数据集需要继承 torch.utils.data.Dataset 并重写以下两个方法：</p>
<ul>
<li><code>__len__</code>：返回数据集的大小。</li>
<li><code>__getitem__</code>：按索引获取一个数据样本及其标签</li>
</ul>
<p>（比如说本文《数据处理与加载》部分有说明定义Dataset和利用Dataloader来加载数据集，所以这里不再说明一遍）</p>
<h3 id="Dataset-与-DataLoader-的自定义应用"><a href="#Dataset-与-DataLoader-的自定义应用" class="headerlink" title="Dataset 与 DataLoader 的自定义应用"></a>Dataset 与 DataLoader 的自定义应用</h3><p>比如说自定义个CSV：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader, Dataset</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CSVDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, file_path</span>):</span><br><span class="line">        <span class="variable language_">self</span>.data = pd.read_csv(file_path)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(<span class="variable language_">self</span>.data)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, index</span>):</span><br><span class="line">        row = <span class="variable language_">self</span>.data.iloc[index]</span><br><span class="line">        <span class="comment"># 取当前行除了最后一列的所有元素[:-1]</span></span><br><span class="line">        features = torch.tensor(row.iloc[:-<span class="number">1</span>].to_numpy(), dtype=torch.float32)</span><br><span class="line">        <span class="comment"># 取最后一行[-1]</span></span><br><span class="line">        label = torch.tensor(row.iloc[-<span class="number">1</span>], dtype=torch.float32)</span><br><span class="line">        <span class="keyword">return</span> features, label</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 实例化数据集和DataLoader</span></span><br><span class="line">dataset = CSVDataset(<span class="string">&quot;xxx.csv&quot;</span>)</span><br><span class="line">dataloader = DataLoader(dataset=dataset, batch_size=<span class="number">4</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> features, label <span class="keyword">in</span> dataloader:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;特征：&quot;</span>, features)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;标签：&quot;</span>, label)</span><br><span class="line">    <span class="keyword">break</span></span><br></pre></td></tr></table></figure>
<p>其中：</p>
<ul>
<li><code>iloc</code> 是 Pandas 库中基于<strong>整数位置</strong>（integer-location）的索引器，用于通过行号和列号来选择数据</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建示例数据</span></span><br><span class="line">data = pd.DataFrame(&#123;</span><br><span class="line">    <span class="string">&#x27;A&#x27;</span>: [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>],</span><br><span class="line">    <span class="string">&#x27;B&#x27;</span>: [<span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>], </span><br><span class="line">    <span class="string">&#x27;C&#x27;</span>: [<span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>]</span><br><span class="line">&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment"># iloc 的各种用法</span></span><br><span class="line"><span class="built_in">print</span>(data.iloc[<span class="number">0</span>])        <span class="comment"># 第0行（第一行）: [1, 5, 9]</span></span><br><span class="line"><span class="built_in">print</span>(data.iloc[-<span class="number">1</span>])       <span class="comment"># 最后一行: [4, 8, 12]</span></span><br><span class="line"><span class="built_in">print</span>(data.iloc[:, -<span class="number">1</span>])    <span class="comment"># 所有行的最后列: [9, 10, 11, 12]</span></span><br><span class="line"><span class="built_in">print</span>(data.iloc[:, :-<span class="number">1</span>])   <span class="comment"># 所有行，除了最后列: </span></span><br><span class="line">                           <span class="comment">#    A  B</span></span><br><span class="line">                           <span class="comment"># 0  1  5</span></span><br><span class="line">                           <span class="comment"># 1  2  6</span></span><br><span class="line">                           <span class="comment"># 2  3  7  </span></span><br><span class="line">                           <span class="comment"># 3  4  8</span></span><br><span class="line"><span class="built_in">print</span>(data.iloc[<span class="number">0</span>, <span class="number">1</span>])     <span class="comment"># 第0行第1列: 5</span></span><br></pre></td></tr></table></figure>
<h2 id="数据转换"><a href="#数据转换" class="headerlink" title="数据转换"></a>数据转换</h2><p><strong>为什么需要数据转换？</strong></p>
<p><strong>数据预处理</strong>：</p>
<ul>
<li>调整数据格式、大小和范围，使其适合模型输入</li>
<li>例如，图像需要调整为固定大小、张量格式并归一化到 [0,1]</li>
</ul>
<p><strong>数据增强</strong>：</p>
<ul>
<li>在训练时对数据进行变换，以增加多样性</li>
<li>例如，通过随机旋转、翻转和裁剪增加数据样本的变种，避免过拟合</li>
</ul>
<p><strong>灵活性</strong>：</p>
<ul>
<li>通过定义一系列转换操作，可以动态地对数据进行处理，简化数据加载的复杂度</li>
</ul>
<h3 id="基础变换操作"><a href="#基础变换操作" class="headerlink" title="基础变换操作"></a>基础变换操作</h3><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">变换函数名称</th>
<th style="text-align:left">描述</th>
<th style="text-align:left">实例</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">transforms.ToTensor()</td>
<td style="text-align:left">将PIL图像或NumPy数组转换为PyTorch张量，并自动将像素值归一化到 [0, 1]</td>
<td style="text-align:left"><code>transform = transforms.ToTensor()</code></td>
</tr>
<tr>
<td style="text-align:left">transforms.Normalize(mean, std)</td>
<td style="text-align:left">对图像进行标准化，使数据符合零均值和单位方差</td>
<td style="text-align:left"><code>transform = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])</code></td>
</tr>
<tr>
<td style="text-align:left">transforms.Resize(size)</td>
<td style="text-align:left">调整图像尺寸，确保输入到网络的图像大小一致</td>
<td style="text-align:left"><code>transform = transforms.Resize((256, 256))</code></td>
</tr>
<tr>
<td style="text-align:left">transforms.CenterCrop(size)</td>
<td style="text-align:left">从图像中心裁剪指定大小的区域</td>
<td style="text-align:left"><code>transform = transforms.CenterCrop(224)</code></td>
</tr>
</tbody>
</table>
</div>
<p><strong>1、ToTensor</strong></p>
<p>将 PIL 图像或 NumPy 数组转换为 PyTorch 张量</p>
<p>同时将像素值从 [0, 255] 归一化为 [0, 1]</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">from torchvision import transforms</span><br><span class="line"></span><br><span class="line">transform = transforms.ToTensor()</span><br></pre></td></tr></table></figure>
<p><strong>2、Normalize</strong></p>
<p>对数据进行标准化，使其符合特定的均值和标准差</p>
<p>通常用于图像数据，将其像素值归一化为零均值和单位方差</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">transform = transforms.Normalize(mean=[0.5], std=[0.5])  # 归一化到 [-1, 1]</span><br></pre></td></tr></table></figure>
<p><strong>3、Resize</strong></p>
<p>调整图像的大小</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">transform = transforms.Resize((128, 128))  # 将图像调整为 128x128</span><br></pre></td></tr></table></figure>
<p><strong>4、CenterCrop</strong></p>
<p>从图像中心裁剪指定大小的区域</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">transform = transforms.CenterCrop(128)  # 裁剪 128x128 的区域</span><br></pre></td></tr></table></figure>
<h3 id="数据增强操作"><a href="#数据增强操作" class="headerlink" title="数据增强操作"></a>数据增强操作</h3><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">变换函数名称</th>
<th style="text-align:left">描述</th>
<th style="text-align:left">实例</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">transforms.RandomHorizontalFlip(p)</td>
<td style="text-align:left">随机水平翻转图像</td>
<td style="text-align:left"><code>transform = transforms.RandomHorizontalFlip(p=0.5)</code></td>
</tr>
<tr>
<td style="text-align:left">transforms.RandomRotation(degrees)</td>
<td style="text-align:left">随机旋转图像</td>
<td style="text-align:left"><code>transform = transforms.RandomRotation(degrees=45)</code></td>
</tr>
<tr>
<td style="text-align:left">transforms.ColorJitter(brightness, contrast, saturation, hue)</td>
<td style="text-align:left">调整图像的亮度、对比度、饱和度和色调</td>
<td style="text-align:left"><code>transform = transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1)</code></td>
</tr>
<tr>
<td style="text-align:left">transforms.RandomCrop(size)</td>
<td style="text-align:left">随机裁剪指定大小的区域</td>
<td style="text-align:left"><code>transform = transforms.RandomCrop(224)</code></td>
</tr>
<tr>
<td style="text-align:left">transforms.RandomResizedCrop(size)</td>
<td style="text-align:left">随机裁剪图像并调整到指定大小</td>
<td style="text-align:left"><code>transform = transforms.RandomResizedCrop(224)</code></td>
</tr>
</tbody>
</table>
</div>
<p><strong>1、RandomCrop</strong></p>
<p>从图像中随机裁剪指定大小</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">transform = transforms.RandomCrop(128)</span><br></pre></td></tr></table></figure>
<p><strong>2、RandomHorizontalFlip</strong></p>
<p>以一定概率水平翻转图像</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">transform = transforms.RandomHorizontalFlip(p=0.5)  # 50% 概率翻转</span><br></pre></td></tr></table></figure>
<p><strong>3、RandomRotation</strong></p>
<p>随机旋转一定角度</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">transform = transforms.RandomRotation(degrees=30)  # 随机旋转 -30 到 +30 度</span><br></pre></td></tr></table></figure>
<p><strong>4、ColorJitter</strong></p>
<p>随机改变图像的亮度、对比度、饱和度或色调</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">transform = transforms.ColorJitter(brightness=0.5, contrast=0.5)</span><br></pre></td></tr></table></figure>
<h3 id="组合变换"><a href="#组合变换" class="headerlink" title="组合变换"></a>组合变换</h3><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">变换函数名称</th>
<th style="text-align:left">描述</th>
<th style="text-align:left">实例</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">transforms.Compose()</td>
<td style="text-align:left">将多个变换组合在一起，按照顺序依次应用</td>
<td style="text-align:left"><code>transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), transforms.Resize((256, 256))])</code></td>
</tr>
</tbody>
</table>
</div>
<p>通过 transforms.Compose 将多个变换组合起来</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.Resize((128, 128)),</span><br><span class="line">    transforms.RandomHorizontalFlip(p=0.5),</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize(mean=[0.5], std=[0.5])</span><br><span class="line">])</span><br></pre></td></tr></table></figure>
<h3 id="自定义转换"><a href="#自定义转换" class="headerlink" title="自定义转换"></a>自定义转换</h3><p>如果 transforms 提供的功能无法满足需求，可以通过自定义类或函数实现</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">CustomTransform</span>:</span><br><span class="line">	<span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, x</span>):</span><br><span class="line">		<span class="keyword">return</span> x * <span class="number">2</span></span><br><span class="line">		</span><br><span class="line">transform = CustomTransfrom()</span><br></pre></td></tr></table></figure>
<h3 id="实例-3"><a href="#实例-3" class="headerlink" title="实例"></a>实例</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets, transforms</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 原始和增强后的图像可视化</span></span><br><span class="line">transform_augment = transforms.Compose([</span><br><span class="line">    transforms.RandomHorizontalFlip(), <span class="comment"># 随机水平翻转图像</span></span><br><span class="line">    transforms.RandomRotation(<span class="number">30</span>),  <span class="comment"># 随机旋转30°</span></span><br><span class="line">    transforms.ToTensor()</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载数据集</span></span><br><span class="line">dataset = datasets.MNIST(root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>, transform=transform_augment)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示图像</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">show_images</span>(<span class="params">dataset</span>):</span><br><span class="line">    fig, axs = plt.subplots(<span class="number">1</span>, <span class="number">5</span>, figsize=(<span class="number">15</span>, <span class="number">5</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">        image, label = dataset[i]</span><br><span class="line">        axs[i].imshow(image.squeeze(<span class="number">0</span>), cmap=<span class="string">&#x27;gray&#x27;</span>)  <span class="comment"># 将 (1, H, W) 转为 (H, W)</span></span><br><span class="line">        axs[i].set_title(<span class="string">f&quot;Label: <span class="subst">&#123;label&#125;</span>&quot;</span>)</span><br><span class="line">        axs[i].axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">show_images(dataset)</span><br></pre></td></tr></table></figure>
<p><img src="/img/lazy.gif" data-original="11.png" alt=""></p>
<h2 id="Transform"><a href="#Transform" class="headerlink" title="Transform"></a>Transform</h2><p>Transformer 模型由 编码器（Encoder） 和 解码器（Decoder） 两部分组成，每部分都由多层堆叠的相同模块构成</p>
<p><img src="/img/lazy.gif" data-original="12.png" alt=""></p>
<h3 id="编码器（Encoder）"><a href="#编码器（Encoder）" class="headerlink" title="编码器（Encoder）"></a>编码器（Encoder）</h3><p>编码器由 NN 层相同的模块堆叠而成，每层包含两个子层：</p>
<ul>
<li><strong>多头自注意力机制（Multi-Head Self-Attention）：</strong>计算输入序列中 <strong>每个词与其他词</strong> 的 <strong>相关性</strong></li>
<li><strong>前馈神经网络（Feed-Forward Neural Network）：</strong>对每个词进行独立的非线性变换</li>
<li>每个子层后面都接有 残差连接（Residual Connection） 和 层归一化（Layer Normalization）</li>
</ul>
<h3 id="解码器（Decoder）"><a href="#解码器（Decoder）" class="headerlink" title="解码器（Decoder）"></a>解码器（Decoder）</h3><p>解码器也由 NN 层相同的模块堆叠而成，每层包含三个子层：</p>
<ul>
<li><strong>掩码多头自注意力机制（Masked Multi-Head Self-Attention）：</strong>计算输出序列中每个词与前面词的相关性（使用掩码防止未来信息泄露）</li>
<li><strong>编码器-解码器注意力机制（Encoder-Decoder Attention）：</strong>计算 <strong>输出序列与输入序列</strong> 的 <strong>相关性</strong></li>
<li><strong>前馈神经网络（Feed-Forward Neural Network）：</strong>对每个词进行独立的非线性变换</li>
</ul>
<p>同样，每个子层后面都接有残差连接和层归一化</p>
<p>在 Transformer 模型出现之前，NLP 领域的主流模型是基于 RNN 的架构，如长短期记忆网络（LSTM）和门控循环单元（GRU）。这些</p>
<p>模型通过顺序处理输入数据来捕捉序列中的依赖关系，但存在以下问题：</p>
<ol>
<li><strong>梯度消失问题</strong>：长距离依赖关系难以捕捉</li>
<li><strong>顺序计算的局限性</strong>：无法充分利用现代硬件的并行计算能力，训练效率低下</li>
</ol>
<h3 id="核心思想"><a href="#核心思想" class="headerlink" title="核心思想"></a>核心思想</h3><h4 id="自注意力（Self-Attention）"><a href="#自注意力（Self-Attention）" class="headerlink" title="自注意力（Self-Attention）"></a>自注意力（Self-Attention）</h4><p>自注意力机制是 Transformer 的核心组件</p>
<p>自注意力机制允许模型在处理序列时，动态地为每个位置分配不同的权重，从而捕捉序列中任意两个位置之间的依赖关系</p>
<ul>
<li><strong>输入表示</strong>：输入序列中的每个词（或标记）通过词嵌入（Embedding）转换为向量表示</li>
<li><strong>注意力权重计算</strong>：通过计算查询（Query）、键（Key）和值（Value）之间的点积，得到每个词与其他词的相关性权重</li>
<li><p><strong>加权求和</strong>：使用注意力权重对值（Value）进行加权求和，得到每个词的上下文表示</p>
</li>
<li><p><strong>Query (Q)</strong>：当前词的“询问”向量 —— “我在找什么？”</p>
</li>
<li><strong>Key (K)</strong>：其他词的“键”向量 —— “我是什么？”</li>
<li><strong>Value (V)</strong>：其他词的“值”向量 —— “我能提供什么信息？”</li>
</ul>
<p>公式如下：</p>
<script type="math/tex; mode=display">
Attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt{d_k}})V</script><p>其中：</p>
<ul>
<li><script type="math/tex">Q</script> 是查询矩阵，<script type="math/tex">K</script> 是键矩阵， <script type="math/tex">V</script> 是值矩阵</li>
<li><script type="math/tex">d_k</script> 是向量的维度，用于缩放点积，防止梯度爆炸</li>
</ul>
<p>下面举个例子：</p>
<p>假设输入序列有 3 个词：<code>[&quot;I&quot;, &quot;love&quot;, &quot;AI&quot;]</code></p>
<p>对 “love” 这个词：</p>
<ul>
<li>它会分别计算与 “I”、“love”、“AI” 的相关性</li>
<li>如果 “love” 和 “I”、“AI” 相关性高，则输出会融合这三者的信息</li>
<li>最终 “love” 的新表示 = α₁·V(“I”) + α₂·V(“love”) + α₃·V(“AI”)</li>
</ul>
<p>其中 α₁, α₂, α₃ 是注意力权重（softmax 后的值）</p>
<p>至于相关性怎么来的，这边简单说明就是通过大量训练数据来提高对应词的权重，权重高的相关性就高（更细节的这里不做说明）</p>
<h4 id="多头注意力（Multi-Head-Attention）"><a href="#多头注意力（Multi-Head-Attention）" class="headerlink" title="多头注意力（Multi-Head Attention）"></a>多头注意力（Multi-Head Attention）</h4><p>为了捕捉更丰富的特征，Transformer 使用多头注意力机制。它将输入分成多个子空间，每个子空间独立计算注意力，最后将结果拼接起来</p>
<ul>
<li><strong>多头注意力的优势</strong>：允许模型关注序列中不同的部分，例如语法结构、语义关系等</li>
<li><strong>并行计算</strong>：多个注意力头可以并行计算，提高效率</li>
</ul>
<h4 id="位置编码（Positional-Encoding）"><a href="#位置编码（Positional-Encoding）" class="headerlink" title="位置编码（Positional Encoding）"></a>位置编码（Positional Encoding）</h4><p>由于 Transformer 没有显式的序列信息（如 RNN 中的时间步），位置编码被用来为输入序列中的每个词添加位置信息。通常使用正弦和余弦函数生成位置编码：</p>
<script type="math/tex; mode=display">
PE_{(pos,2i)}=sin(\frac{pos}{10000^{\frac{2i}{d_{model}}}})</script><script type="math/tex; mode=display">
PE_{(pos,2i+1)}=cos(\frac{pos}{10000^{\frac{2i}{d_{model}}}})</script><p>其中，<script type="math/tex">pos</script> 是词的位置，<script type="math/tex">i</script> 是维度索引</p>
<h4 id="编码器-解码器架构"><a href="#编码器-解码器架构" class="headerlink" title="编码器-解码器架构"></a>编码器-解码器架构</h4><p>和本章一开始的图一样</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">			---------&gt; 前馈神经网络</span><br><span class="line">			|</span><br><span class="line">			|</span><br><span class="line">Encoder -------------&gt; 多头自注意力		----------&gt; 输出序列</span><br><span class="line">			|							|	</span><br><span class="line">			|							|</span><br><span class="line">			|--------&gt; Decoder -------------------&gt; 多头自注意力</span><br><span class="line">										|</span><br><span class="line">										|</span><br><span class="line">										----------&gt; 编码器-解码器注意力</span><br><span class="line">										|</span><br><span class="line">										|</span><br><span class="line">										----------&gt; 前馈神经网络</span><br></pre></td></tr></table></figure>
<h3 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h3><ol>
<li><strong>自然语言处理（NLP）</strong>：<ul>
<li>机器翻译（如 Google Translate）</li>
<li>文本生成（如 GPT 系列模型）</li>
<li>文本分类、问答系统等。</li>
</ul>
</li>
<li><strong>计算机视觉（CV）</strong>：<ul>
<li>图像分类（如 Vision Transformer）</li>
<li>目标检测、图像生成等。</li>
</ul>
</li>
<li><strong>多模态任务</strong>：<ul>
<li>结合文本和图像的任务（如 CLIP、DALL-E）</li>
</ul>
</li>
</ol>
<p><strong>实例</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TransformerModel</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_dim, model_dim, num_heads, num_layers, output_dim</span>):</span><br><span class="line">        <span class="built_in">super</span>(TransformerModel, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.embedding = nn.Embedding(input_dim, model_dim)</span><br><span class="line">        <span class="variable language_">self</span>.positional_encoding = nn.Parameter(torch.zeros(<span class="number">1</span>, <span class="number">1000</span>, model_dim))  <span class="comment"># 假设序列长度最大为1000</span></span><br><span class="line">        <span class="variable language_">self</span>.transformer = nn.Transformer(d_model=model_dim, nhead=num_heads, num_encoder_layers=num_layers)</span><br><span class="line">        <span class="variable language_">self</span>.fc = nn.Linear(model_dim, output_dim)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, src, tgt</span>):</span><br><span class="line">        src_seq_length, tgt_seq_length = src.size(<span class="number">1</span>), tgt.size(<span class="number">1</span>)</span><br><span class="line">        src = <span class="variable language_">self</span>.embedding(src) + <span class="variable language_">self</span>.positional_encoding[:, :src_seq_length, :]</span><br><span class="line">        tgt = <span class="variable language_">self</span>.embedding(tgt) + <span class="variable language_">self</span>.positional_encoding[:, :tgt_seq_length, :]</span><br><span class="line">        transformer_output = <span class="variable language_">self</span>.transformer(src, tgt)</span><br><span class="line">        output = <span class="variable language_">self</span>.fc(transformer_output)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"><span class="comment"># 超参数</span></span><br><span class="line">input_dim = <span class="number">10000</span>  <span class="comment"># 词汇表大小</span></span><br><span class="line">model_dim = <span class="number">512</span>    <span class="comment"># 模型维度</span></span><br><span class="line">num_heads = <span class="number">8</span>      <span class="comment"># 多头注意力头数</span></span><br><span class="line">num_layers = <span class="number">6</span>     <span class="comment"># 编码器和解码器层数</span></span><br><span class="line">output_dim = <span class="number">10000</span> <span class="comment"># 输出维度（通常与词汇表大小相同）</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化模型、损失函数和优化器</span></span><br><span class="line">model = TransformerModel(input_dim, model_dim, num_heads, num_layers, output_dim)</span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = optim.Adam(model.parameters(), lr=<span class="number">0.001</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 假设输入数据</span></span><br><span class="line">src = torch.randint(<span class="number">0</span>, input_dim, (<span class="number">10</span>, <span class="number">32</span>))  <span class="comment"># (序列长度, 批量大小)</span></span><br><span class="line">tgt = torch.randint(<span class="number">0</span>, input_dim, (<span class="number">20</span>, <span class="number">32</span>))  <span class="comment"># (序列长度, 批量大小)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 前向传播</span></span><br><span class="line">output = model(src, tgt)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算损失</span></span><br><span class="line">loss = criterion(output.view(-<span class="number">1</span>, output_dim), tgt.view(-<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 反向传播和优化</span></span><br><span class="line">optimizer.zero_grad()</span><br><span class="line">loss.backward()</span><br><span class="line">optimizer.step()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Loss:&quot;</span>, loss.item())</span><br></pre></td></tr></table></figure>
<p>其中：</p>
<ul>
<li><code>nn.Embedding</code>：将离散的 <strong>词索引</strong>（如 0, 1, 2, …）转换为 <strong>连续的向量表示</strong>（词嵌入）</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 假设 input_dim=1000, model_dim=512</span></span><br><span class="line">embedding = nn.Embedding(<span class="number">1000</span>, <span class="number">512</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输入一个词索引 [2, 5, 8]（代表 &quot;I&quot;, &quot;love&quot;, &quot;AI&quot;）</span></span><br><span class="line">word_ids = torch.tensor([[<span class="number">2</span>, <span class="number">5</span>, <span class="number">8</span>]])  <span class="comment"># (batch_size=1, seq_len=3)</span></span><br><span class="line">embedded = embedding(word_ids)       <span class="comment"># 输出 (1, 3, 512) 的向量</span></span><br></pre></td></tr></table></figure>
<ul>
<li><code>nn.Parameter</code>：将一个普通张量标记为模型的 <strong>可训练参数</strong>，使其在 <code>model.parameters()</code> 中被自动识别并更新</li>
</ul>
<h2 id="Pytorch-Transformer"><a href="#Pytorch-Transformer" class="headerlink" title="Pytorch Transformer"></a>Pytorch Transformer</h2><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/338817680">(12 封私信 / 80 条消息) Transformer模型详解（图解最完整版） - 知乎</a></p>
<p><a target="_blank" rel="noopener" href="https://neverbiasu.github.io/zh/posts/papers/transformer.html">【论文精读】Transformer：Attention Is All You Need | Nlog</a></p>
<p>Transformer 是现代机器学习中最强大的模型之一</p>
<p>Transformer 模型是一种基于自注意力机制（Self-Attention） 的深度学习架构，它彻底改变了自然语言处理（NLP）领域，并成为现代</p>
<p>深度学习模型（如 BERT、GPT 等）的基础</p>
<p>Transformer 是现代 NLP 领域的核心架构，凭借其强大的长距离依赖建模能力和高效的并行计算优势，在语言翻译和文本摘要等任务中</p>
<p>超越了传统的 长短期记忆 (LSTM) 网络</p>
<h3 id="定义多头注意力"><a href="#定义多头注意力" class="headerlink" title="定义多头注意力"></a>定义多头注意力</h3><p><img src="/img/lazy.gif" data-original="13.png" alt=""></p>
<p>MultiHeadAttention 类封装了 Transformer 模型中常用的多头注意力机制，负责将输入拆分成多个注意力头，对每个注意力头施加注意</p>
<p>力，然后将结果组合起来，这样模型就可以在不同尺度上捕捉输入数据中的各种关系，提高模型的表达能力</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 自定义Transfromer模块</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MutiHeadAttention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, num_heads</span>):</span><br><span class="line">        <span class="built_in">super</span>(MutiHeadAttention, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="keyword">assert</span> d_model % num_heads == <span class="number">0</span>, <span class="string">&quot;d_model必须能被num_heads整除&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.d_model = d_model            <span class="comment"># 模型维度</span></span><br><span class="line">        <span class="variable language_">self</span>.num_heads = num_heads        <span class="comment"># 注意力头数</span></span><br><span class="line">        <span class="variable language_">self</span>.d_k = d_model // num_heads   <span class="comment"># 每个头的维度</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 定义线性变换层（无需偏置）</span></span><br><span class="line">        <span class="variable language_">self</span>.W_q = nn.Linear(d_model, d_model) <span class="comment"># 查询变换</span></span><br><span class="line">        <span class="variable language_">self</span>.W_k = nn.Linear(d_model, d_model) <span class="comment"># 键变换</span></span><br><span class="line">        <span class="variable language_">self</span>.W_v = nn.Linear(d_model, d_model) <span class="comment"># 值变换</span></span><br><span class="line">        <span class="variable language_">self</span>.W_o = nn.Linear(d_model, d_model) <span class="comment"># 输出变换</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">scale_dot_product_attention</span>(<span class="params">self, Q, K, V, mask=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        计算缩放点积注意力</span></span><br><span class="line"><span class="string">        输入形状：</span></span><br><span class="line"><span class="string">            Q: (batch_size, num_heads, seq_length, d_k)</span></span><br><span class="line"><span class="string">            K, V: 同Q</span></span><br><span class="line"><span class="string">        输出形状： (batch_size, num_heads, seq_length, d_k)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 计算注意力分散（Q与K的点积）</span></span><br><span class="line">        <span class="comment"># K.transpose(-2, 1)代表矩阵的转置</span></span><br><span class="line">        <span class="comment">#原始K: (batch_size, num_heads, seq_length, d_k)</span></span><br><span class="line">        <span class="comment">#转置后: (batch_size, num_heads, d_k, seq_length)</span></span><br><span class="line">        attn_scores = torch.matmul(Q, K.transpose(-<span class="number">2</span>, -<span class="number">1</span>)) / math.sqrt(<span class="variable language_">self</span>.d_k)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 应用掩码</span></span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            attn_scores = attn_scores.masked_fill(mask==<span class="number">0</span>, -<span class="number">1e9</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算注意力权重（softmax归一化）</span></span><br><span class="line">        attn_probs = torch.softmax(attn_scores, dim=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 对值向量加权求和</span></span><br><span class="line">        output = torch.matmul(attn_probs, V)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">split_heads</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        将输入张量分割为多个头</span></span><br><span class="line"><span class="string">        输入形状: (batch_size, seq_length, d_model)</span></span><br><span class="line"><span class="string">        输出形状: (batch_size, num_heads, seq_length, d_k)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        batch_size, seq_length, d_model = x.size()</span><br><span class="line">        <span class="keyword">return</span> x.view(batch_size, seq_length, <span class="variable language_">self</span>.num_heads, <span class="variable language_">self</span>.d_k).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">combine_heads</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        将多个头的输出合并回原始形状</span></span><br><span class="line"><span class="string">        输入形状: (batch_size, num_heads, seq_length, d_k)</span></span><br><span class="line"><span class="string">        输出形状: (batch_size, seq_length, d_model)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        batch_size, _, seq_length, d_k = x.size()</span><br><span class="line">        <span class="keyword">return</span> x.transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous().view(batch_size, seq_length, <span class="variable language_">self</span>.d_model)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, Q, K, V, mask=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        前向传播</span></span><br><span class="line"><span class="string">        输入形状: Q/K/V: (batch_size, seq_length, d_model)</span></span><br><span class="line"><span class="string">        输出形状: (batch_size, seq_length, d_model)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 线性变换并分割多头</span></span><br><span class="line">        Q = <span class="variable language_">self</span>.split_heads(<span class="variable language_">self</span>.W_q(Q)) <span class="comment"># (batch, heads, seq_len, d_k)</span></span><br><span class="line">        K = <span class="variable language_">self</span>.split_heads(<span class="variable language_">self</span>.W_k(K))</span><br><span class="line">        V = <span class="variable language_">self</span>.split_heads(<span class="variable language_">self</span>.W_v(V))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算注意力</span></span><br><span class="line">        attn_output = <span class="variable language_">self</span>.scale_dot_product_attention(Q, K, V, mask)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 合并多头并输出变换</span></span><br><span class="line">        output = <span class="variable language_">self</span>.W_o(<span class="variable language_">self</span>.combine_heads(attn_output))</span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>
<p>其中：</p>
<ul>
<li><strong>什么是掩码？</strong> 掩码是一个与注意力分数形状相同的二进制张量（通常包含0和1），用于指定哪些位置的注意力应该被忽略或保留，在自回归模型（如Transformer decoder）中，当前位置只能关注到它前面的token，而不会看到未来的信息（掩码会屏蔽掉未来位置）（masked_fill）</li>
</ul>
<p>假设我们有一个序列 <code>[A, B, C, &lt;pad&gt;, &lt;pad&gt;]</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">原始注意力分数矩阵：</span><br><span class="line">[[ <span class="number">1.2</span>,  <span class="number">0.8</span>, -<span class="number">0.3</span>,  <span class="number">0.1</span>,  <span class="number">0.2</span>],</span><br><span class="line"> [ <span class="number">0.5</span>,  <span class="number">1.5</span>,  <span class="number">0.2</span>, -<span class="number">0.1</span>,  <span class="number">0.3</span>],</span><br><span class="line"> [-<span class="number">0.2</span>,  <span class="number">0.4</span>,  <span class="number">1.8</span>,  <span class="number">0.5</span>, -<span class="number">0.2</span>],</span><br><span class="line"> [ <span class="number">0.3</span>, -<span class="number">0.1</span>,  <span class="number">0.6</span>,  <span class="number">0.9</span>,  <span class="number">0.4</span>],</span><br><span class="line"> [-<span class="number">0.1</span>,  <span class="number">0.2</span>, -<span class="number">0.3</span>,  <span class="number">0.7</span>,  <span class="number">1.1</span>]]</span><br><span class="line"></span><br><span class="line">掩码（<span class="number">1</span>=有效，<span class="number">0</span>=填充）：</span><br><span class="line">[[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line"> [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line"> [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line"> [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line"> [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>]]</span><br><span class="line"></span><br><span class="line">应用掩码后：</span><br><span class="line">[[ <span class="number">1.2</span>,  <span class="number">0.8</span>, -<span class="number">0.3</span>, -<span class="number">1e9</span>, -<span class="number">1e9</span>],</span><br><span class="line"> [ <span class="number">0.5</span>,  <span class="number">1.5</span>,  <span class="number">0.2</span>, -<span class="number">1e9</span>, -<span class="number">1e9</span>],</span><br><span class="line"> [-<span class="number">0.2</span>,  <span class="number">0.4</span>,  <span class="number">1.8</span>, -<span class="number">1e9</span>, -<span class="number">1e9</span>],</span><br><span class="line"> [ <span class="number">0.3</span>, -<span class="number">0.1</span>,  <span class="number">0.6</span>, -<span class="number">1e9</span>, -<span class="number">1e9</span>],</span><br><span class="line"> [-<span class="number">0.1</span>,  <span class="number">0.2</span>, -<span class="number">0.3</span>, -<span class="number">1e9</span>, -<span class="number">1e9</span>]]</span><br></pre></td></tr></table></figure>
<p>应用softmax后，填充位置的注意力权重会变成≈0，确保模型不会关注到无效位置</p>
<ul>
<li><strong>为什么需要分割后又合并？</strong> 因为这是多头注意力的核心思想。多头注意力的核心思想是：<strong>将输入的特征空间分割成多个子空间，每个子空间独立计算注意力，最后合并结果</strong>，详情可以看以下步骤，然后就可以倒推combine的步骤了</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">split_heads</span>(<span class="params">self, x</span>):</span><br><span class="line">	batch_size, seq_length, d_model = x.size()</span><br><span class="line">	<span class="keyword">return</span> x.view(batch_size, seq_length, <span class="variable language_">self</span>.num_heads, <span class="variable language_">self</span>.d_k).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<p><strong>输入</strong>: <code>(batch_size, seq_length, d_model)</code></p>
<p>例如: <code>(32, 10, 512)</code> - 32个样本，序列长度10，特征维度512</p>
<p><strong>view重塑</strong>: <code>(batch_size, seq_length, num_heads, d_k)</code></p>
<p>假设 <code>num_heads=8</code>, <code>d_k=64</code> (因为 8×64=512)</p>
<p>形状变为: <code>(32, 10, 8, 64)</code></p>
<p><strong>transpose(1, 2)</strong>: 交换第1和第2维度</p>
<p>形状变为: <code>(32, 8, 10, 64)</code></p>
<p><strong>意义</strong>: 将8个头分离出来，每个头独立处理64维特征</p>
<ul>
<li><strong>为什么需要 <code>contiguous</code>？但是为什么分割头的时候不需要进行 <code>contiguous</code>？</strong>当我们对张量进行某些操作时，会 <strong>改变其逻辑视图但不改变底层内存布局</strong>，导致张量变成 <strong>非连续</strong> 状态。比如说，转置操作只改变了维度的顺序，但底层内存存储方式没变，变成了 <strong>非连续</strong> 状态，所以这个时候我们需要进行 <code>contiguous</code> 操作，然后这里我们有一个前提就是 <code>view</code> 操作要求内存一定要是连续的，而装置对于连不连续没有啥要求，所以合并头的时候需要进行 <code>contiguous</code> 操作，而分割的时候不需要</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 错误示例</span></span><br><span class="line">x = torch.randn(<span class="number">32</span>, <span class="number">8</span>, <span class="number">10</span>, <span class="number">64</span>)</span><br><span class="line">x_transposed = x.transpose(<span class="number">1</span>, <span class="number">2</span>)  <span class="comment"># 非连续</span></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    x_view = x_transposed.view(<span class="number">32</span>, <span class="number">10</span>, <span class="number">512</span>)  <span class="comment"># 会报错！</span></span><br><span class="line"><span class="keyword">except</span> RuntimeError <span class="keyword">as</span> e:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Error: <span class="subst">&#123;e&#125;</span>&quot;</span>)</span><br><span class="line"><span class="comment"># 错误: view size is not compatible with input tensor&#x27;s size and stride</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#try:</span></span><br><span class="line"><span class="comment">#    x_view = x.contiguous().view(32, 10, 512)</span></span><br><span class="line"><span class="comment">#    print(&quot;success&quot;)</span></span><br><span class="line"><span class="comment">#except Exception as e:</span></span><br><span class="line"><span class="comment">#    print(f&quot;Error: &#123;e&#125;&quot;)</span></span><br><span class="line"><span class="comment">#success</span></span><br></pre></td></tr></table></figure>
<h3 id="定义前馈网络"><a href="#定义前馈网络" class="headerlink" title="定义前馈网络"></a>定义前馈网络</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 前馈神经网络</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">PositionWiseFeedForward</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, d_ff</span>):</span><br><span class="line">        <span class="built_in">super</span>(PositionWiseFeedForward, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.fc1 = nn.Linear(d_model, d_ff)   <span class="comment"># 第一层全连接</span></span><br><span class="line">        <span class="variable language_">self</span>.fc2 = nn.Linear(d_ff, d_model)   <span class="comment"># 第二层全连接</span></span><br><span class="line">        <span class="variable language_">self</span>.relu = nn.ReLU()                 <span class="comment"># 激活函数 </span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># 前馈网络的计算</span></span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.fc2(<span class="variable language_">self</span>.relu(<span class="variable language_">self</span>.fc1(x)))</span><br></pre></td></tr></table></figure>
<h3 id="位置编码"><a href="#位置编码" class="headerlink" title="位置编码"></a>位置编码</h3><p>位置编码用于注入输入序列中每个 token 的位置信息</p>
<p>使用不同频率的正弦和余弦函数来生成位置编码</p>
<p>（具体公式可以见上一章）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 位置编码</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">PositionalEncoding</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, max_seq_length</span>):</span><br><span class="line">        <span class="built_in">super</span>(PositionalEncoding, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        pe = torch.zeros(max_seq_length, d_model) <span class="comment"># 初始化位置编码矩阵</span></span><br><span class="line">        position = torch.arange(<span class="number">0</span>, max_seq_length, dtype=torch.<span class="built_in">float</span>).unsqueeze(<span class="number">1</span>)</span><br><span class="line">        div_term = torch.exp(torch.arange(<span class="number">0</span>, d_model, <span class="number">2</span>).<span class="built_in">float</span>() * -math.log(<span class="number">10000.0</span>) / d_model) </span><br><span class="line">        <span class="comment"># 10000^(2i/d_model)</span></span><br><span class="line">        pe[:, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(position * div_term)  <span class="comment"># 偶数位置使用正弦函数</span></span><br><span class="line">        pe[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(position * div_term)  <span class="comment"># 奇数位置使用余弦函数</span></span><br><span class="line">        <span class="variable language_">self</span>.register_buffer(<span class="string">&#x27;pe&#x27;</span>, pe.unsqueeze(<span class="number">0</span>))   <span class="comment"># 注册为缓冲区</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> x + <span class="variable language_">self</span>.pe[:, x.size(<span class="number">1</span>)]  <span class="comment"># 将位置编码添加到输入中  </span></span><br></pre></td></tr></table></figure>
<p>其中：</p>
<ul>
<li><code>torch.arange()</code> - 创建等差数列，<code>torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)</code>，创建一个从0开始，步长为1，最大长度为 <code>max_seq_length</code> 的等差数列，并且将形状变成 <code>(max_seq_length, 1)</code></li>
<li><strong>为什么需要转换成列向量？</strong>位置编码需要为序列中的每个位置（0,1,2,…）计算不同的编码值，转换成列向量是为了后续的矩阵乘法运算</li>
<li><strong>为什么使用exp？</strong><code>exp(x * log(y)) = y^x</code>，这是数学上的等价转换，避免直接计算大数的幂次方（数值稳定性），然后这个 <code>-</code> 就是单纯的负号，用于表示为分母</li>
<li><code>register_buffer()</code> - 注册缓冲区，位置编码是<strong>预定义的、固定的</strong>编码模式，不应该通过学习来改变，需要随模型一起保存，以便推理时使用</li>
</ul>
<h3 id="构建编码器"><a href="#构建编码器" class="headerlink" title="构建编码器"></a>构建编码器</h3><p><img src="/img/lazy.gif" data-original="14.png" alt=""></p>
<p><strong>编码器层：</strong>包含一个自注意力机制和一个前馈网络，每个子层后接残差连接和层归一化</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 编码器层</span></span><br><span class="line"><span class="comment"># 编码器层：包含一个自注意力机制和一个前馈网络，每个子层后接残差连接和层归一化</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">EncoderLayer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, num_heads, d_ff, dropout</span>):</span><br><span class="line">        <span class="built_in">super</span>(EncoderLayer, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.self_atten = MutiHeadAttention(d_model, num_heads)</span><br><span class="line">        <span class="variable language_">self</span>.feed_forward = PositionWiseFeedForward(d_model, d_ff)</span><br><span class="line">        <span class="variable language_">self</span>.norm1 = nn.LayerNorm(d_model)  <span class="comment"># 层归一化</span></span><br><span class="line">        <span class="variable language_">self</span>.norm2 = nn.LayerNorm(d_model)</span><br><span class="line">        <span class="variable language_">self</span>.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, mask</span>):</span><br><span class="line">        <span class="comment"># 自注意力机制</span></span><br><span class="line">        atten_output = <span class="variable language_">self</span>.self_atten(x, x, x, mask)</span><br><span class="line">        x = <span class="variable language_">self</span>.norm1(x + <span class="variable language_">self</span>.dropout(atten_output))  <span class="comment"># 残差链接和层归一化</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 前馈网络</span></span><br><span class="line">        ff_output = <span class="variable language_">self</span>.feed_forward(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.norm2(x + <span class="variable language_">self</span>.dropout(ff_output))  <span class="comment"># 残差链接和层归一化</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p>其中：</p>
<ul>
<li><strong>Dropout</strong> 是一种在训练过程中 <strong>随机丢弃</strong> 神经网络中一部分神经元的技术，<strong>训练时</strong>：随机”关闭”一部分神经元（设为0），强制网络不依赖于任何单个神经元，<strong>推理时</strong>：所有神经元都参与计算，但需要对输出进行缩放</li>
<li><strong>为什么需要进行Dropout？</strong>防止过拟合，Dropout通过随机丢弃神经元，强制模型学习更鲁棒的特征</li>
</ul>
<h3 id="构建解码器"><a href="#构建解码器" class="headerlink" title="构建解码器"></a>构建解码器</h3><p><img src="/img/lazy.gif" data-original="15.png" alt=""></p>
<p><strong>解码器层：</strong>包含一个自注意力机制、一个交叉注意力机制和一个前馈网络，每个子层后接残差连接和层归一化</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 解码器层</span></span><br><span class="line"><span class="comment"># 解码器层：包含一个自注意力机制、一个交叉注意力机制和一个前馈网络，每个子层后接残差连接和层归一化</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DecoderLayer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, num_heads, d_ff, dropout</span>):</span><br><span class="line">        <span class="built_in">super</span>(DecoderLayer, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.self_atten = MutiHeadAttention(d_model, num_heads)</span><br><span class="line">        <span class="variable language_">self</span>.cross_atten = MutiHeadAttention(d_model, num_heads)</span><br><span class="line">        <span class="variable language_">self</span>.feed_forward = PositionWiseFeedForward(d_model, d_ff)</span><br><span class="line">        <span class="variable language_">self</span>.norm1 = nn.LayerNorm(d_model)</span><br><span class="line">        <span class="variable language_">self</span>.norm2 = nn.LayerNorm(d_model)</span><br><span class="line">        <span class="variable language_">self</span>.norm3 = nn.LayerNorm(d_model)</span><br><span class="line">        <span class="variable language_">self</span>.dropout = nn.Dropout(dropout)  <span class="comment"># 丢掉</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, enc_output, src_mask, tgt_mask</span>):</span><br><span class="line">        <span class="comment"># 自注意力</span></span><br><span class="line">        atten_output = <span class="variable language_">self</span>.self_atten(x, x, x, tgt_mask)</span><br><span class="line">        x = <span class="variable language_">self</span>.norm1(x + <span class="variable language_">self</span>.dropout(atten_output))  <span class="comment"># 残差链接和层归一化</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 交叉注意力机制</span></span><br><span class="line">        atten_output = <span class="variable language_">self</span>.cross_atten(x, enc_output, enc_output, src_mask)</span><br><span class="line">        x = <span class="variable language_">self</span>.norm2(x + <span class="variable language_">self</span>.dropout(atten_output))  <span class="comment"># 残差链接和层归一化</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 前馈网络</span></span><br><span class="line">        ff_output = <span class="variable language_">self</span>.feed_forward(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.norm3(x + <span class="variable language_">self</span>.dropout(ff_output))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p>其中：</p>
<ul>
<li><code>enc_output</code> - 编码器（Encoder）处理完整输入序列后的输出表示 <code>(batch_size, src_seq_length, d_model)</code></li>
<li><code>src_mask</code> - 用于屏蔽源序列（输入序列）中填充（padding）位置的掩码，形状通常为 <code>(batch_size, 1, src_seq_length)</code> 或 <code>(batch_size, src_seq_length)</code></li>
<li><code>tgt_mask</code> - 用于屏蔽目标序列（输出序列）中<strong>未来位置</strong>和<strong>填充位置</strong>的掩码，形状通常为 <code>(batch_size, tgt_seq_length, tgt_seq_length)</code></li>
</ul>
<p><strong>示例：</strong></p>
<ul>
<li>源序列：<code>[&quot;I&quot;, &quot;love&quot;, &quot;you&quot;, &quot;&lt;pad&gt;&quot;, &quot;&lt;pad&gt;&quot;]</code></li>
<li>目标序列：<code>[&quot;&lt;sos&gt;&quot;, &quot;我&quot;, &quot;爱&quot;, &quot;你&quot;, &quot;&lt;eos&gt;&quot;]</code></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 参数对应关系</span></span><br><span class="line"></span><br><span class="line">enc_output = 编码器对 [<span class="string">&quot;I&quot;</span>, <span class="string">&quot;love&quot;</span>, <span class="string">&quot;you&quot;</span>, <span class="string">&quot;&lt;pad&gt;&quot;</span>, <span class="string">&quot;&lt;pad&gt;&quot;</span>] 的编码</span><br><span class="line"></span><br><span class="line">src_mask = [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>]  <span class="comment"># 屏蔽两个padding</span></span><br><span class="line"></span><br><span class="line">tgt_mask = 因果掩码(不能看到未来) + 填充掩码(不能看到padding)  <span class="comment"># 确保不能看未来，且屏蔽padding</span></span><br></pre></td></tr></table></figure>
<ul>
<li><strong>自注意力</strong>：生成”爱”时，只能看到<code>[&quot;&lt;sos&gt;&quot;, &quot;我&quot;]</code></li>
<li><strong>交叉注意力</strong>：生成”爱”时，可以关注源序列中相关的词（如”love”）</li>
</ul>
<h3 id="构建Transformer"><a href="#构建Transformer" class="headerlink" title="构建Transformer"></a>构建Transformer</h3><p><img src="/img/lazy.gif" data-original="16.png" alt=""></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Transformer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout</span>):</span><br><span class="line">        <span class="built_in">super</span>(Transformer, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.encoder_embedding = nn.Embedding(src_vocab_size, d_model)         <span class="comment"># 编码器嵌入</span></span><br><span class="line">        <span class="variable language_">self</span>.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)         <span class="comment"># 解码器嵌入</span></span><br><span class="line">        <span class="variable language_">self</span>.positional_encoding = PositionalEncoding(d_model, max_seq_length) <span class="comment"># 位置编码</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 编码器和解码器层</span></span><br><span class="line">        <span class="variable language_">self</span>.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_layers)])</span><br><span class="line">        <span class="variable language_">self</span>.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_layers)])</span><br><span class="line">        </span><br><span class="line">        <span class="variable language_">self</span>.fc = nn.Linear(d_model, tgt_vocab_size) <span class="comment"># 最终的全连接层（最终输出阶段）</span></span><br><span class="line">        <span class="variable language_">self</span>.dropout = nn.Dropout(dropout)  <span class="comment"># 丢弃</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">generate_mask</span>(<span class="params">self, src, tgt</span>):</span><br><span class="line">        <span class="comment"># 源代码：屏蔽填充符（假设填充符索引为0）</span></span><br><span class="line">        <span class="comment"># 形状：（batch_size，1，1，seq_length）</span></span><br><span class="line">        src_mask = (src != <span class="number">0</span>).unsqueeze(<span class="number">1</span>).unsqueeze(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 目标掩码：屏蔽填充符和未来信息</span></span><br><span class="line">        <span class="comment"># 形状：（batch_size，1，seq_length，1）</span></span><br><span class="line">        tgt_mask = (tgt != <span class="number">0</span>).unsqueeze(<span class="number">1</span>).unsqueeze(<span class="number">2</span>)</span><br><span class="line">        seq_length = tgt.size(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 生成上三角矩阵掩码，防止解码时看到未来信息</span></span><br><span class="line">        nopeak_mask = (<span class="number">1</span> - torch.triu(torch.ones(<span class="number">1</span>, seq_length, seq_length), diagonal=<span class="number">1</span>)).<span class="built_in">bool</span>()</span><br><span class="line">        tgt_mask = tgt_mask &amp; nopeak_mask <span class="comment"># 合并填充掩码和未来信息掩码</span></span><br><span class="line">        <span class="keyword">return</span> src_mask, tgt_mask</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, src, tgt</span>):</span><br><span class="line">        <span class="comment"># 生成掩码</span></span><br><span class="line">        src_mask, tgt_mask = <span class="variable language_">self</span>.generate_mask(src, tgt)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 编码器部分</span></span><br><span class="line">        src_embedded = <span class="variable language_">self</span>.dropout(<span class="variable language_">self</span>.positional_encoding(<span class="variable language_">self</span>.decoder_embedding(src)))</span><br><span class="line">        enc_output = src_embedded</span><br><span class="line">        <span class="keyword">for</span> enc_layer <span class="keyword">in</span> <span class="variable language_">self</span>.encoder_layers:</span><br><span class="line">            enc_output = enc_layer(enc_output, src_mask)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 解码器部分</span></span><br><span class="line">        tgt_embedded = <span class="variable language_">self</span>.dropout(<span class="variable language_">self</span>.positional_encoding(<span class="variable language_">self</span>.decoder_embedding(tgt)))</span><br><span class="line">        dec_output = tgt_embedded</span><br><span class="line">        <span class="keyword">for</span> dec_layer <span class="keyword">in</span> <span class="variable language_">self</span>.decoder_layers:</span><br><span class="line">            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 最终输出</span></span><br><span class="line">        output = <span class="variable language_">self</span>.fc(dec_output)</span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>
<p>其中：</p>
<ul>
<li>关注到在编解码器层里有一个循环操作：<code>[EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)]</code></li>
</ul>
<p><strong>循环做了什么？</strong></p>
<ul>
<li><code>for _ in range(num_layers)</code>：循环 <code>num_layers</code> 次（比如代码中 <code>num_layers = 6</code>）</li>
<li>每次循环都会 <strong>创建一个新的 <code>EncoderLayer</code> 实例</strong></li>
<li>最终生成一个包含  <strong>6 个独立 <code>EncoderLayer</code> 对象 </strong>的列表</li>
</ul>
<p><strong>为什么需要 <code>nn.ModuleList</code>？</strong></p>
<ul>
<li><strong>管理子模块</strong>：<code>nn.ModuleList</code> 是 PyTorch 的特殊容器，它会自动将里面的所有层注册为模型的子模块</li>
<li><strong>梯度追踪</strong>：确保所有层的参数都能被优化器识别和更新</li>
<li><strong>方便遍历</strong>：在 <code>forward</code> 中可以用 <code>for enc_layer in self.encoder_layers:</code> 依次处理</li>
</ul>
<p>这行代码相当于：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 当 num_layers = 6 时，等价于：</span></span><br><span class="line"><span class="variable language_">self</span>.encoder_layers = nn.ModuleList([</span><br><span class="line">    EncoderLayer(d_model, num_heads, d_ff, dropout),  <span class="comment"># 第1层</span></span><br><span class="line">    EncoderLayer(d_model, num_heads, d_ff, dropout),  <span class="comment"># 第2层</span></span><br><span class="line">    EncoderLayer(d_model, num_heads, d_ff, dropout),  <span class="comment"># 第3层</span></span><br><span class="line">    EncoderLayer(d_model, num_heads, d_ff, dropout),  <span class="comment"># 第4层</span></span><br><span class="line">    EncoderLayer(d_model, num_heads, d_ff, dropout),  <span class="comment"># 第5层</span></span><br><span class="line">    EncoderLayer(d_model, num_heads, d_ff, dropout)   <span class="comment"># 第6层</span></span><br><span class="line">])</span><br></pre></td></tr></table></figure>
<p><strong>为什么需要多层？</strong></p>
<p>Transformer 的核心思想就是 <strong>深层堆叠（Deep Stacking）</strong>：</p>
<ul>
<li>数据会依次经过 6 个编码器层，每层都会提取不同层次的特征</li>
<li>第 1 层可能学习基础语法，第 6 层可能学习复杂语义</li>
</ul>
<p>注意到 <code>__init__</code> 里有个最终的全连接层，他的作用域如下图所示：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">输入 → 嵌入 → 位置编码 → 编码器层×6 → 解码器层×6 → 全连接层 → 输出</span><br><span class="line">                                                        ↑</span><br><span class="line">                                                  这里就是self.fc</span><br></pre></td></tr></table></figure>
<ul>
<li>注意到 <code>generate_mask</code> 这里有个 <code>src_mask = (src != 0).unsqueeze(1).unsqueeze(2)</code>，这里 <code>(src != 0)</code> 是 <strong>判断每个位置是否为填充符（Padding Token）</strong>，用于创建 <strong>填充掩码（Padding Mask）</strong>，在NLP任务中，不同句子长度不同，需要用 <strong>填充符</strong> 补到相同长度，这里的  <strong>增加维度</strong> 是为了 <strong>对齐注意力头</strong></li>
<li><code>src != 0</code> 会生成一个<strong>布尔掩码</strong>：<ul>
<li><code>True</code>：该位置是<strong>真实词</strong>（需要关注）</li>
<li><code>False</code>：该位置是<strong>填充符</strong>（需要忽略）</li>
</ul>
</li>
<li><strong>上三角掩码：</strong>防止模型在生成当前词时”偷看”未来的词，对于上面代码中的 <code>nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool()</code> 解释如下：(<code>diagonal=1</code> 是为了让 <strong>第一步先排除主对角线</strong> )</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1. 创建全1矩阵</span></span><br><span class="line">[[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],</span><br><span class="line"> [<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],</span><br><span class="line"> [<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],</span><br><span class="line"> [<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],</span><br><span class="line"> [<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>]]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. triu(diagonal=1) 保留对角线右上三角（从对角线上方第1条开始）</span></span><br><span class="line">[[<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],  <span class="comment"># 第0列置0</span></span><br><span class="line"> [<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],  <span class="comment"># 第0-1列置0</span></span><br><span class="line"> [<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>],  <span class="comment"># 第0-2列置0</span></span><br><span class="line"> [<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>],  <span class="comment"># 第0-3列置0</span></span><br><span class="line"> [<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>]]  <span class="comment"># 全部置0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. 1 - ... 取反，得到下三角+对角线 (这里对应 1 - torch.triu的部分)</span></span><br><span class="line">[[<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>],</span><br><span class="line"> [<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>],</span><br><span class="line"> [<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>],</span><br><span class="line"> [<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>],</span><br><span class="line"> [<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>]]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. .bool() 转为布尔值</span></span><br><span class="line">[[<span class="literal">True</span>, <span class="literal">False</span>, <span class="literal">False</span>, <span class="literal">False</span>, <span class="literal">False</span>],</span><br><span class="line"> [<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">False</span>, <span class="literal">False</span>, <span class="literal">False</span>],</span><br><span class="line"> [<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">False</span>, <span class="literal">False</span>],</span><br><span class="line"> [<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">False</span>],</span><br><span class="line"> [<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>]]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">          列：要关注的词位置</span><br><span class="line">          <span class="number">0</span>   <span class="number">1</span>   <span class="number">2</span>   <span class="number">3</span>   <span class="number">4</span></span><br><span class="line">行：当前词  ┌───────────────────┐</span><br><span class="line">   <span class="number">0</span>      │ <span class="number">1</span>   <span class="number">0</span>   <span class="number">0</span>   <span class="number">0</span>   <span class="number">0</span> │  ← 第<span class="number">0</span>个词只能关注自己</span><br><span class="line">   <span class="number">1</span>      │ <span class="number">1</span>   <span class="number">1</span>   <span class="number">0</span>   <span class="number">0</span>   <span class="number">0</span> │  ← 第<span class="number">1</span>个词能关注第<span class="number">0</span>、<span class="number">1</span>个词</span><br><span class="line">   <span class="number">2</span>      │ <span class="number">1</span>   <span class="number">1</span>   <span class="number">1</span>   <span class="number">0</span>   <span class="number">0</span> │  ← 第<span class="number">2</span>个词能关注第<span class="number">0</span>、<span class="number">1</span>、<span class="number">2</span>个词</span><br><span class="line">   <span class="number">3</span>      │ <span class="number">1</span>   <span class="number">1</span>   <span class="number">1</span>   <span class="number">1</span>   <span class="number">0</span> │  ← 第<span class="number">3</span>个词能关注第<span class="number">0</span>、<span class="number">1</span>、<span class="number">2</span>、<span class="number">3</span>个词</span><br><span class="line">   <span class="number">4</span>      │ <span class="number">1</span>   <span class="number">1</span>   <span class="number">1</span>   <span class="number">1</span>   <span class="number">1</span> │  ← 第<span class="number">4</span>个词能关注所有前面的词</span><br><span class="line">          └───────────────────┘</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>PS：</strong>上三角掩码更清晰易懂，更清晰地表达了不让看未来信息的意图，属于一种习惯，完全可以直接 <code>torch.tril(torch.ones(5, 5), diagonal=0)</code>（直接创造下三角）</li>
</ul>
<h3 id="训练模型-1"><a href="#训练模型-1" class="headerlink" title="训练模型"></a>训练模型</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 超参数</span></span><br><span class="line">src_vocab_size = <span class="number">5000</span> <span class="comment"># 源词汇表大小</span></span><br><span class="line">tgt_vocab_size = <span class="number">5000</span> <span class="comment"># 目标词汇表大小</span></span><br><span class="line">d_model = <span class="number">512</span>         <span class="comment"># 模型维度</span></span><br><span class="line">num_heads = <span class="number">8</span>         <span class="comment"># 注意力头数量</span></span><br><span class="line">num_layers = <span class="number">6</span>        <span class="comment"># 编码器和解码器层数</span></span><br><span class="line">d_ff = <span class="number">2048</span>           <span class="comment"># 前馈网络内层维度</span></span><br><span class="line">max_seq_length = <span class="number">100</span>  <span class="comment"># 最大序列长度</span></span><br><span class="line">dropout = <span class="number">0.1</span>         <span class="comment"># 丢弃概率</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化模型</span></span><br><span class="line">transformer = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成随机数据</span></span><br><span class="line">src_data = torch.randint(<span class="number">1</span>, src_vocab_size, (<span class="number">64</span>, max_seq_length)) <span class="comment"># 源序列</span></span><br><span class="line">tgt_data = torch.randint(<span class="number">1</span>, tgt_vocab_size, (<span class="number">64</span>, max_seq_length)) <span class="comment"># 目标序列</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义损失函数和优化器</span></span><br><span class="line">creterion = nn.CrossEntropyLoss(ignore_index=<span class="number">0</span>)</span><br><span class="line">optimizer = optim.Adam(transformer.parameters(), lr=<span class="number">0.0001</span>, betas=(<span class="number">0.9</span>, <span class="number">0.98</span>), eps=<span class="number">1e-9</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练循环</span></span><br><span class="line">transformer.train()</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    optimizer.zero_grad() </span><br><span class="line"></span><br><span class="line">    <span class="comment"># 输入目标序列时去掉最后一个词（用于预测下一个词）</span></span><br><span class="line">    output = transformer(src_data, tgt_data[:, :-<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算损失时，目标序列从第二个词开始（即预测下一个词）</span></span><br><span class="line">    <span class="comment"># output形状: (batch_size, seq_length-1, tgt_vocab_size)</span></span><br><span class="line">    <span class="comment"># 目标形状: (batch_size, seq_length-1)</span></span><br><span class="line">    loss = creterion(</span><br><span class="line">        output.contiguous().view(-<span class="number">1</span>, tgt_vocab_size),</span><br><span class="line">        tgt_data[:, <span class="number">1</span>:].contiguous().view(-<span class="number">1</span>)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    loss.backward()  <span class="comment"># 反向传播</span></span><br><span class="line">    optimizer.step()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Epoch: <span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>, Loss: <span class="subst">&#123;loss.item()&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>其中：</p>
<ul>
<li><code>betas</code> 和 <code>eps</code> 都是 Adam 优化器里的超参数，用来控制 <strong>梯度一阶/二阶动量的平滑程度</strong> 和 <strong>数值稳定性</strong>，Adam 里会维护两个”滑动平均“</li>
<li><strong>一阶矩（类似带动量的梯度）</strong>，公式如下：</li>
</ul>
<script type="math/tex; mode=display">
m_t=\beta_1m_{t-1}+(1-\beta_1)g_t</script><ul>
<li><strong>二阶矩（梯度平方的滑动平均）</strong>，公式如下：</li>
</ul>
<script type="math/tex; mode=display">
v_t=\beta_2v_{t-1}+(1-\beta_2)g_t^2</script><p>在这公式里：</p>
<ul>
<li><script type="math/tex">g_t</script>：当前步的梯度</li>
<li><script type="math/tex">\beta_1</script> 对应 <code>betas[0]</code>，<script type="math/tex">\beta_2</script> 对应 <code>betas[1]</code> </li>
<li><script type="math/tex">\beta_1</script> 越接近 1，越“记仇”，历史梯度权重大，更新方向更稳定，<script type="math/tex">\beta_2</script> 越接近 1，历史信息保留得越久，学习率调整会更平滑、变化更慢</li>
<li>经典 Transformer（Attention is All You Need）里用的就是 <code>betas=(0.9, 0.98)</code></li>
<li>不推荐将两个值设置的无限接近于1，因为这样会导致历史权重梯度变得很大，效率很低</li>
</ul>
<h3 id="评估模型-1"><a href="#评估模型-1" class="headerlink" title="评估模型"></a>评估模型</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 评估模式</span></span><br><span class="line">transformer.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line">val_src_data = torch.randint(<span class="number">1</span>, src_vocab_size, (<span class="number">64</span>, max_seq_length))</span><br><span class="line">val_tgt_data = torch.randint(<span class="number">1</span>, tgt_vocab_size, (<span class="number">64</span>, max_seq_length))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 假设输入为一批英文和对应的中文翻译（已转换为索引）</span></span><br><span class="line"><span class="comment"># 示例数据：</span></span><br><span class="line"><span class="comment"># src_data: [[3, 14, 25, ..., 0, 0], ...]  # 英文句子（0为填充符）</span></span><br><span class="line"><span class="comment"># tgt_data: [[5, 20, 36, ..., 0, 0], ...]  # 中文翻译（0为填充符）</span></span><br><span class="line"><span class="comment"># 注意：实际应用中需对文本进行分词、编码、填充等预处理</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    val_output = transformer(val_src_data, val_tgt_data[:, :-<span class="number">1</span>])</span><br><span class="line">    val_loss = creterion(</span><br><span class="line">        output.contiguous().view(-<span class="number">1</span>, tgt_vocab_size),</span><br><span class="line">        tgt_data[:, <span class="number">1</span>:].contiguous().view(-<span class="number">1</span>)</span><br><span class="line">    )</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Validation Loss: <span class="subst">&#123;val_loss.item()&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>最终结果：</p>
<p><img src="/img/lazy.gif" data-original="17.png" alt=""></p>
<hr>
<h2 id="模型部署"><a href="#模型部署" class="headerlink" title="模型部署"></a>模型部署</h2><p>（感觉现在学了也没啥用，这边先水一下）</p>
<p><a target="_blank" rel="noopener" href="https://www.runoob.com/pytorch/pytorch-model-deployment.html">PyTorch 模型部署 | 菜鸟教程</a></p>
<p>部署流程：</p>
<p><code>训练模型</code> -&gt; <code>模型优化</code> -&gt; <code>格式转换</code> -&gt; <code>部署环境选择</code> -&gt; <code>服务封装</code> -&gt; <code>性能监控</code></p>
<h3 id="模型导出格式"><a href="#模型导出格式" class="headerlink" title="模型导出格式"></a>模型导出格式</h3><p>PyTorch 主要支持以下导出格式：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">格式</th>
<th style="text-align:left">特点</th>
<th style="text-align:left">适用场景</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">TorchScript</td>
<td style="text-align:left">PyTorch原生格式，保持动态图特性</td>
<td style="text-align:left">PyTorch生态内部使用</td>
</tr>
<tr>
<td style="text-align:left">ONNX</td>
<td style="text-align:left">开放标准，跨框架兼容</td>
<td style="text-align:left">多框架协作环境</td>
</tr>
<tr>
<td style="text-align:left">Torch-TensorRT</td>
<td style="text-align:left">NVIDIA优化格式</td>
<td style="text-align:left">GPU推理加速</td>
</tr>
</tbody>
</table>
</div>
<h3 id="导出为TorchScript"><a href="#导出为TorchScript" class="headerlink" title="导出为TorchScript"></a>导出为TorchScript</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载预训练模型</span></span><br><span class="line">model = torchvision.models.resnet18(pretrained=<span class="literal">True</span>)</span><br><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 示例输入</span></span><br><span class="line">example_input = torch.rand(<span class="number">1</span>, <span class="number">3</span>, <span class="number">224</span>, <span class="number">224</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 方法1: 通过追踪(tracing)导出</span></span><br><span class="line">traced_script = torch.jit.trace(model, example_input)</span><br><span class="line">traced_script.save(<span class="string">&quot;resnet18_traced.pt&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 方法2: 通过脚本(scripting)导出</span></span><br><span class="line">scripted_model = torch.jit.script(model)</span><br><span class="line">scripted_model.save(<span class="string">&quot;resnet18_scripted.pt&quot;</span>)</span><br></pre></td></tr></table></figure>
<p><strong>注意事项</strong>：</p>
<ol>
<li><code>torch.jit.trace</code> 更适合没有控制流的模型</li>
<li><code>torch.jit.script</code> 能处理包含条件判断等复杂逻辑的模型</li>
<li>导出前务必调用 <code>model.eval()</code></li>
</ol>
<h2 id="模型保存和加载"><a href="#模型保存和加载" class="headerlink" title="模型保存和加载"></a>模型保存和加载</h2><h3 id="基本保存和加载方法"><a href="#基本保存和加载方法" class="headerlink" title="基本保存和加载方法"></a>基本保存和加载方法</h3><h4 id="保存整个模型"><a href="#保存整个模型" class="headerlink" title="保存整个模型"></a>保存整个模型</h4><p>这是最简单的方法，保存模型的架构和参数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision.models <span class="keyword">as</span> models</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建并训练一个模型</span></span><br><span class="line">model = models.resnet18(pretrained=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># ... 训练代码 ...</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 保存整个模型</span></span><br><span class="line">torch.save(model, <span class="string">&#x27;model.pth&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载整个模型</span></span><br><span class="line">loaded_model = torch.load(<span class="string">&#x27;model.pth&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p><strong>优点</strong>：</p>
<ul>
<li>代码简单直观</li>
<li>保存了完整的模型结构</li>
</ul>
<p><strong>缺点</strong>：</p>
<ul>
<li>文件体积较大</li>
<li>对模型类的定义有依赖</li>
</ul>
<h4 id="仅保存模型参数（推荐方式）"><a href="#仅保存模型参数（推荐方式）" class="headerlink" title="仅保存模型参数（推荐方式）"></a>仅保存模型参数（推荐方式）</h4><p>更推荐的方式是只保存模型的状态字典(state_dict)：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 保存模型参数</span></span><br><span class="line">torch.save(model.state_dict(), <span class="string">&#x27;model_weights.pth&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载模型参数</span></span><br><span class="line">model = models.resnet18() <span class="comment"># 必须先创建相同架构的模型</span></span><br><span class="line">model.load_state_dict(torch.load(<span class="string">&#x27;model_weights.pth&#x27;</span>))</span><br><span class="line">model.<span class="built_in">eval</span>() <span class="comment"># 设置为评估模式</span></span><br></pre></td></tr></table></figure>
<p><strong>优点</strong>：</p>
<ul>
<li>文件更小</li>
<li>更灵活，可以加载到不同架构中</li>
<li>兼容性更好</li>
</ul>
<h3 id="保存和加载训练状态"><a href="#保存和加载训练状态" class="headerlink" title="保存和加载训练状态"></a>保存和加载训练状态</h3><p>除了保存模型的状态字典，也可以保存优化器和损失</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 保存检查点</span></span><br><span class="line">checkpoint = &#123;</span><br><span class="line">    <span class="string">&#x27;epoch&#x27;</span>: epoch,</span><br><span class="line">    <span class="string">&#x27;model_state_dict&#x27;</span>: model.state_dict(),</span><br><span class="line">    <span class="string">&#x27;optimizer_state_dict&#x27;</span>: optimizer.state_dict(),</span><br><span class="line">    <span class="string">&#x27;loss&#x27;</span>: loss,</span><br><span class="line">    <span class="comment"># 可以添加其他需要保存的信息</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">torch.save(checkpoint, <span class="string">&#x27;checkpoint.pth&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载检查点</span></span><br><span class="line">checkpoint = torch.load(<span class="string">&#x27;checkpoint.pth&#x27;</span>)</span><br><span class="line">model.load_state_dict(checkpoint[<span class="string">&#x27;model_state_dict&#x27;</span>])</span><br><span class="line">optimizer.load_state_dict(checkpoint[<span class="string">&#x27;optimizer_state_dict&#x27;</span>])</span><br><span class="line">epoch = checkpoint[<span class="string">&#x27;epoch&#x27;</span>]</span><br><span class="line">loss = checkpoint[<span class="string">&#x27;loss&#x27;</span>]</span><br><span class="line"></span><br><span class="line">model.<span class="built_in">eval</span>()  <span class="comment"># 或者 model.train() 取决于你的需求</span></span><br></pre></td></tr></table></figure>
<h3 id="跨设备加载模型"><a href="#跨设备加载模型" class="headerlink" title="跨设备加载模型"></a>跨设备加载模型</h3><h4 id="CPU-GPU兼容性处理"><a href="#CPU-GPU兼容性处理" class="headerlink" title="CPU/GPU兼容性处理"></a>CPU/GPU兼容性处理</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 保存时指定map_location</span></span><br><span class="line">torch.save(model.state_dict(), <span class="string">&#x27;model_weights.pth&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载到CPU（当模型是在GPU上训练时）</span></span><br><span class="line">device = torch.device(<span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line">model.load_state_dict(torch.load(<span class="string">&#x27;model_weights.pth&#x27;</span>, map_location=device))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载到GPU</span></span><br><span class="line">device = torch.device(<span class="string">&#x27;cuda&#x27;</span>)</span><br><span class="line">model.load_state_dict(torch.load(<span class="string">&#x27;model_weights.pth&#x27;</span>, map_location=device))</span><br><span class="line">model.to(device)</span><br></pre></td></tr></table></figure>
<h4 id="多GPU训练模型加载"><a href="#多GPU训练模型加载" class="headerlink" title="多GPU训练模型加载"></a>多GPU训练模型加载</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 保存多GPU模型</span></span><br><span class="line">torch.save(model.module.state_dict(), <span class="string">&#x27;multigpu_model.pth&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载到单GPU</span></span><br><span class="line">model = ModelClass()</span><br><span class="line">model.load_state_dict(torch.load(<span class="string">&#x27;multigpu_model.pth&#x27;</span>))</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="模型转换与兼容性"><a href="#模型转换与兼容性" class="headerlink" title="模型转换与兼容性"></a>模型转换与兼容性</h3><h4 id="PyTorch版本兼容性"><a href="#PyTorch版本兼容性" class="headerlink" title="PyTorch版本兼容性"></a>PyTorch版本兼容性</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 保存时指定_use_new_zipfile_serialization=True以获得更好的兼容性</span></span><br><span class="line">torch.save(model.state_dict(), <span class="string">&#x27;model.pth&#x27;</span>, _use_new_zipfile_serialization=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<h4 id="转换为TorchScript"><a href="#转换为TorchScript" class="headerlink" title="转换为TorchScript"></a>转换为TorchScript</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将模型转换为TorchScript格式</span></span><br><span class="line">scripted_model = torch.jit.script(model)</span><br><span class="line">torch.jit.save(scripted_model, <span class="string">&#x27;model_scripted.pt&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载TorchScript模型</span></span><br><span class="line">loaded_script = torch.jit.load(<span class="string">&#x27;model_scripted.pt&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h3 id="最佳实践与常见问题"><a href="#最佳实践与常见问题" class="headerlink" title="最佳实践与常见问题"></a>最佳实践与常见问题</h3><h4 id="最佳实践"><a href="#最佳实践" class="headerlink" title="最佳实践"></a>最佳实践</h4><ol>
<li><strong>命名规范</strong>：使用有意义的文件名，如<code>resnet18_epoch50.pth</code></li>
<li><strong>定期保存</strong>：每隔几个epoch保存一次检查点</li>
<li><strong>验证加载</strong>：保存后立即测试加载功能</li>
<li><strong>文档记录</strong>：记录模型架构和训练参数</li>
<li><strong>版本控制</strong>：将模型文件纳入版本控制系统</li>
</ol>
<h4 id="常见问题解决方案"><a href="#常见问题解决方案" class="headerlink" title="常见问题解决方案"></a>常见问题解决方案</h4><p><strong>问题1</strong>：<code>Missing key(s) in state_dict</code></p>
<p><strong>解决</strong>：确保模型架构完全匹配，或使用<code>strict=False</code>参数：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.load_state_dict(torch.load(&#x27;model.pth&#x27;), strict=False)</span><br></pre></td></tr></table></figure>
<p><strong>问题2</strong>：<code>CUDA out of memory</code></p>
<p><strong>解决</strong>：加载时先放到CPU：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.load_state_dict(torch.load(&#x27;model.pth&#x27;, map_location=&#x27;cpu&#x27;))</span><br></pre></td></tr></table></figure>
<p><strong>问题3</strong>：<code>无法加载旧版本模型</code></p>
<p><strong>解决</strong>：尝试在不同PyTorch版本中加载，或转换模型格式</p>
<hr>
<h2 id="Pytorch-图像分类"><a href="#Pytorch-图像分类" class="headerlink" title="Pytorch 图像分类"></a>Pytorch 图像分类</h2><p>图像分类是计算机视觉中最基础的任务之一，其目标是让计算机能够识别图像中的主要内容并将其归类到预定义的类别中。例如，识别一张图片中是猫还是狗</p>
<p>深度学习模型，特别是卷积神经网络(CNN)，已成为图像分类任务的主流解决方案。PyTorch作为深度学习框架，提供了构建和训练CNN模型的完整工具链。</p>
<p><strong>项目流程概述</strong></p>
<p>一个完整的图像分类项目通常包含以下步骤：</p>
<ol>
<li>数据准备与预处理</li>
<li>模型构建</li>
<li>模型训练</li>
<li>模型评估</li>
<li>模型应用</li>
</ol>
<p><img src="/img/lazy.gif" data-original="19.png" alt=""></p>
<p>很明显，根据这张图，就可以简单写出一个CNN来</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize((<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>), (<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>))</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载CIFAR-10训练集</span></span><br><span class="line">trainset = torchvision.datasets.CIFAR10(root=<span class="string">&quot;./data&quot;</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>, transform=transform)</span><br><span class="line"></span><br><span class="line">trainloader = torch.utils.data.DataLoader(trainset, batch_size=<span class="number">4</span>, shuffle=<span class="literal">True</span>, num_workers=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载CIFAR-10测试集</span></span><br><span class="line">testset = torchvision.datasets.CIFAR10(root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">False</span>, download=<span class="literal">True</span>, transform=transform)</span><br><span class="line">testloader = torch.utils.data.DataLoader(testset, batch_size=<span class="number">4</span>, shuffle=<span class="literal">False</span>, num_workers=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义类别名称</span></span><br><span class="line">classes = (<span class="string">&#x27;plane&#x27;</span>, <span class="string">&#x27;car&#x27;</span>, <span class="string">&#x27;bird&#x27;</span>, <span class="string">&#x27;cat&#x27;</span>, <span class="string">&#x27;deer&#x27;</span>, <span class="string">&#x27;dog&#x27;</span>, <span class="string">&#x27;frog&#x27;</span>, <span class="string">&#x27;horse&#x27;</span>, <span class="string">&#x27;ship&#x27;</span>, <span class="string">&#x27;truck&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Net</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Net, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="comment"># 卷积层1：输入3通道(RGB)，输出6通道，5x5卷积核</span></span><br><span class="line">        <span class="variable language_">self</span>.cnn1 = nn.Conv2d(<span class="number">3</span>, <span class="number">6</span>, <span class="number">5</span>)</span><br><span class="line">        <span class="comment"># 池化层：2x2窗口，步长2</span></span><br><span class="line">        <span class="variable language_">self</span>.pool = nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">        <span class="comment"># 卷积层2：输入6通道，输出16通道，5x5卷积核</span></span><br><span class="line">        <span class="variable language_">self</span>.cnn2 = nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>)</span><br><span class="line">        <span class="comment"># 全连接层1：输入16*5*5，输出120</span></span><br><span class="line">        <span class="variable language_">self</span>.fc1 = nn.Linear(<span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">120</span>)</span><br><span class="line">        <span class="comment"># 全连接层2：输入120，输出84</span></span><br><span class="line">        <span class="variable language_">self</span>.fc2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</span><br><span class="line">        <span class="comment"># 全连接层3：输入84，输出10（对应10个类别）</span></span><br><span class="line">        <span class="variable language_">self</span>.fc3 = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># 第一层卷积+ReLU+池化</span></span><br><span class="line">        x = <span class="variable language_">self</span>.pool(F.relu(<span class="variable language_">self</span>.cnn1(x)))</span><br><span class="line">        <span class="comment"># 第二层卷积+ReLU+池化</span></span><br><span class="line">        x = <span class="variable language_">self</span>.pool(F.relu(<span class="variable language_">self</span>.cnn2(x)))</span><br><span class="line">        <span class="comment"># 展平特征图</span></span><br><span class="line">        x = x.view(-<span class="number">1</span>, <span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>)</span><br><span class="line">        <span class="comment"># 全连接层+ReLU</span></span><br><span class="line">        x = F.relu(<span class="variable language_">self</span>.fc1(x))</span><br><span class="line">        x = F.relu(<span class="variable language_">self</span>.fc2(x))</span><br><span class="line">        <span class="comment"># 输出层</span></span><br><span class="line">        x = <span class="variable language_">self</span>.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">net = Net()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 损失器和优化器</span></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr=<span class="number">0.001</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">net.train()</span><br><span class="line">epochs = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    running_loss = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> i, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(trainloader, <span class="number">0</span>):</span><br><span class="line">        inputs, labels = data</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        </span><br><span class="line">        output = net(inputs)</span><br><span class="line">        loss = criterion(output, labels)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        running_loss += loss.item()</span><br><span class="line">        <span class="keyword">if</span>  i % <span class="number">2000</span> == <span class="number">1999</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&#x27;[<span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>, <span class="subst">&#123;i + <span class="number">1</span>:5d&#125;</span>] loss: <span class="subst">&#123;running_loss / <span class="number">2000</span>:<span class="number">.3</span>f&#125;</span>&#x27;</span>)</span><br><span class="line">            running_loss = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Finished Training&quot;</span>)</span><br><span class="line"></span><br><span class="line">correct = <span class="number">0</span></span><br><span class="line">total = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> testloader:</span><br><span class="line">        images, labels = data</span><br><span class="line">        outputs = net(images)</span><br><span class="line"></span><br><span class="line">        _, predicted = torch.<span class="built_in">max</span>(outputs.data, <span class="number">1</span>)</span><br><span class="line">        total += labels.size(<span class="number">0</span>)</span><br><span class="line">        correct += (predicted == labels).<span class="built_in">sum</span>().item()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Accuracy on test images: <span class="subst">&#123;<span class="number">100</span> * correct / total:<span class="number">.2</span>f&#125;</span>%&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class_correct = <span class="built_in">list</span>(<span class="number">0.</span> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>))</span><br><span class="line">class_total = <span class="built_in">list</span>(<span class="number">0.</span> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>))</span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> testloader:</span><br><span class="line">        images, labels = data</span><br><span class="line">        outputs = net(images)</span><br><span class="line">        _, predicted = torch.<span class="built_in">max</span>(outputs, <span class="number">1</span>)</span><br><span class="line">        c = (predicted == labels).squeeze()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>):</span><br><span class="line">            label = labels[i]</span><br><span class="line">            class_correct[label] += c[i].item()</span><br><span class="line">            class_total[label] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;Accuracy of <span class="subst">&#123;classes[i]:5s&#125;</span>: <span class="subst">&#123;<span class="number">100</span> * class_correct[i] / class_total[i]:<span class="number">.2</span>f&#125;</span>%&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 保存模型参数</span></span><br><span class="line">PATH = <span class="string">&#x27;./cifar_net.pth&#x27;</span></span><br><span class="line">torch.save(net.state_dict(), PATH)</span><br></pre></td></tr></table></figure>
<p>最后再进行一个加载：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 加载模型</span></span><br><span class="line">net = Net()</span><br><span class="line">net.load_state_dict(torch.load(PATH))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用模型进行预测</span></span><br><span class="line">outputs = net(images)</span><br><span class="line">_, predicted = torch.<span class="built_in">max</span>(outputs, <span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Predicted: &#x27;</span>, <span class="string">&#x27; &#x27;</span>.join(<span class="string">f&#x27;<span class="subst">&#123;classes[predicted[j]]:5s&#125;</span>&#x27;</span> <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>)))</span><br></pre></td></tr></table></figure>
<h2 id="Pytorch-文本情感分析"><a href="#Pytorch-文本情感分析" class="headerlink" title="Pytorch 文本情感分析"></a>Pytorch 文本情感分析</h2><p>文本情感分析是自然语言处理(NLP)中的一项基础任务，旨在判断一段文本表达的情感倾向(正面/负面)。本项目将使用PyTorch构建一个深度学习模型，实现对电影评论的情感分类</p>
<p><strong>情感分析的应用场景</strong></p>
<ul>
<li>产品评论分析</li>
<li>社交媒体舆情监控</li>
<li>客户服务反馈分类</li>
<li>市场趋势预测</li>
</ul>
<p><strong>所需库：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> torchtext.data <span class="keyword">import</span> Field, TabularDataset, BucketIterator</span><br><span class="line"><span class="keyword">import</span> spacy</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure>
<p><strong>安装依赖：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip install torch torchtext spacy</span><br><span class="line">python -m spacy download en_core_web_sm</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h3><h4 id="数据集介绍"><a href="#数据集介绍" class="headerlink" title="数据集介绍"></a>数据集介绍</h4><p>使用IMDB电影评论数据集，包含50,000条带有情感标签(正面/负面)的评论。</p>
<h4 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义字段处理</span></span><br><span class="line">TEXT = Field(tokenize=<span class="string">&#x27;spacy&#x27;</span>, </span><br><span class="line">            tokenizer_language=<span class="string">&#x27;en_core_web_sm&#x27;</span>,</span><br><span class="line">            include_lengths=<span class="literal">True</span>)</span><br><span class="line">LABEL = Field(sequential=<span class="literal">False</span>, use_vocab=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载数据集</span></span><br><span class="line">train_data, test_data = TabularDataset.splits(</span><br><span class="line">    path=<span class="string">&#x27;./data&#x27;</span>,</span><br><span class="line">    train=<span class="string">&#x27;train.csv&#x27;</span>,</span><br><span class="line">    test=<span class="string">&#x27;test.csv&#x27;</span>,</span><br><span class="line">    <span class="built_in">format</span>=<span class="string">&#x27;csv&#x27;</span>,</span><br><span class="line">    fields=[(<span class="string">&#x27;text&#x27;</span>, TEXT), (<span class="string">&#x27;label&#x27;</span>, LABEL)]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建词汇表</span></span><br><span class="line">TEXT.build_vocab(train_data, </span><br><span class="line">                max_size=<span class="number">25000</span>,</span><br><span class="line">                vectors=<span class="string">&quot;glove.6B.100d&quot;</span>)</span><br></pre></td></tr></table></figure>
<hr>
<h2 id="模型构建"><a href="#模型构建" class="headerlink" title="模型构建"></a>模型构建</h2><h3 id="LSTM模型架构"><a href="#LSTM模型架构" class="headerlink" title="LSTM模型架构"></a>LSTM模型架构</h3><p><img src="/img/lazy.gif" data-original="20.png" alt="img"></p>
<h3 id="4-2-模型实现代码"><a href="#4-2-模型实现代码" class="headerlink" title="4.2 模型实现代码"></a>4.2 模型实现代码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SentimentLSTM</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.embedding = nn.Embedding(vocab_size, embedding_dim)</span><br><span class="line">        <span class="variable language_">self</span>.lstm = nn.LSTM(embedding_dim, </span><br><span class="line">                           hidden_dim, </span><br><span class="line">                           num_layers=n_layers,</span><br><span class="line">                           bidirectional=<span class="literal">True</span>)</span><br><span class="line">        <span class="variable language_">self</span>.fc = nn.Linear(hidden_dim * <span class="number">2</span>, output_dim)</span><br><span class="line">        <span class="variable language_">self</span>.dropout = nn.Dropout(<span class="number">0.5</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, text, text_lengths</span>):</span><br><span class="line">        embedded = <span class="variable language_">self</span>.dropout(<span class="variable language_">self</span>.embedding(text))</span><br><span class="line">        packed_embedded = nn.utils.rnn.pack_padded_sequence(</span><br><span class="line">            embedded, text_lengths.to(<span class="string">&#x27;cpu&#x27;</span>))</span><br><span class="line">        packed_output, (hidden, cell) = <span class="variable language_">self</span>.lstm(packed_embedded)</span><br><span class="line">        hidden = <span class="variable language_">self</span>.dropout(torch.cat((hidden[-<span class="number">2</span>,:,:], hidden[-<span class="number">1</span>,:,:]), dim=<span class="number">1</span>))</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.fc(hidden)</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h3><h4 id="训练参数设置"><a href="#训练参数设置" class="headerlink" title="训练参数设置"></a>训练参数设置</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 模型参数</span></span><br><span class="line">INPUT_DIM = <span class="built_in">len</span>(TEXT.vocab)</span><br><span class="line">EMBEDDING_DIM = <span class="number">100</span></span><br><span class="line">HIDDEN_DIM = <span class="number">256</span></span><br><span class="line">OUTPUT_DIM = <span class="number">1</span></span><br><span class="line">N_LAYERS = <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化模型</span></span><br><span class="line">model = SentimentLSTM(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 优化器和损失函数</span></span><br><span class="line">optimizer = optim.Adam(model.parameters())</span><br><span class="line">criterion = nn.BCEWithLogitsLoss()</span><br></pre></td></tr></table></figure>
<h4 id="训练循环"><a href="#训练循环" class="headerlink" title="训练循环"></a>训练循环</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">model, iterator, optimizer, criterion</span>):</span><br><span class="line">    epoch_loss = <span class="number">0</span></span><br><span class="line">    epoch_acc = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    model.train()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> iterator:</span><br><span class="line">        text, text_lengths = batch.text</span><br><span class="line">        predictions = model(text, text_lengths).squeeze(<span class="number">1</span>)</span><br><span class="line">        loss = criterion(predictions, batch.label)</span><br><span class="line">        </span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        </span><br><span class="line">        epoch_loss += loss.item()</span><br><span class="line">        epoch_acc += accuracy(predictions, batch.label)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> epoch_loss / <span class="built_in">len</span>(iterator), epoch_acc / <span class="built_in">len</span>(iterator)</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="模型评估"><a href="#模型评估" class="headerlink" title="模型评估"></a>模型评估</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate</span>(<span class="params">model, iterator, criterion</span>):</span><br><span class="line">    epoch_loss = <span class="number">0</span></span><br><span class="line">    epoch_acc = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> batch <span class="keyword">in</span> iterator:</span><br><span class="line">            text, text_lengths = batch.text</span><br><span class="line">            predictions = model(text, text_lengths).squeeze(<span class="number">1</span>)</span><br><span class="line">            loss = criterion(predictions, batch.label)</span><br><span class="line">            epoch_loss += loss.item()</span><br><span class="line">            epoch_acc += accuracy(predictions, batch.label)</span><br><span class="line">            </span><br><span class="line">    <span class="keyword">return</span> epoch_loss / <span class="built_in">len</span>(iterator), epoch_acc / <span class="built_in">len</span>(iterator)</span><br></pre></td></tr></table></figure>
<h4 id="准确率计算"><a href="#准确率计算" class="headerlink" title="准确率计算"></a>准确率计算</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">accuracy</span>(<span class="params">preds, y</span>):</span><br><span class="line">    rounded_preds = torch.<span class="built_in">round</span>(torch.sigmoid(preds))</span><br><span class="line">    correct = (rounded_preds == y).<span class="built_in">float</span>()</span><br><span class="line">    acc = correct.<span class="built_in">sum</span>() / <span class="built_in">len</span>(correct)</span><br><span class="line">    <span class="keyword">return</span> acc</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="模型应用"><a href="#模型应用" class="headerlink" title="模型应用"></a>模型应用</h3><h4 id="预测新文本"><a href="#预测新文本" class="headerlink" title="预测新文本"></a>预测新文本</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">predict_sentiment</span>(<span class="params">model, sentence</span>):</span><br><span class="line">    tokenized = [tok.text <span class="keyword">for</span> tok <span class="keyword">in</span> nlp.tokenizer(sentence)]</span><br><span class="line">    indexed = [TEXT.vocab.stoi[t] <span class="keyword">for</span> t <span class="keyword">in</span> tokenized]</span><br><span class="line">    length = [<span class="built_in">len</span>(indexed)]</span><br><span class="line">    tensor = torch.LongTensor(indexed).to(device)</span><br><span class="line">    tensor = tensor.unsqueeze(<span class="number">1</span>)</span><br><span class="line">    length_tensor = torch.LongTensor(length)</span><br><span class="line">    prediction = torch.sigmoid(model(tensor, length_tensor))</span><br><span class="line">    <span class="keyword">return</span> prediction.item()</span><br></pre></td></tr></table></figure>
<h4 id="示例预测"><a href="#示例预测" class="headerlink" title="示例预测"></a>示例预测</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">positive_review = <span class="string">&quot;This movie was fantastic! I really enjoyed it.&quot;</span></span><br><span class="line">negative_review = <span class="string">&quot;The film was terrible and boring.&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Positive review score: <span class="subst">&#123;predict_sentiment(model, positive_review):<span class="number">.4</span>f&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Negative review score: <span class="subst">&#123;predict_sentiment(model, negative_review):<span class="number">.4</span>f&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://PureStream108.github.io/project">Pure Stream</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://purestream108.github.io/project/2025/10/22/Pytorch/">https://purestream108.github.io/project/2025/10/22/Pytorch/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="https://PureStream108.github.io/project" target="_blank">PureStream & Marblue</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/AI/">AI</a><a class="post-meta__tags" href="/tags/Pytorch/">Pytorch</a></div><div class="post-share"><div class="social-share" data-image="/img/purestream.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2025/11/13/AI%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%E6%94%BB%E5%87%BB/" title="AI数据投毒"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">AI数据投毒</div></div><div class="info-2"><div class="info-item-1">AI安全学习(2)</div></div></div></a><a class="pagination-related" href="/2025/10/12/%E7%BE%8A%E5%9F%8E%E6%9D%AF2025Web-1345/" title="羊城杯2025Web 1345"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">羊城杯2025Web 1345</div></div><div class="info-2"><div class="info-item-1">羊城杯Web1345 wp我想说什么，但是也不懂说什么，那就不说了 ez_unserialize123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102&lt;?phperror_reporting(0);highlight_file(__FILE__);class A &#123;    public $first;    public $step;    public $next;    public function __construct() &#123;        $this-&gt;first = &quot;继续加油！&quot;;    &#125;    public function start() &#123;     ...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2025/11/13/AI%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%E6%94%BB%E5%87%BB/" title="AI数据投毒"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-11-13</div><div class="info-item-2">AI数据投毒</div></div><div class="info-2"><div class="info-item-1">AI安全学习(2)</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/lazy.gif" data-original="/img/purestream.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">Pure Stream</div><div class="author-info-description">X1cT34m & Web</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">26</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">31</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">21</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://www.cnblogs.com/LAMENTXU"><i class="fab fa-github"></i><span>请支持LMTX喵</span></a><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="https://www.mihoyo.com/" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">推：知更鸟，芙宁娜，夏洛蒂，春日野穹，星（崩铁），希儿（崩铁），三无Marblue，朔夜观星，遐蝶，浮波柚叶，二阶堂希罗</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Pytorch%EF%BC%88%E5%B7%B2%E5%AE%8C%E7%BB%93%EF%BC%89"><span class="toc-number">1.</span> <span class="toc-text">Pytorch（已完结）</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BC%80%E5%A7%8B"><span class="toc-number">1.1.</span> <span class="toc-text">开始</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Pytorch-torch%EF%BC%88API%EF%BC%89"><span class="toc-number">1.2.</span> <span class="toc-text">Pytorch torch（API）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Tensor-%E5%88%9B%E5%BB%BA"><span class="toc-number">1.2.1.</span> <span class="toc-text">Tensor 创建</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Tensor-%E6%93%8D%E4%BD%9C"><span class="toc-number">1.2.2.</span> <span class="toc-text">Tensor 操作</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E5%AD%A6%E8%BF%90%E7%AE%97"><span class="toc-number">1.2.3.</span> <span class="toc-text">数学运算</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%9A%8F%E6%9C%BA%E6%95%B0%E7%94%9F%E6%88%90"><span class="toc-number">1.2.4.</span> <span class="toc-text">随机数生成</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0"><span class="toc-number">1.2.5.</span> <span class="toc-text">线性代数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%BE%E5%A4%87%E7%AE%A1%E7%90%86"><span class="toc-number">1.2.6.</span> <span class="toc-text">设备管理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9E%E4%BE%8B"><span class="toc-number">1.2.7.</span> <span class="toc-text">实例</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Pytorch-torch-nn%EF%BC%88API%EF%BC%89"><span class="toc-number">1.3.</span> <span class="toc-text">Pytorch torch.nn（API）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AE%B9%E5%99%A8"><span class="toc-number">1.3.1.</span> <span class="toc-text">神经网络容器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E5%B1%82"><span class="toc-number">1.3.2.</span> <span class="toc-text">线性层</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8D%B7%E7%A7%AF%E5%B1%82"><span class="toc-number">1.3.3.</span> <span class="toc-text">卷积层</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B1%A0%E5%8C%96%E5%B1%82"><span class="toc-number">1.3.4.</span> <span class="toc-text">池化层</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="toc-number">1.3.5.</span> <span class="toc-text">激活函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">1.3.6.</span> <span class="toc-text">损失函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BD%92%E4%B8%80%E5%8C%96%E5%B1%82"><span class="toc-number">1.3.7.</span> <span class="toc-text">归一化层</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%B1%82"><span class="toc-number">1.3.8.</span> <span class="toc-text">循环神经网络层</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B5%8C%E5%85%A5%E5%B1%82"><span class="toc-number">1.3.9.</span> <span class="toc-text">嵌入层</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Dropout-%E5%B1%82"><span class="toc-number">1.3.10.</span> <span class="toc-text">Dropout 层</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9E%E7%94%A8%E5%87%BD%E6%95%B0"><span class="toc-number">1.3.11.</span> <span class="toc-text">实用函数</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BC%A0%E9%87%8F%EF%BC%88tensor%EF%BC%89"><span class="toc-number">1.4.</span> <span class="toc-text">张量（tensor）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BC%A0%E9%87%8F%E5%9F%BA%E6%9C%AC%E6%96%B9%E5%BC%8F"><span class="toc-number">1.4.1.</span> <span class="toc-text">张量基本方式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BC%A0%E9%87%8F%E5%9F%BA%E6%9C%AC%E5%B1%9E%E6%80%A7"><span class="toc-number">1.4.2.</span> <span class="toc-text">张量基本属性</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BC%A0%E9%87%8F%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C"><span class="toc-number">1.4.3.</span> <span class="toc-text">张量基本操作</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9F%BA%E7%A1%80%E6%93%8D%E4%BD%9C%EF%BC%9A"><span class="toc-number">1.4.3.1.</span> <span class="toc-text">基础操作：</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%BD%A2%E7%8A%B6%E6%93%8D%E4%BD%9C%EF%BC%9A"><span class="toc-number">1.4.3.2.</span> <span class="toc-text">形状操作：</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#GPU%E5%8A%A0%E9%80%9F"><span class="toc-number">1.4.4.</span> <span class="toc-text">GPU加速</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BC%A0%E9%87%8F%E4%B8%8ENumpy%E4%BA%92%E7%9B%B8%E6%93%8D%E4%BD%9C"><span class="toc-number">1.4.5.</span> <span class="toc-text">张量与Numpy互相操作</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80"><span class="toc-number">1.5.</span> <span class="toc-text">神经网络基础</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%A5%9E%E7%BB%8F%E5%85%83%EF%BC%88Neuron%EF%BC%89"><span class="toc-number">1.5.1.</span> <span class="toc-text">神经元（Neuron）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B1%82%EF%BC%88Layer%EF%BC%89"><span class="toc-number">1.5.2.</span> <span class="toc-text">层（Layer）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88FNN%EF%BC%89"><span class="toc-number">1.5.3.</span> <span class="toc-text">前馈神经网络（FNN）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88RNN%EF%BC%89"><span class="toc-number">1.5.4.</span> <span class="toc-text">循环神经网络（RNN）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88CNN%EF%BC%89"><span class="toc-number">1.5.5.</span> <span class="toc-text">卷积神经网络（CNN）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9E%E4%BE%8B-1"><span class="toc-number">1.5.6.</span> <span class="toc-text">实例</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%EF%BC%88Activation-Function%EF%BC%89"><span class="toc-number">1.5.7.</span> <span class="toc-text">激活函数（Activation Function）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%EF%BC%88Loss-Function%EF%BC%89"><span class="toc-number">1.5.8.</span> <span class="toc-text">损失函数（Loss Function）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BC%98%E5%8C%96%E5%99%A8%EF%BC%88Optimizer%EF%BC%89"><span class="toc-number">1.5.9.</span> <span class="toc-text">优化器（Optimizer）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B%EF%BC%88Training-Process%EF%BC%89"><span class="toc-number">1.5.10.</span> <span class="toc-text">训练过程（Training Process）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Pytorch-torch-optim"><span class="toc-number">1.6.</span> <span class="toc-text">Pytorch torch.optim</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E9%9C%80%E8%A6%81%E4%BC%98%E5%8C%96%E5%99%A8"><span class="toc-number">1.6.1.</span> <span class="toc-text">为什么需要优化器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B8%B8%E8%A7%81%E4%BC%98%E5%8C%96%E5%99%A8%E7%B1%BB%E5%9E%8B"><span class="toc-number">1.6.2.</span> <span class="toc-text">常见优化器类型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B8%B8%E7%94%A8%E4%BC%98%E5%8C%96%E5%99%A8"><span class="toc-number">1.6.3.</span> <span class="toc-text">常用优化器</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#SGD-%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="toc-number">1.6.3.1.</span> <span class="toc-text">SGD (随机梯度下降)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Adam-%E8%87%AA%E9%80%82%E5%BA%94%E7%9F%A9%E4%BC%B0%E8%AE%A1"><span class="toc-number">1.6.3.2.</span> <span class="toc-text">Adam (自适应矩估计)</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%AB%98%E7%BA%A7%E6%8A%80%E5%B7%A7"><span class="toc-number">1.6.4.</span> <span class="toc-text">高级技巧</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%B0%83%E5%BA%A6"><span class="toc-number">1.6.4.1.</span> <span class="toc-text">学习率调度</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8F%82%E6%95%B0%E5%88%86%E7%BB%84%E4%BC%98%E5%8C%96"><span class="toc-number">1.6.4.2.</span> <span class="toc-text">参数分组优化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E8%A3%81%E5%89%AA"><span class="toc-number">1.6.4.3.</span> <span class="toc-text">梯度裁剪</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AC%AC%E4%B8%80%E4%B8%AA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">1.7.</span> <span class="toc-text">第一个神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9E%E4%BE%8B1"><span class="toc-number">1.7.1.</span> <span class="toc-text">实例1</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95"><span class="toc-number">1.7.2.</span> <span class="toc-text">梯度下降算法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9E%E4%BE%8B2"><span class="toc-number">1.7.3.</span> <span class="toc-text">实例2</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E4%B8%8E%E5%8A%A0%E8%BD%BD"><span class="toc-number">1.8.</span> <span class="toc-text">数据处理与加载</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%87%AA%E5%AE%9A%E4%B9%89Dataset"><span class="toc-number">1.8.1.</span> <span class="toc-text">自定义Dataset</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#DataLoader%E5%8A%A0%E8%BD%BD%E6%95%B0%E6%8D%AE"><span class="toc-number">1.8.2.</span> <span class="toc-text">DataLoader加载数据</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%A2%84%E5%A4%84%E7%90%86%E5%92%8C%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA"><span class="toc-number">1.8.3.</span> <span class="toc-text">预处理和数据增强</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8A%A0%E8%BD%BD%E5%9B%BE%E5%83%8F%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">1.8.4.</span> <span class="toc-text">加载图像数据集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%94%A8%E5%A4%9A%E4%B8%AA%E6%95%B0%E6%8D%AE%E6%BA%90%EF%BC%88Multi-source-Dataset%EF%BC%89"><span class="toc-number">1.8.5.</span> <span class="toc-text">用多个数据源（Multi-source Dataset）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="toc-number">1.9.</span> <span class="toc-text">线性回归</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9A%E4%B9%89%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.9.1.</span> <span class="toc-text">定义线性回归模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9A%E4%B9%89%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E4%B8%8E%E4%BC%98%E5%8C%96%E5%99%A8"><span class="toc-number">1.9.2.</span> <span class="toc-text">定义损失函数与优化器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.9.3.</span> <span class="toc-text">训练模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AF%84%E4%BC%B0"><span class="toc-number">1.9.4.</span> <span class="toc-text">评估</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-number">1.9.5.</span> <span class="toc-text">总结</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">1.10.</span> <span class="toc-text">卷积神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%84"><span class="toc-number">1.10.1.</span> <span class="toc-text">基本结构</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%BE%93%E5%85%A5%E5%B1%82%EF%BC%88Input-Layer%EF%BC%89"><span class="toc-number">1.10.1.1.</span> <span class="toc-text">输入层（Input Layer）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8D%B7%E7%A7%AF%E5%B1%82%EF%BC%88Convolutional-Layer%EF%BC%89"><span class="toc-number">1.10.1.2.</span> <span class="toc-text">卷积层（Convolutional Layer）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%EF%BC%88Activation-Function%EF%BC%89-1"><span class="toc-number">1.10.1.3.</span> <span class="toc-text">激活函数（Activation Function）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%B1%A0%E5%8C%96%E5%B1%82%EF%BC%88Pooling-Layer%EF%BC%89"><span class="toc-number">1.10.1.4.</span> <span class="toc-text">池化层（Pooling Layer）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%BD%92%E4%B8%80%E5%8C%96%E5%B1%82%EF%BC%88Normalization-Layer%EF%BC%8C-%E5%8F%AF%E9%80%89%EF%BC%89"><span class="toc-number">1.10.1.5.</span> <span class="toc-text">归一化层（Normalization Layer， 可选）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%B1%82%EF%BC%88Fully-Connected-Layer%EF%BC%89"><span class="toc-number">1.10.1.6.</span> <span class="toc-text">全连接层（Fully Connected Layer）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%BE%93%E5%87%BA%E5%B1%82%EF%BC%88Output-Layer%EF%BC%89"><span class="toc-number">1.10.1.7.</span> <span class="toc-text">输出层（Output Layer）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%EF%BC%88Loss-Function%EF%BC%89-1"><span class="toc-number">1.10.1.8.</span> <span class="toc-text">损失函数（Loss Function）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BC%98%E5%8C%96%E5%99%A8%EF%BC%88Optimizer%EF%BC%89-1"><span class="toc-number">1.10.1.9.</span> <span class="toc-text">优化器（Optimizer）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%AD%A3%E5%88%99%E5%8C%96%EF%BC%88Regularization%EF%BC%89"><span class="toc-number">1.10.1.10.</span> <span class="toc-text">正则化（Regularization）</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9E%E4%BE%8B-2"><span class="toc-number">1.10.2.</span> <span class="toc-text">实例</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%8A%A0%E8%BD%BD"><span class="toc-number">1.10.2.1.</span> <span class="toc-text">数据加载</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%9A%E4%B9%89CNN%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.10.2.2.</span> <span class="toc-text">定义CNN模型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E3%80%81%E4%BC%98%E5%8C%96%E5%87%BD%E6%95%B0%E5%92%8C%E8%AE%AD%E7%BB%83"><span class="toc-number">1.10.2.3.</span> <span class="toc-text">损失函数、优化函数和训练</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%B5%8B%E8%AF%95%E8%AF%84%E4%BC%B0"><span class="toc-number">1.10.2.4.</span> <span class="toc-text">测试评估</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%8C%E6%95%B4%E4%BB%A3%E7%A0%81%E5%A6%82%E4%B8%8B%EF%BC%9A%EF%BC%88%E5%8F%AF%E8%A7%86%E5%8C%96%E9%83%A8%E5%88%86%E5%8F%AF%E4%BB%A5%E4%B8%8D%E7%9C%8B%EF%BC%89"><span class="toc-number">1.10.2.5.</span> <span class="toc-text">完整代码如下：（可视化部分可以不看）</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">1.11.</span> <span class="toc-text">循环神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%84-1"><span class="toc-number">1.11.1.</span> <span class="toc-text">基本结构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E5%9D%97"><span class="toc-number">1.11.2.</span> <span class="toc-text">模块</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9E%E4%BE%8B1-1"><span class="toc-number">1.11.3.</span> <span class="toc-text">实例1</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%9A%E4%B9%89RNN%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.11.3.1.</span> <span class="toc-text">定义RNN模型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE"><span class="toc-number">1.11.3.2.</span> <span class="toc-text">训练数据</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%EF%BC%8C%E4%BC%98%E5%8C%96%E5%99%A8%E5%92%8C%E8%AE%AD%E7%BB%83"><span class="toc-number">1.11.3.3.</span> <span class="toc-text">损失函数，优化器和训练</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AF%84%E4%BC%B0%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.11.3.4.</span> <span class="toc-text">评估模型</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9E%E4%BE%8B2-1"><span class="toc-number">1.11.4.</span> <span class="toc-text">实例2</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">1.12.</span> <span class="toc-text">数据集</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#PyTorch-%E5%86%85%E7%BD%AE%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">1.12.1.</span> <span class="toc-text">PyTorch 内置数据集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#torchvision-%E5%92%8C-torchtext"><span class="toc-number">1.12.2.</span> <span class="toc-text">torchvision 和 torchtext</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#torch-utils-data-Dataset"><span class="toc-number">1.12.3.</span> <span class="toc-text">torch.utils.data.Dataset</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Dataset-%E4%B8%8E-DataLoader-%E7%9A%84%E8%87%AA%E5%AE%9A%E4%B9%89%E5%BA%94%E7%94%A8"><span class="toc-number">1.12.4.</span> <span class="toc-text">Dataset 与 DataLoader 的自定义应用</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E8%BD%AC%E6%8D%A2"><span class="toc-number">1.13.</span> <span class="toc-text">数据转换</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9F%BA%E7%A1%80%E5%8F%98%E6%8D%A2%E6%93%8D%E4%BD%9C"><span class="toc-number">1.13.1.</span> <span class="toc-text">基础变换操作</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA%E6%93%8D%E4%BD%9C"><span class="toc-number">1.13.2.</span> <span class="toc-text">数据增强操作</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%84%E5%90%88%E5%8F%98%E6%8D%A2"><span class="toc-number">1.13.3.</span> <span class="toc-text">组合变换</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%87%AA%E5%AE%9A%E4%B9%89%E8%BD%AC%E6%8D%A2"><span class="toc-number">1.13.4.</span> <span class="toc-text">自定义转换</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9E%E4%BE%8B-3"><span class="toc-number">1.13.5.</span> <span class="toc-text">实例</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Transform"><span class="toc-number">1.14.</span> <span class="toc-text">Transform</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BC%96%E7%A0%81%E5%99%A8%EF%BC%88Encoder%EF%BC%89"><span class="toc-number">1.14.1.</span> <span class="toc-text">编码器（Encoder）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A7%A3%E7%A0%81%E5%99%A8%EF%BC%88Decoder%EF%BC%89"><span class="toc-number">1.14.2.</span> <span class="toc-text">解码器（Decoder）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3"><span class="toc-number">1.14.3.</span> <span class="toc-text">核心思想</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%EF%BC%88Self-Attention%EF%BC%89"><span class="toc-number">1.14.3.1.</span> <span class="toc-text">自注意力（Self-Attention）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A4%9A%E5%A4%B4%E6%B3%A8%E6%84%8F%E5%8A%9B%EF%BC%88Multi-Head-Attention%EF%BC%89"><span class="toc-number">1.14.3.2.</span> <span class="toc-text">多头注意力（Multi-Head Attention）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81%EF%BC%88Positional-Encoding%EF%BC%89"><span class="toc-number">1.14.3.3.</span> <span class="toc-text">位置编码（Positional Encoding）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%BC%96%E7%A0%81%E5%99%A8-%E8%A7%A3%E7%A0%81%E5%99%A8%E6%9E%B6%E6%9E%84"><span class="toc-number">1.14.3.4.</span> <span class="toc-text">编码器-解码器架构</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BA%94%E7%94%A8"><span class="toc-number">1.14.4.</span> <span class="toc-text">应用</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Pytorch-Transformer"><span class="toc-number">1.15.</span> <span class="toc-text">Pytorch Transformer</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9A%E4%B9%89%E5%A4%9A%E5%A4%B4%E6%B3%A8%E6%84%8F%E5%8A%9B"><span class="toc-number">1.15.1.</span> <span class="toc-text">定义多头注意力</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9A%E4%B9%89%E5%89%8D%E9%A6%88%E7%BD%91%E7%BB%9C"><span class="toc-number">1.15.2.</span> <span class="toc-text">定义前馈网络</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81"><span class="toc-number">1.15.3.</span> <span class="toc-text">位置编码</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9E%84%E5%BB%BA%E7%BC%96%E7%A0%81%E5%99%A8"><span class="toc-number">1.15.4.</span> <span class="toc-text">构建编码器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9E%84%E5%BB%BA%E8%A7%A3%E7%A0%81%E5%99%A8"><span class="toc-number">1.15.5.</span> <span class="toc-text">构建解码器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9E%84%E5%BB%BATransformer"><span class="toc-number">1.15.6.</span> <span class="toc-text">构建Transformer</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B-1"><span class="toc-number">1.15.7.</span> <span class="toc-text">训练模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AF%84%E4%BC%B0%E6%A8%A1%E5%9E%8B-1"><span class="toc-number">1.15.8.</span> <span class="toc-text">评估模型</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2"><span class="toc-number">1.16.</span> <span class="toc-text">模型部署</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E5%AF%BC%E5%87%BA%E6%A0%BC%E5%BC%8F"><span class="toc-number">1.16.1.</span> <span class="toc-text">模型导出格式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AF%BC%E5%87%BA%E4%B8%BATorchScript"><span class="toc-number">1.16.2.</span> <span class="toc-text">导出为TorchScript</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E4%BF%9D%E5%AD%98%E5%92%8C%E5%8A%A0%E8%BD%BD"><span class="toc-number">1.17.</span> <span class="toc-text">模型保存和加载</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9F%BA%E6%9C%AC%E4%BF%9D%E5%AD%98%E5%92%8C%E5%8A%A0%E8%BD%BD%E6%96%B9%E6%B3%95"><span class="toc-number">1.17.1.</span> <span class="toc-text">基本保存和加载方法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BF%9D%E5%AD%98%E6%95%B4%E4%B8%AA%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.17.1.1.</span> <span class="toc-text">保存整个模型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BB%85%E4%BF%9D%E5%AD%98%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0%EF%BC%88%E6%8E%A8%E8%8D%90%E6%96%B9%E5%BC%8F%EF%BC%89"><span class="toc-number">1.17.1.2.</span> <span class="toc-text">仅保存模型参数（推荐方式）</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BF%9D%E5%AD%98%E5%92%8C%E5%8A%A0%E8%BD%BD%E8%AE%AD%E7%BB%83%E7%8A%B6%E6%80%81"><span class="toc-number">1.17.2.</span> <span class="toc-text">保存和加载训练状态</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B7%A8%E8%AE%BE%E5%A4%87%E5%8A%A0%E8%BD%BD%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.17.3.</span> <span class="toc-text">跨设备加载模型</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#CPU-GPU%E5%85%BC%E5%AE%B9%E6%80%A7%E5%A4%84%E7%90%86"><span class="toc-number">1.17.3.1.</span> <span class="toc-text">CPU&#x2F;GPU兼容性处理</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A4%9AGPU%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E5%8A%A0%E8%BD%BD"><span class="toc-number">1.17.3.2.</span> <span class="toc-text">多GPU训练模型加载</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E8%BD%AC%E6%8D%A2%E4%B8%8E%E5%85%BC%E5%AE%B9%E6%80%A7"><span class="toc-number">1.17.4.</span> <span class="toc-text">模型转换与兼容性</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#PyTorch%E7%89%88%E6%9C%AC%E5%85%BC%E5%AE%B9%E6%80%A7"><span class="toc-number">1.17.4.1.</span> <span class="toc-text">PyTorch版本兼容性</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%BD%AC%E6%8D%A2%E4%B8%BATorchScript"><span class="toc-number">1.17.4.2.</span> <span class="toc-text">转换为TorchScript</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5%E4%B8%8E%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98"><span class="toc-number">1.17.5.</span> <span class="toc-text">最佳实践与常见问题</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5"><span class="toc-number">1.17.5.1.</span> <span class="toc-text">最佳实践</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88"><span class="toc-number">1.17.5.2.</span> <span class="toc-text">常见问题解决方案</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Pytorch-%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB"><span class="toc-number">1.18.</span> <span class="toc-text">Pytorch 图像分类</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Pytorch-%E6%96%87%E6%9C%AC%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90"><span class="toc-number">1.19.</span> <span class="toc-text">Pytorch 文本情感分析</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%87%86%E5%A4%87"><span class="toc-number">1.19.1.</span> <span class="toc-text">数据准备</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E9%9B%86%E4%BB%8B%E7%BB%8D"><span class="toc-number">1.19.1.1.</span> <span class="toc-text">数据集介绍</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86"><span class="toc-number">1.19.1.2.</span> <span class="toc-text">数据预处理</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E6%9E%84%E5%BB%BA"><span class="toc-number">1.20.</span> <span class="toc-text">模型构建</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#LSTM%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84"><span class="toc-number">1.20.1.</span> <span class="toc-text">LSTM模型架构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-%E6%A8%A1%E5%9E%8B%E5%AE%9E%E7%8E%B0%E4%BB%A3%E7%A0%81"><span class="toc-number">1.20.2.</span> <span class="toc-text">4.2 模型实现代码</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83"><span class="toc-number">1.20.3.</span> <span class="toc-text">模型训练</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E5%8F%82%E6%95%B0%E8%AE%BE%E7%BD%AE"><span class="toc-number">1.20.3.1.</span> <span class="toc-text">训练参数设置</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E5%BE%AA%E7%8E%AF"><span class="toc-number">1.20.3.2.</span> <span class="toc-text">训练循环</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0"><span class="toc-number">1.20.4.</span> <span class="toc-text">模型评估</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%87%86%E7%A1%AE%E7%8E%87%E8%AE%A1%E7%AE%97"><span class="toc-number">1.20.4.1.</span> <span class="toc-text">准确率计算</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8"><span class="toc-number">1.20.5.</span> <span class="toc-text">模型应用</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%A2%84%E6%B5%8B%E6%96%B0%E6%96%87%E6%9C%AC"><span class="toc-number">1.20.5.1.</span> <span class="toc-text">预测新文本</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%A4%BA%E4%BE%8B%E9%A2%84%E6%B5%8B"><span class="toc-number">1.20.5.2.</span> <span class="toc-text">示例预测</span></a></li></ol></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/12/06/%E6%9E%81%E5%AE%A2%E5%A4%A7%E6%8C%91%E6%88%982025-Web/" title="极客大挑战2025 Web">极客大挑战2025 Web</a><time datetime="2025-12-06T11:52:11.000Z" title="发表于 2025-12-06 19:52:11">2025-12-06</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/11/25/XSLeaks/" title="XSLeaks">XSLeaks</a><time datetime="2025-11-25T09:25:46.000Z" title="发表于 2025-11-25 17:25:46">2025-11-25</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/11/13/AI%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%E6%94%BB%E5%87%BB/" title="AI数据投毒">AI数据投毒</a><time datetime="2025-11-13T09:45:57.000Z" title="发表于 2025-11-13 17:45:57">2025-11-13</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/10/22/Pytorch/" title="Pytorch">Pytorch</a><time datetime="2025-10-22T10:49:46.000Z" title="发表于 2025-10-22 18:49:46">2025-10-22</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/10/12/%E7%BE%8A%E5%9F%8E%E6%9D%AF2025Web-1345/" title="羊城杯2025Web 1345">羊城杯2025Web 1345</a><time datetime="2025-10-12T04:56:13.000Z" title="发表于 2025-10-12 12:56:13">2025-10-12</time></div></div></div></div></div></div></main><footer id="footer"><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;&nbsp;2025 By Pure Stream</span><span class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 7.3.0</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.5.0-b1</a></span></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"><script>(() => {
  const loadMathjax = () => {
    if (!window.MathJax) {
      window.MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          tags: 'none',
        },
        chtml: {
          scale: 1.1
        },
        options: {
          enableMenu: true,
          renderActions: {
            findScript: [10, doc => {
              for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
                const display = !!node.type.match(/; *mode=display/)
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
                const text = document.createTextNode('')
                node.parentNode.replaceChild(text, node)
                math.start = {node: text, delim: '', n: 0}
                math.end = {node: text, delim: '', n: 0}
                doc.math.push(math)
              }
            }, '']
          }
        }
      }

      const script = document.createElement('script')
      script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
      script.id = 'MathJax-script'
      script.async = true
      document.head.appendChild(script)
    } else {
      MathJax.startup.document.state(0)
      MathJax.texReset()
      MathJax.typesetPromise()
    }
  }

  btf.addGlobalFn('encrypt', loadMathjax, 'mathjax')
  window.pjax ? loadMathjax() : window.addEventListener('load', loadMathjax)
})()</script></div><script defer="defer" id="fluttering_ribbon" mobile="true" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-fluttering-ribbon.min.js"></script><script id="click-show-text" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/click-show-text.min.js" data-mobile="true" data-text="CTF,Web,miHoYo,PureStream,Marblue" data-fontsize="15px" data-random="true" async="async"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div>
        <style>
            [bg-lazy] {
                background-image: none !important;
                background-color: #eee !important;
            }
        </style>
        <script>
            window.imageLazyLoadSetting = {
                isSPA: false,
                preloadRatio: 1,
                processImages: null,
            };
        </script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z\d\-\.\+]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(t.test(e.href)||r.test(e.href))&&(e.href=a.dataset.original)})});</script><script>(r=>{r.imageLazyLoadSetting.processImages=t;var a=r.imageLazyLoadSetting.isSPA,o=r.imageLazyLoadSetting.preloadRatio||1,d=i();function i(){var t=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")),e=Array.prototype.slice.call(document.querySelectorAll("[bg-lazy]"));return t.concat(e)}function t(t){(a||t)&&(d=i());for(var e,n=0;n<d.length;n++)0<=(e=(e=d[n]).getBoundingClientRect()).bottom&&0<=e.left&&e.top<=(r.innerHeight*o||document.documentElement.clientHeight*o)&&(()=>{var t,e,a,o,i=d[n];e=function(){d=d.filter(function(t){return i!==t}),r.imageLazyLoadSetting.onImageLoaded&&r.imageLazyLoadSetting.onImageLoaded(i)},(t=i).dataset.loaded||(t.hasAttribute("bg-lazy")?(t.removeAttribute("bg-lazy"),e&&e()):(a=new Image,o=t.getAttribute("data-original"),a.onload=function(){t.src=o,t.removeAttribute("data-original"),t.setAttribute("data-loaded",!0),e&&e()},a.onerror=function(){t.removeAttribute("data-original"),t.setAttribute("data-loaded",!1),t.src=o},t.src!==o&&(a.src=o)))})()}function e(){clearTimeout(t.tId),t.tId=setTimeout(t,500)}t(),document.addEventListener("scroll",e),r.addEventListener("resize",e),r.addEventListener("orientationchange",e)})(this);</script></body></html>